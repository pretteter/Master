{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "from IPython import get_ipython\n",
    "import librosa\n",
    "\n",
    "import ipynb.fs.defs.build_spectogram_ds as wave_to_spec\n",
    "\n",
    "\n",
    "\n",
    "global history\n",
    "global model\n",
    "global test_ds\n",
    "global label_names\n",
    "global TEST_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training and validation loss curves to check how your model has improved during training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_results():\n",
    "    metrics = history.history\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    print(metrics)\n",
    "    plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "    # plt.plot(history.epoch, metrics['loss'])\n",
    "    plt.legend(['loss', 'val_loss'])\n",
    "    plt.ylim([0, max(plt.ylim())])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss [CrossEntropy]')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
    "    # plt.plot(history.epoch, 100*np.array(metrics['accuracy']))\n",
    "    plt.legend(['accuracy', 'val_accuracy'])\n",
    "    plt.ylim([0, 100])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy [%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix():\n",
    "    y_pred = model.predict(test_ds)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "    y_true = tf.concat(list(test_ds.map(lambda s,lab: lab)), axis=0)\n",
    "\n",
    "    confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(confusion_mtx,\n",
    "                xticklabels=label_names,\n",
    "                yticklabels=label_names,\n",
    "                annot=True, fmt='g')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference on an audio file\n",
    "\n",
    "Finally, verify the model's prediction output using an input audio file of someone saying \"no\". How well does your model perform?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_audio(file_path):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path, sr=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process file '{file_path}': {e}\")\n",
    "        return\n",
    "    \n",
    "    # Überprüfen, ob das Audio mehrdimensional ist (z.B. Stereo)\n",
    "    if audio.ndim > 1:\n",
    "        waveform = tf.squeeze(audio)\n",
    "    else:\n",
    "        waveform = audio \n",
    "\n",
    "    # Berechne die Anzahl der Samples, die eine Sekunde darstellen\n",
    "    one_second_samples = sample_rate\n",
    "\n",
    "    # Finde die Mitte des Audios und schneide eine Sekunde heraus\n",
    "    mid_point = len(audio) // 2\n",
    "    start = max(0, mid_point - one_second_samples // 2)\n",
    "    end = start + one_second_samples\n",
    "    audio_segment = audio[start:end]\n",
    "\n",
    "    # Überprüfen, ob das Audio mehrdimensional ist (z.B. Stereo)\n",
    "    if audio_segment.ndim > 1:\n",
    "        waveform = tf.squeeze(audio_segment)\n",
    "    else:\n",
    "        waveform = audio_segment\n",
    "\n",
    "    print(f\"Form des Audiosignals: {waveform.shape}\")\n",
    "    print(f\"Sample Rate: {sample_rate}\")\n",
    "\n",
    "    spectrogram = wave_to_spec.get_spectrogram(waveform)\n",
    "    # spectrogram = wave_to_spec.get_mel_spectrogram(waveform)\n",
    "\n",
    "    # Dimension anpassen für das Modell\n",
    "    input_tensor = spectrogram[tf.newaxis, ...]\n",
    "\n",
    "    prediction = model(input_tensor)\n",
    "\n",
    "    x_labels = label_names  \n",
    "\n",
    "    plt.bar(x_labels, tf.nn.softmax(prediction[0]))\n",
    "    plt.title(f'Vorhersage für {os.path.basename(file_path)}')\n",
    "    plt.show()\n",
    "\n",
    "    display.display(display.Audio(waveform, rate=sample_rate))\n",
    "\n",
    "def process_directory_for_visualization(directory_path):\n",
    "    # wav_files = glob.glob(os.path.join(directory_path, \"*.wav\"))\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:    \n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Verarbeite Datei: {file_path}\")\n",
    "                visualize_audio(file_path)\n",
    "            else:\n",
    "                print(\"Keine WAV-Dateien im Verzeichnis gefunden.\")\n",
    "                return\n",
    "\n",
    "# extract_zip(TEST_DIR, DATA_DIR)\n",
    "# rename_audio_files(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(_test_dir, _test_ds,_history, _model, _label_names):\n",
    "    global TEST_DIR, history, model, test_ds, label_names\n",
    "    \n",
    "    TEST_DIR =_test_dir\n",
    "    history =_history\n",
    "    model = _model\n",
    "    test_ds = _test_ds\n",
    "    label_names = _label_names\n",
    "    \n",
    "    model_train_results()\n",
    "    \n",
    "    #Run the model on the test set and check the model's performance\n",
    "    model.evaluate(test_ds, return_dict=True)\n",
    "    \n",
    "    confusion_matrix()\n",
    "    process_directory_for_visualization(TEST_DIR)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
