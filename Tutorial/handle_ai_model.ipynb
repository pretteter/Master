{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from IPython import get_ipython\n",
    "import librosa\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "import ipynb.fs.defs.build_spectogram_ds as wave_to_spec\n",
    "\n",
    "\n",
    "\n",
    "global history\n",
    "global model\n",
    "global test_ds\n",
    "global val_ds\n",
    "global train_ds\n",
    "global label_names\n",
    "global TEST_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training and validation loss curves to check how your model has improved during training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_zip_file_path(base_name = \"model\", decimals=3):\n",
    "\n",
    "        val_loss, val_accuracy = model.evaluate(val_ds, verbose=0)\n",
    "        loss, accuracy = model.evaluate(train_ds, verbose=0)\n",
    "\n",
    "        # Metriken runden\n",
    "        loss_rounded = round(loss, decimals)\n",
    "        accuracy_rounded = round(accuracy, decimals)\n",
    "        val_loss_rounded = round(val_loss, decimals)\n",
    "        val_accuracy_rounded = round(val_accuracy, decimals)\n",
    "\n",
    "        # Dynamischer Dateiname\n",
    "        return (f\"{base_name}_loss_{loss_rounded:.{decimals}f}_acc_{accuracy_rounded:.{decimals}f}\"\n",
    "                f\"_val_loss_{val_loss_rounded:.{decimals}f}_val_acc_{val_accuracy_rounded:.{decimals}f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_results():\n",
    "    metrics = history.history\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    print(metrics)\n",
    "    plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "    # plt.plot(history.epoch, metrics['loss'])\n",
    "    plt.legend(['loss', 'val_loss'])\n",
    "    plt.ylim([0, max(plt.ylim())])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss [CrossEntropy]')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
    "    # plt.plot(history.epoch, 100*np.array(metrics['accuracy']))\n",
    "    plt.legend(['accuracy', 'val_accuracy'])\n",
    "    plt.ylim([0, 100])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy [%]')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./saved/{adjust_zip_file_path(base_name='history')}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix():\n",
    "    y_pred = model.predict(test_ds)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "    y_true = tf.concat(list(test_ds.map(lambda s,lab: lab)), axis=0)\n",
    "\n",
    "    confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(confusion_mtx,\n",
    "                xticklabels=label_names,\n",
    "                yticklabels=label_names,\n",
    "                annot=True, fmt='g')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference on an audio file\n",
    "\n",
    "Finally, verify the model's prediction output using an input audio file of someone saying \"no\". How well does your model perform?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_audio(file_path):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path, sr=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process file '{file_path}': {e}\")\n",
    "        return\n",
    "    \n",
    "    # Überprüfen, ob das Audio mehrdimensional ist (z.B. Stereo)\n",
    "    if audio.ndim > 1:\n",
    "        waveform = tf.squeeze(audio)\n",
    "    else:\n",
    "        waveform = audio \n",
    "\n",
    "    # Berechne die Anzahl der Samples, die eine Sekunde darstellen\n",
    "    one_second_samples = sample_rate\n",
    "\n",
    "    # Finde die Mitte des Audios und schneide eine Sekunde heraus\n",
    "    mid_point = len(audio) // 2\n",
    "    start = max(0, mid_point - one_second_samples // 2)\n",
    "    end = start + one_second_samples\n",
    "    audio_segment = audio[start:end]\n",
    "\n",
    "    # Überprüfen, ob das Audio mehrdimensional ist (z.B. Stereo)\n",
    "    if audio_segment.ndim > 1:\n",
    "        waveform = tf.squeeze(audio_segment)\n",
    "    else:\n",
    "        waveform = audio_segment\n",
    "\n",
    "    print(f\"Form des Audiosignals: {waveform.shape}\")\n",
    "    print(f\"Sample Rate: {sample_rate}\")\n",
    "\n",
    "    spectrogram = wave_to_spec.get_spectrogram(waveform)\n",
    "    # spectrogram = wave_to_spec.get_mel_spectrogram(waveform)\n",
    "\n",
    "    # Dimension anpassen für das Modell\n",
    "    input_tensor = spectrogram[tf.newaxis, ...]\n",
    "\n",
    "    prediction = model(input_tensor)\n",
    "\n",
    "    x_labels = label_names  \n",
    "\n",
    "    plt.bar(x_labels, tf.nn.softmax(prediction[0]))\n",
    "    plt.title(f'Vorhersage fuer {os.path.basename(file_path)}')\n",
    "    plt.show()\n",
    "\n",
    "    display.display(display.Audio(waveform, rate=sample_rate))\n",
    "\n",
    "def process_directory_for_visualization(directory_path):\n",
    "    # wav_files = glob.glob(os.path.join(directory_path, \"*.wav\"))\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:    \n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Verarbeite Datei: {file_path}\")\n",
    "                visualize_audio(file_path)\n",
    "            else:\n",
    "                print(\"Keine WAV-Dateien im Verzeichnis gefunden.\")\n",
    "                return\n",
    "\n",
    "# extract_zip(TEST_DIR, DATA_DIR)\n",
    "# rename_audio_files(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_Model():\n",
    "    class ExportModel(tf.Module):\n",
    "        def __init__(self, model):\n",
    "            self.model = model\n",
    "\n",
    "            # Accept either a string-filename or a batch of waveforms.\n",
    "            # YOu could add additional signatures for a single wave, or a ragged-batch. \n",
    "            self.__call__.get_concrete_function(\n",
    "                x=tf.TensorSpec(shape=(), dtype=tf.string))\n",
    "            self.__call__.get_concrete_function(\n",
    "                x=tf.TensorSpec(shape=[None, 44100], dtype=tf.float32))\n",
    "\n",
    "\n",
    "        @tf.function\n",
    "        def __call__(self, x):\n",
    "            # If they pass a string, load the file and decode it. \n",
    "            if x.dtype == tf.string:\n",
    "                x = tf.io.read_file(x)\n",
    "                x, _ = tf.audio.decode_wav(x, desired_channels=1, desired_samples=44100,)\n",
    "                x = tf.squeeze(x, axis=-1)\n",
    "                x = x[tf.newaxis, :]\n",
    "            \n",
    "            # x = wave_to_spec.get_mel_spectrogram(x)  \n",
    "            x = wave_to_spec.get_spectrogram(x)  \n",
    "            result = self.model(x, training=False)\n",
    "            \n",
    "            class_ids = tf.argmax(result, axis=-1)\n",
    "            class_names = tf.gather(label_names, class_ids)\n",
    "            return {'predictions':result,\n",
    "                    'class_ids': class_ids,\n",
    "                    'class_names': class_names}\n",
    "            \n",
    "    export = ExportModel(model)\n",
    "    # export(tf.constant(str(data_dir/'no/01bb6a2a_nohash_0.wav')))\n",
    "    export(tf.constant(str(\"data/medium_test_ds/upscale-from-mp3-128/upscale-from-mp3-128_TRAINED_Action Time.wav\")))\n",
    "            \n",
    "    model_save_path = \"saved\"\n",
    "    tf.saved_model.save(export, model_save_path)\n",
    "    model_train_results()\n",
    "    # zip_file_path = \"very_good_model1.zip\"\n",
    "    zip_file_path = f\"saved_models/{adjust_zip_file_path()}.zip\"\n",
    "    with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(model_save_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                zipf.write(file_path, os.path.relpath(file_path, model_save_path))\n",
    "\n",
    "    shutil.rmtree(model_save_path)\n",
    "    return zip_file_path\n",
    "\n",
    "    # imported = tf.saved_model.load(\"saved\")\n",
    "    # imported(waveform[tf.newaxis, :])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(_test_dir, _test_ds, _val_ds, _train_ds, _history, _model, _label_names):\n",
    "    global TEST_DIR, history, model, test_ds, train_ds, val_ds, label_names\n",
    "    \n",
    "    TEST_DIR =_test_dir\n",
    "    history =_history\n",
    "    model = _model\n",
    "    test_ds = _test_ds\n",
    "    val_ds=_val_ds\n",
    "    train_ds =_train_ds\n",
    "    label_names = _label_names\n",
    "    \n",
    "    # model_train_results()\n",
    "    \n",
    "    #Run the model on the test set and check the model's performance\n",
    "    model.evaluate(test_ds, return_dict=True)\n",
    "    \n",
    "    confusion_matrix()\n",
    "    process_directory_for_visualization(TEST_DIR)\n",
    "    path=save_Model()\n",
    "    \n",
    "    return model, path\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
