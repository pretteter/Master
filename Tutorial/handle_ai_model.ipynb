{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from IPython import get_ipython\n",
    "import librosa\n",
    "import zipfile\n",
    "import shutil\n",
    "import visualkeras\n",
    "\n",
    "import ipynb.fs.defs.build_spectogram_ds as wave_to_spec\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "\n",
    "global history\n",
    "global model\n",
    "global test_ds\n",
    "global val_ds\n",
    "global train_ds\n",
    "global label_names\n",
    "global TEST_DIR\n",
    "global SECONDS\n",
    "global session\n",
    "\n",
    "HP_DROPOUT = hp.HParam('dropout')\n",
    "HP_REGULARIZATION = hp.HParam('regularization')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training and validation loss curves to check how your model has improved during training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_zip_file_path(base_name=\"model\", decimals=3):\n",
    "    base_name = f\"{base_name}_({sum(1 for _ in train_ds.unbatch())}-{SECONDS})\"\n",
    "\n",
    "    # val_loss, val_accuracy = model.evaluate(val_ds, verbose=0)\n",
    "    # loss, accuracy = model.evaluate(train_ds, verbose=0)\n",
    "\n",
    "    val_loss, val_accuracy, train_loss, train_accuracy = session.model_values\n",
    "\n",
    "    # Metriken runden\n",
    "    loss_rounded = round(train_loss, decimals)\n",
    "    accuracy_rounded = round(train_accuracy, decimals)\n",
    "    val_loss_rounded = round(val_loss, decimals)\n",
    "    val_accuracy_rounded = round(val_accuracy, decimals)\n",
    "\n",
    "    # Dynamischer Dateiname\n",
    "    return (\n",
    "        f\"{base_name}_loss_{loss_rounded:.{decimals}f}_acc_{accuracy_rounded:.{decimals}f}\"\n",
    "        f\"_val_loss_{val_loss_rounded:.{decimals}f}_val_acc_{val_accuracy_rounded:.{decimals}f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_results():\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Extrahieren der Daten aus dem Session-Objekt\n",
    "    history = session.history\n",
    "    metrics = history.history\n",
    "    \n",
    "    # Zugriff auf EarlyStopping Callback und Best-Weight Epoche\n",
    "    early_stopping = session.callbacks[1]  # EarlyStopping Callback\n",
    "    best_epoch = early_stopping.best_epoch  # Epoche des besten Modells (mit restore_best_weights)\n",
    "    \n",
    "    batch_size = session.model_batch_size\n",
    "    \n",
    "    epochs = np.array(history.epoch)\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    # Plot für den Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, metrics[\"loss\"], label=f\"Train Loss {session.model_values[2]:.3f}\")\n",
    "    plt.plot(epochs, metrics[\"val_loss\"], label=f\"Val Loss {session.model_values[0]:.3f}\")\n",
    "\n",
    "    # Vertikale Linie bei der Best-Weights Epoche\n",
    "    if best_epoch is not None:\n",
    "        plt.axvline(\n",
    "            x=best_epoch,  # 1-basierte Epoche\n",
    "            color=\"green\", \n",
    "            linestyle=\"--\",\n",
    "            label=f\"Best Weights Epoch {best_epoch}\"\n",
    "        )\n",
    "\n",
    "    print(session.best_model_values)  \n",
    "\n",
    "    # Überprüfen, ob 'session.best_model_values' nicht None ist\n",
    "    if session.best_model_values is not None:\n",
    "        # Initialisiere die Variablen mit 'N/A'\n",
    "        dropout_value = 'N/A'\n",
    "        regularization_value = 'N/A'\n",
    "\n",
    "        # Iteriere über das Dictionary, um den richtigen Wert für dropout und regularization zu extrahieren\n",
    "        for param, value in session.best_model_values.items():\n",
    "            if param.name == 'dropout':\n",
    "                dropout_value = value  # Der Wert von dropout\n",
    "            elif param.name == 'regularization':\n",
    "                regularization_value = value  # Der Wert von regularization\n",
    "    else:\n",
    "        dropout_value = 'N/A'\n",
    "        regularization_value = 'N/A'\n",
    "\n",
    "    # Überprüfe die Ausgaben\n",
    "    print(f\"dropout: {dropout_value}, regularization: {regularization_value}\")\n",
    "\n",
    "    # Jetzt korrektes Anzeigen im Plot\n",
    "    plt.figtext(\n",
    "        0.5, 0.025,\n",
    "        f\"Batch Size: {session.model_batch_size} || \"\n",
    "        f\"Training with: {'Mel_Spectogram' if mel_spectogram else 'Spectogram'} || \"\n",
    "        f\"HParams: {'Default' if session.best_model_values is None else f'dropout: {dropout_value}, regularization: {regularization_value}'}\",\n",
    "        fontsize=10, ha=\"center\", va=\"bottom\", color=\"black\"\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylim([0, max(plt.ylim())])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss [CrossEntropy]\")\n",
    "\n",
    "    # Plot für die Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, 100 * np.array(metrics[\"accuracy\"]), label=f\"Train Accuracy {session.model_values[3]:.3f}\")\n",
    "    plt.plot(epochs, 100 * np.array(metrics[\"val_accuracy\"]), label=f\"Val Accuracy {session.model_values[1]:.3f}\")\n",
    "\n",
    "    # Vertikale Linie bei der Best-Weights Epoche\n",
    "    if best_epoch is not None:\n",
    "        plt.axvline(\n",
    "            x=best_epoch,  # 1-basierte Epoche\n",
    "            color=\"green\", \n",
    "            linestyle=\"--\",\n",
    "            label=f\"Best Weights Epoch {best_epoch}\"\n",
    "        )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 100])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy [%]\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./saved/{adjust_zip_file_path(base_name='history')}.png\")\n",
    "    plt.savefig(f\"./saved_models/{adjust_zip_file_path(base_name='history')}.png\")\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def confusion_matrix():\n",
    "    y_pred = model.predict(test_ds)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "    y_true = tf.concat(list(test_ds.map(lambda s, lab: lab)), axis=0)\n",
    "\n",
    "    confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        confusion_mtx,\n",
    "        xticklabels=label_names,\n",
    "        yticklabels=label_names,\n",
    "        annot=True,\n",
    "        fmt=\"g\",\n",
    "    )\n",
    "    plt.xlabel(\"Prediction\")\n",
    "    plt.ylabel(\"Label\")\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference on an audio file\n",
    "\n",
    "Finally, verify the model's prediction output using an input audio file of someone saying \"no\". How well does your model perform?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def visualize_audio(file_path):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path, sr=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process file '{file_path}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Überprüfen, ob das Audio mehrdimensional ist (z.B. Stereo)\n",
    "    if audio.ndim > 1:\n",
    "        waveform = tf.squeeze(audio)\n",
    "    else:\n",
    "        waveform = audio\n",
    "\n",
    "    # Berechne die Anzahl der Samples, die eine Sekunde darstellen\n",
    "    one_second_samples = sample_rate\n",
    "\n",
    "    # Finde die Mitte des Audios und schneide eine Sekunde heraus\n",
    "    mid_point = len(audio) // 2\n",
    "    start = max(0, mid_point - one_second_samples // 2)\n",
    "    end = start + one_second_samples\n",
    "    audio_segment = audio[start:end]\n",
    "\n",
    "    # Überprüfen, ob das Audio mehrdimensional ist (z.B. Stereo)\n",
    "    if audio_segment.ndim > 1:\n",
    "        waveform = tf.squeeze(audio_segment)\n",
    "    else:\n",
    "        waveform = audio_segment\n",
    "\n",
    "    print(f\"Form des Audiosignals: {waveform.shape}\")\n",
    "    print(f\"Sample Rate: {sample_rate}\")\n",
    "\n",
    "    spectrogram = wave_to_spec.get_spectrogram(waveform)\n",
    "\n",
    "    # Dimension anpassen für das Modell\n",
    "    input_tensor = spectrogram[tf.newaxis, ...]\n",
    "\n",
    "    prediction = model(input_tensor)\n",
    "\n",
    "    x_labels = label_names\n",
    "\n",
    "    plt.bar(x_labels, tf.nn.softmax(prediction[0]))\n",
    "    plt.title(f\"Vorhersage fuer {os.path.basename(file_path)}\")\n",
    "    plt.show()\n",
    "\n",
    "    display.display(display.Audio(waveform, rate=sample_rate))\n",
    "\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def process_directory_for_visualization(directory_path):\n",
    "    # wav_files = glob.glob(os.path.join(directory_path, \"*.wav\"))\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Verarbeite Datei: {file_path}\")\n",
    "                visualize_audio(file_path)\n",
    "            else:\n",
    "                print(\"Keine WAV-Dateien im Verzeichnis gefunden.\")\n",
    "                return\n",
    "\n",
    "\n",
    "# extract_zip(TEST_DIR, DATA_DIR)\n",
    "# rename_audio_files(DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def save_Model():\n",
    "    class ExportModel(tf.Module):\n",
    "        def __init__(self, model, label_names, history):\n",
    "            self.model = model\n",
    "            self.label_names = label_names\n",
    "            self.history = history\n",
    "\n",
    "            # Accept either a string-filename or a batch of waveforms.\n",
    "            # YOu could add additional signatures for a single wave, or a ragged-batch.\n",
    "            self.__call__.get_concrete_function(\n",
    "                x=tf.TensorSpec(shape=(), dtype=tf.string)\n",
    "            )\n",
    "            self.__call__.get_concrete_function(\n",
    "                x=tf.TensorSpec(shape=[None, 44100], dtype=tf.float32)\n",
    "            )\n",
    "        @tf.function\n",
    "        def __call__(self, x):\n",
    "            # If they pass a string, load the file and decode it.\n",
    "            if x.dtype == tf.string:\n",
    "                x = tf.io.read_file(x)\n",
    "                x, _ = tf.audio.decode_wav(\n",
    "                    x,\n",
    "                    desired_channels=1,\n",
    "                    desired_samples=44100,\n",
    "                )\n",
    "                x = tf.squeeze(x, axis=-1)\n",
    "                x = x[tf.newaxis, :]\n",
    "\n",
    "            # x = wave_to_spec.get_mel_spectrogram(x)\n",
    "            x = wave_to_spec.get_spectrogram(x)\n",
    "            result = self.model(x, training=False)\n",
    "\n",
    "            class_ids = tf.argmax(result, axis=-1)\n",
    "            class_names = tf.gather(self.label_names, class_ids)\n",
    "            return {\n",
    "                \"predictions\": result,\n",
    "                \"class_ids\": class_ids,\n",
    "                \"class_names\": class_names,\n",
    "            }\n",
    "\n",
    "    export = ExportModel(model, label_names, history)\n",
    "    # export(tf.constant(str(data_dir/'no/01bb6a2a_nohash_0.wav')))\n",
    "    # export(\n",
    "    #     tf.constant(\n",
    "    #         str(\n",
    "    #             \"data/medium_test_ds/upscale-from-mp3-128/upscale-from-mp3-128_TRAINED_Action Time.wav\"\n",
    "    #         )\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    model_save_path = \"saved\"\n",
    "    tf.saved_model.save(export, model_save_path)\n",
    "    # tf.keras.models.save_model(model, model_save_path)\n",
    "    model_train_results()\n",
    "    visualkeras.layered_view(model, legend=True, show_dimension=True, to_file=f'{model_save_path}/model.png')\n",
    "    # zip_file_path = \"very_good_model1.zip\"\n",
    "    zip_file_path = f\"saved_models/{adjust_zip_file_path()}.zip\"\n",
    "    with zipfile.ZipFile(zip_file_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(model_save_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                zipf.write(file_path, os.path.relpath(file_path, model_save_path))\n",
    "\n",
    "    shutil.rmtree(model_save_path)\n",
    "    return zip_file_path\n",
    "\n",
    "    # imported = tf.saved_model.load(\"saved\")\n",
    "    # imported(waveform[tf.newaxis, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def run(\n",
    "    _test_dir, _test_ds, _val_ds, _train_ds, _session, _label_names, _SECONDS, _mel_spectogram\n",
    "):\n",
    "    global TEST_DIR, history, model, session, test_ds, train_ds, val_ds, label_names, SECONDS, mel_spectogram\n",
    "\n",
    "    TEST_DIR = _test_dir\n",
    "    session = _session\n",
    "    history = session.history\n",
    "    model = session.model\n",
    "    test_ds = _test_ds\n",
    "    val_ds = _val_ds\n",
    "    train_ds = _train_ds\n",
    "    label_names = _label_names\n",
    "    SECONDS = _SECONDS\n",
    "    mel_spectogram = _mel_spectogram\n",
    "\n",
    "    # model_train_results()\n",
    "\n",
    "    # Run the model on the test set and check the model's performance\n",
    "    model.evaluate(test_ds, return_dict=True)\n",
    "\n",
    "    confusion_matrix()\n",
    "    # process_directory_for_visualization(TEST_DIR)\n",
    "    path = save_Model()\n",
    "\n",
    "    return model, path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
