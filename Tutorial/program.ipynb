{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fluF3_oOgkWF"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:16.549129Z",
     "iopub.status.busy": "2024-08-16T07:47:16.548739Z",
     "iopub.status.idle": "2024-08-16T07:47:16.552401Z",
     "shell.execute_reply": "2024-08-16T07:47:16.551835Z"
    },
    "id": "AJs7HHFmg1M9"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "# Simple audio recognition: Recognizing keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNbqmZy0gbyE"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/audio/simple_audio\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/audio/simple_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPfDNFlb66XF"
   },
   "source": [
    "This tutorial demonstrates how to preprocess audio files in the WAV format and build and train a basic [automatic speech recognition](https://en.wikipedia.org/wiki/Speech_recognition) (ASR) model for recognizing ten different words. You will use a portion of the [Speech Commands dataset](https://www.tensorflow.org/datasets/catalog/speech_commands) ([Warden, 2018](https://arxiv.org/abs/1804.03209)), which contains short (one-second or less) audio clips of commands, such as \"down\", \"go\", \"left\", \"no\", \"right\", \"stop\", \"up\" and \"yes\".\n",
    "\n",
    "Real-world speech and audio recognition [systems](https://ai.googleblog.com/search/label/Speech%20Recognition) are complex. But, like [image classification with the MNIST dataset](../quickstart/beginner.ipynb), this tutorial should give you a basic understanding of the techniques involved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go9C3uLL8Izc"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary modules and dependencies. You'll be using `tf.keras.utils.audio_dataset_from_directory` (introduced in TensorFlow 2.10), which helps generate audio classification datasets from directories of `.wav` files. You'll also need [seaborn](https://seaborn.pydata.org) for visualization in this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:16.555919Z",
     "iopub.status.busy": "2024-08-16T07:47:16.555546Z",
     "iopub.status.idle": "2024-08-16T07:47:17.865607Z",
     "shell.execute_reply": "2024-08-16T07:47:17.864292Z"
    },
    "id": "hhNW45sjDEDe"
   },
   "outputs": [],
   "source": [
    "# # Step 1: Install TensorFlow and Datasets\n",
    "# %pip install -U -q tensorflow tensorflow_datasets\n",
    "\n",
    "# # Step 2: Install Wrapt\n",
    "# %pip install wrapt==1.14.1\n",
    "\n",
    "# # Step 3: Install Visualization Libraries\n",
    "# %pip install matplotlib seaborn\n",
    "\n",
    "# # Step 4: Install PySoundFile\n",
    "# %pip install pysoundfile\n",
    "\n",
    "# # Step 5: Reinstall TensorFlow I/O\n",
    "# # !pip uninstall -y tensorflow-io \n",
    "# %pip install tensorflow-io\n",
    "# # %pip install --upgrade tensorflow\n",
    "\n",
    "# %pip install nbformat\n",
    "\n",
    "# # Step 6: Install IPykernel\n",
    "# %pip install ipykernel\n",
    "\n",
    "# %pip install ipynb\n",
    "\n",
    "# %pip install pickleshare\n",
    "\n",
    "# %pip install librosa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import glob\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "# import tensorflow_io as tfio\n",
    "import pathlib\n",
    "from IPython import get_ipython\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "DATA_DIR = pathlib.Path('data')\n",
    "\n",
    "SECONDS = 20\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "# # small Dataset\n",
    "# TRAIN_DIR = pathlib.Path('data/small_train_ds')\n",
    "# TEST_DIR = pathlib.Path('data/small_test_ds')\n",
    "# VAL_DIR = pathlib.Path('data/small_val_ds')\n",
    "\n",
    "# # medium Dataset\n",
    "# TRAIN_DIR = pathlib.Path('data/medium_train_ds')\n",
    "# TEST_DIR = pathlib.Path('data/medium_test_ds')\n",
    "# VAL_DIR = pathlib.Path('data/medium_val_ds')\n",
    "\n",
    "# large Dataset\n",
    "TRAIN_DIR = pathlib.Path('data/large_train_ds')\n",
    "VAL_DIR = pathlib.Path('data/large_val_ds')\n",
    "TEST_DIR = pathlib.Path('data/medium_test_ds')\n",
    "\n",
    "\n",
    "import ipynb.fs.defs.audio_extraction as audio_extraction\n",
    "import ipynb.fs.defs.build_database as build_database\n",
    "import ipynb.fs.defs.build_spectogram_ds as wave_to_spec\n",
    "import ipynb.fs.defs.build_train_model as build_train_model\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"Gefundene GPUs: {gpus}\")\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "else:\n",
    "    print(\"Keine GPUs gefunden, TensorFlow wird auf der CPU ausgefuehrt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notebook_extract():\n",
    "    audio_extraction.extract_zip(TRAIN_DIR, DATA_DIR)\n",
    "    print('-' * 50)  \n",
    "    audio_extraction.extract_zip(TEST_DIR, DATA_DIR)\n",
    "    print('-' * 50)  \n",
    "    audio_extraction.extract_zip(VAL_DIR, DATA_DIR)\n",
    "    print('-' * 50)\n",
    "    print('-' * 50)  \n",
    "    audio_extraction.rename_audio_files(DATA_DIR)\n",
    "    print('-' * 50)\n",
    "    print('-' * 50)  \n",
    "    # audio_extraction.process_directory(TRAIN_DIR) \n",
    "    # audio_extraction.process_directory(VAL_DIR)\n",
    "    # audio_extraction.process_directory(TEST_DIR)  \n",
    "\n",
    "notebook_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = build_database.run(TRAIN_DIR, TEST_DIR, DATA_DIR, VAL_DIR, SECONDS, BATCH_SIZE)\n",
    "train_ds, val_ds, test_ds, label_names = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert waveforms to spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = wave_to_spec.run(label_names, train_ds, val_ds, test_ds)\n",
    "train_spectrogram_ds, val_spectrogram_ds, test_spectrogram_ds = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5KdY8IF8rkt"
   },
   "source": [
    "## Build and train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = build_train_model.run(train_spectrogram_ds, val_spectrogram_ds, test_spectrogram_ds, label_names)\n",
    "train_spectrogram_ds, val_spectrogram_ds, test_spectrogram_ds, model, history = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjpCDeQ4mUfS"
   },
   "source": [
    "Let's plot the training and validation loss curves to check how your model has improved during training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:46.277290Z",
     "iopub.status.busy": "2024-08-16T07:47:46.276985Z",
     "iopub.status.idle": "2024-08-16T07:47:46.551824Z",
     "shell.execute_reply": "2024-08-16T07:47:46.551207Z"
    },
    "id": "nzhipg3Gu2AY"
   },
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "print(metrics)\n",
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "# plt.plot(history.epoch, metrics['loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss [CrossEntropy]')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
    "# plt.plot(history.epoch, 100*np.array(metrics['accuracy']))\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.ylim([0, 100])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy [%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZTt3kO3mfm4"
   },
   "source": [
    "## Evaluate the model performance\n",
    "\n",
    "Run the model on the test set and check the model's performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:46.555333Z",
     "iopub.status.busy": "2024-08-16T07:47:46.555094Z",
     "iopub.status.idle": "2024-08-16T07:47:46.741008Z",
     "shell.execute_reply": "2024-08-16T07:47:46.740155Z"
    },
    "id": "FapuRT_SsWGQ"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_spectrogram_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en9Znt1NOabH"
   },
   "source": [
    "### Display a confusion matrix\n",
    "\n",
    "Use a [confusion matrix](https://developers.google.com/machine-learning/glossary#confusion-matrix) to check how well the model did classifying each of the commands in the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:47.150558Z",
     "iopub.status.busy": "2024-08-16T07:47:47.150278Z",
     "iopub.status.idle": "2024-08-16T07:47:47.738680Z",
     "shell.execute_reply": "2024-08-16T07:47:47.737949Z"
    },
    "id": "LvoSAOiXU3lL"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_spectrogram_ds)\n",
    "y_pred = tf.argmax(y_pred, axis=1)\n",
    "y_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)\n",
    "\n",
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx,\n",
    "            xticklabels=label_names,\n",
    "            yticklabels=label_names,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQGi_mzPcLvl"
   },
   "source": [
    "## Run inference on an audio file\n",
    "\n",
    "Finally, verify the model's prediction output using an input audio file of someone saying \"no\". How well does your model perform?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_audio(file_path):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path, sr=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process file '{file_path}': {e}\")\n",
    "        return\n",
    "    \n",
    "    # Überprüfen, ob das Audio mehrdimensional ist (z.B. Stereo)\n",
    "    if audio.ndim > 1:\n",
    "        waveform = tf.squeeze(audio)\n",
    "    else:\n",
    "        waveform = audio \n",
    "\n",
    "    # Berechne die Anzahl der Samples, die eine Sekunde darstellen\n",
    "    one_second_samples = sample_rate\n",
    "\n",
    "    # Finde die Mitte des Audios und schneide eine Sekunde heraus\n",
    "    mid_point = len(audio) // 2\n",
    "    start = max(0, mid_point - one_second_samples // 2)\n",
    "    end = start + one_second_samples\n",
    "    audio_segment = audio[start:end]\n",
    "\n",
    "    # Überprüfen, ob das Audio mehrdimensional ist (z.B. Stereo)\n",
    "    if audio_segment.ndim > 1:\n",
    "        waveform = tf.squeeze(audio_segment)\n",
    "    else:\n",
    "        waveform = audio_segment\n",
    "\n",
    "    print(f\"Form des Audiosignals: {waveform.shape}\")\n",
    "    print(f\"Sample Rate: {sample_rate}\")\n",
    "\n",
    "    spectrogram = wave_to_spec.get_spectrogram(waveform)\n",
    "    # spectrogram = wave_to_spec.get_mel_spectrogram(waveform)\n",
    "\n",
    "    # Dimension anpassen für das Modell\n",
    "    input_tensor = spectrogram[tf.newaxis, ...]\n",
    "\n",
    "    prediction = model(input_tensor)\n",
    "\n",
    "    x_labels = label_names  \n",
    "\n",
    "    plt.bar(x_labels, tf.nn.softmax(prediction[0]))\n",
    "    plt.title(f'Vorhersage für {os.path.basename(file_path)}')\n",
    "    plt.show()\n",
    "\n",
    "    display.display(display.Audio(waveform, rate=sample_rate))\n",
    "\n",
    "def process_directory_for_visualization(directory_path):\n",
    "    # wav_files = glob.glob(os.path.join(directory_path, \"*.wav\"))\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:    \n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Verarbeite Datei: {file_path}\")\n",
    "                visualize_audio(file_path)\n",
    "            else:\n",
    "                print(\"Keine WAV-Dateien im Verzeichnis gefunden.\")\n",
    "                return\n",
    "\n",
    "# extract_zip(TEST_DIR, DATA_DIR)\n",
    "# rename_audio_files(DATA_DIR)\n",
    "\n",
    "process_directory_for_visualization(TEST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1icqlM3ISW0"
   },
   "source": [
    "## Export the model with preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7HX-MjgIbji"
   },
   "source": [
    "The model's not very easy to use if you have to apply those preprocessing steps before passing data to the model for inference. So build an end-to-end version:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:48.457991Z",
     "iopub.status.busy": "2024-08-16T07:47:48.457717Z",
     "iopub.status.idle": "2024-08-16T07:47:48.464142Z",
     "shell.execute_reply": "2024-08-16T07:47:48.463566Z"
    },
    "id": "2lIeXdWjIbDE"
   },
   "outputs": [],
   "source": [
    "class ExportModel(tf.Module):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "\n",
    "    # Accept either a string-filename or a batch of waveforms.\n",
    "    # YOu could add additional signatures for a single wave, or a ragged-batch. \n",
    "    self.__call__.get_concrete_function(\n",
    "        x=tf.TensorSpec(shape=(), dtype=tf.string))\n",
    "    self.__call__.get_concrete_function(\n",
    "        x=tf.TensorSpec(shape=[None, 44100], dtype=tf.float32))\n",
    "\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, x):\n",
    "    # If they pass a string, load the file and decode it. \n",
    "    if x.dtype == tf.string:\n",
    "      x = tf.io.read_file(x)\n",
    "      x, _ = tf.audio.decode_wav(x, desired_channels=1, desired_samples=44100,)\n",
    "      x = tf.squeeze(x, axis=-1)\n",
    "      x = x[tf.newaxis, :]\n",
    "    \n",
    "    # x = wave_to_spec.get_mel_spectrogram(x)  \n",
    "    x = wave_to_spec.get_spectrogram(x)  \n",
    "    result = self.model(x, training=False)\n",
    "    \n",
    "    class_ids = tf.argmax(result, axis=-1)\n",
    "    class_names = tf.gather(label_names, class_ids)\n",
    "    return {'predictions':result,\n",
    "            'class_ids': class_ids,\n",
    "            'class_names': class_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtZBmUiB9HGY"
   },
   "source": [
    "Test run the \"export\" model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:48.467167Z",
     "iopub.status.busy": "2024-08-16T07:47:48.466892Z",
     "iopub.status.idle": "2024-08-16T07:47:48.770920Z",
     "shell.execute_reply": "2024-08-16T07:47:48.770248Z"
    },
    "id": "Z1_8TYaCIRue"
   },
   "outputs": [],
   "source": [
    "export = ExportModel(model)\n",
    "# export(tf.constant(str(data_dir/'no/01bb6a2a_nohash_0.wav')))\n",
    "export(tf.constant(str(\"data/medium_test_ds/upscale-from-mp3-128/upscale-from-mp3-128_TRAINED_Action Time.wav\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1J6Iuz829Cxo"
   },
   "source": [
    "Save and reload the model, the reloaded model gives identical output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:48.774390Z",
     "iopub.status.busy": "2024-08-16T07:47:48.774117Z",
     "iopub.status.idle": "2024-08-16T07:47:49.775771Z",
     "shell.execute_reply": "2024-08-16T07:47:49.775108Z"
    },
    "id": "wTAg4vsn3oEb"
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(export, \"saved\")\n",
    "# imported = tf.saved_model.load(\"saved\")\n",
    "# imported(waveform[tf.newaxis, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3jF933m9z1J"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "This tutorial demonstrated how to carry out simple audio classification/automatic speech recognition using a convolutional neural network with TensorFlow and Python. To learn more, consider the following resources:\n",
    "\n",
    "- The [Sound classification with YAMNet](https://www.tensorflow.org/hub/tutorials/yamnet) tutorial shows how to use transfer learning for audio classification.\n",
    "- The notebooks from [Kaggle's TensorFlow speech recognition challenge](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/overview).\n",
    "- The\n",
    "  [TensorFlow.js - Audio recognition using transfer learning codelab](https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0) teaches how to build your own interactive web app for audio classification.\n",
    "- [A tutorial on deep learning for music information retrieval](https://arxiv.org/abs/1709.04396) (Choi et al., 2017) on arXiv.\n",
    "- TensorFlow also has additional support for [audio data preparation and augmentation](https://www.tensorflow.org/io/tutorials/audio) to help with your own audio-based projects.\n",
    "- Consider using the [librosa](https://librosa.org/) library for music and audio analysis.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "CPU",
  "colab": {
   "collapsed_sections": [],
   "name": "simple_audio.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
