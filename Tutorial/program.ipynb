{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fluF3_oOgkWF"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:16.549129Z",
     "iopub.status.busy": "2024-08-16T07:47:16.548739Z",
     "iopub.status.idle": "2024-08-16T07:47:16.552401Z",
     "shell.execute_reply": "2024-08-16T07:47:16.551835Z"
    },
    "id": "AJs7HHFmg1M9"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "# Simple audio recognition: Recognizing keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNbqmZy0gbyE"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/audio/simple_audio\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/audio/simple_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPfDNFlb66XF"
   },
   "source": [
    "This tutorial demonstrates how to preprocess audio files in the WAV format and build and train a basic [automatic speech recognition](https://en.wikipedia.org/wiki/Speech_recognition) (ASR) model for recognizing ten different words. You will use a portion of the [Speech Commands dataset](https://www.tensorflow.org/datasets/catalog/speech_commands) ([Warden, 2018](https://arxiv.org/abs/1804.03209)), which contains short (one-second or less) audio clips of commands, such as \"down\", \"go\", \"left\", \"no\", \"right\", \"stop\", \"up\" and \"yes\".\n",
    "\n",
    "Real-world speech and audio recognition [systems](https://ai.googleblog.com/search/label/Speech%20Recognition) are complex. But, like [image classification with the MNIST dataset](../quickstart/beginner.ipynb), this tutorial should give you a basic understanding of the techniques involved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go9C3uLL8Izc"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary modules and dependencies. You'll be using `tf.keras.utils.audio_dataset_from_directory` (introduced in TensorFlow 2.10), which helps generate audio classification datasets from directories of `.wav` files. You'll also need [seaborn](https://seaborn.pydata.org) for visualization in this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall pydot\n",
    "# %pip uninstall graphviz\n",
    "# %pip install pydot\n",
    "# %pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:16.555919Z",
     "iopub.status.busy": "2024-08-16T07:47:16.555546Z",
     "iopub.status.idle": "2024-08-16T07:47:17.865607Z",
     "shell.execute_reply": "2024-08-16T07:47:17.864292Z"
    },
    "id": "hhNW45sjDEDe"
   },
   "outputs": [],
   "source": [
    "# # Step 1: Install TensorFlow and Datasets\n",
    "# %pip install -U -q tensorflow tensorflow_datasets\n",
    "\n",
    "# # Step 2: Install Wrapt\n",
    "# %pip install wrapt==1.14.1\n",
    "\n",
    "# # Step 3: Install Visualization Libraries\n",
    "# %pip install matplotlib seaborn\n",
    "\n",
    "# # Step 4: Install PySoundFile\n",
    "# %pip install pysoundfile\n",
    "\n",
    "# # Step 5: Reinstall TensorFlow I/O\n",
    "# # !pip uninstall -y tensorflow-io \n",
    "# %pip install tensorflow-io\n",
    "# %pip install --upgrade tensorflow\n",
    "\n",
    "# %pip install nbformat\n",
    "\n",
    "# # Step 6: Install IPykernel\n",
    "# %pip install ipykernel\n",
    "\n",
    "# %pip install ipynb\n",
    "\n",
    "# %pip install pickleshare\n",
    "\n",
    "# %pip install librosa\n",
    "\n",
    "# %pip install AudioSegment\n",
    "\n",
    "# %pip install pydot\n",
    "\n",
    "# %pip install graphviz\n",
    "\n",
    "# %pip install \"numpy<2\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "# import tensorflow_io as tfio\n",
    "from IPython import get_ipython\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import seaborn as sns\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "DATA_DIR = pathlib.Path('data')\n",
    "\n",
    "SECONDS = 1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "# # small Dataset\n",
    "# TRAIN_DIR = pathlib.Path('data/small_train_ds')\n",
    "# TEST_DIR = pathlib.Path('data/small_test_ds')\n",
    "# VAL_DIR = pathlib.Path('data/small_val_ds')\n",
    "\n",
    "# # medium Dataset\n",
    "# TRAIN_DIR = pathlib.Path('data/medium_train_ds')\n",
    "# VAL_DIR = pathlib.Path('data/medium_val_ds')\n",
    "# TEST_DIR = pathlib.Path('data/medium_test_ds')\n",
    "\n",
    "\n",
    "# large Dataset\n",
    "TRAIN_DIR = pathlib.Path('data/large_train_ds')\n",
    "VAL_DIR = pathlib.Path('data/large_val_ds')\n",
    "TEST_DIR = pathlib.Path('data/medium_test_ds')\n",
    "\n",
    "# # no_mod Dataset\n",
    "# TRAIN_DIR = pathlib.Path('data/no_mod_train_ds')\n",
    "# VAL_DIR = pathlib.Path('data/no_mod_val_ds')\n",
    "# TEST_DIR = pathlib.Path('data/no_mod_test_ds')\n",
    "\n",
    "\n",
    "import ipynb.fs.defs.audio_extraction as audio_extraction\n",
    "import ipynb.fs.defs.build_database as build_database\n",
    "import ipynb.fs.defs.build_spectogram_ds as wave_to_spec\n",
    "import ipynb.fs.defs.build_train_model as build_train_model\n",
    "import ipynb.fs.defs.handle_ai_model as handle_ai_model\n",
    "\n",
    "\n",
    "# Liste der verfügbaren GPUs abrufen\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    # Überprüfe die Namen der GPUs, um die RTX 4070 zu identifizieren\n",
    "    for gpu in gpus:\n",
    "        print(f\"Gefundene GPU: {gpu.name}\")\n",
    "    \n",
    "\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    \n",
    "    # Speicherwachstum für die RTX 4070 aktivieren\n",
    "    try:\n",
    "        # tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"memory optimized\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Fehler bei der Speicherwachstumseinstellung: {e}\")\n",
    "\n",
    "    print(\"Nur die RTX 4070 wird verwendet.\")\n",
    "else:\n",
    "    print(\"Keine GPUs gefunden, TensorFlow wird auf der CPU ausgeführt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notebook_extract():\n",
    "    audio_extraction.extract_zip(TRAIN_DIR, DATA_DIR)\n",
    "    print('-' * 50)  \n",
    "    audio_extraction.extract_zip(TEST_DIR, DATA_DIR)\n",
    "    print('-' * 50)  \n",
    "    audio_extraction.extract_zip(VAL_DIR, DATA_DIR)\n",
    "    print('-' * 50)\n",
    "    print('-' * 50)  \n",
    "    audio_extraction.rename_audio_files(DATA_DIR)\n",
    "    print('-' * 50)\n",
    "    print('-' * 50)  \n",
    "    # audio_extraction.process_directory(TRAIN_DIR) \n",
    "    # audio_extraction.process_directory(VAL_DIR)\n",
    "    # audio_extraction.process_directory(TEST_DIR)  \n",
    "\n",
    "notebook_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds, label_names = build_database.run(TRAIN_DIR, TEST_DIR, DATA_DIR, VAL_DIR, SECONDS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert waveforms to spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spectrogram_ds, val_spectrogram_ds, test_spectrogram_ds = wave_to_spec.run(label_names, train_ds, val_ds, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5KdY8IF8rkt"
   },
   "source": [
    "## Build and train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_spectrogram_ds, val_spectrogram_ds, test_spectrogram_ds, model, history = build_train_model.run(train_spectrogram_ds, val_spectrogram_ds, test_spectrogram_ds, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle AI Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, path = handle_ai_model.run(TEST_DIR, test_spectrogram_ds, val_spectrogram_ds, train_spectrogram_ds, history, model, label_names, SECONDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipynb.fs.defs.use_model as use_model\n",
    "\n",
    "# use_model.run(path, SECONDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.defs.use_model as use_model\n",
    "from IPython.display import clear_output\n",
    "\n",
    "for i in range(1):\n",
    "    print(f\"Run: {i+1}\")\n",
    "    train_spectrogram_ds, val_spectrogram_ds, test_spectrogram_ds, model, history = build_train_model.run(train_spectrogram_ds, val_spectrogram_ds, test_spectrogram_ds, label_names)\n",
    "    model, path = handle_ai_model.run(TEST_DIR, test_spectrogram_ds, val_spectrogram_ds, train_spectrogram_ds, history, model, label_names, SECONDS)\n",
    "    use_model.run(path, SECONDS)\n",
    "    clear_output(wait=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3jF933m9z1J"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "This tutorial demonstrated how to carry out simple audio classification/automatic speech recognition using a convolutional neural network with TensorFlow and Python. To learn more, consider the following resources:\n",
    "\n",
    "- The [Sound classification with YAMNet](https://www.tensorflow.org/hub/tutorials/yamnet) tutorial shows how to use transfer learning for audio classification.\n",
    "- The notebooks from [Kaggle's TensorFlow speech recognition challenge](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/overview).\n",
    "- The\n",
    "  [TensorFlow.js - Audio recognition using transfer learning codelab](https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0) teaches how to build your own interactive web app for audio classification.\n",
    "- [A tutorial on deep learning for music information retrieval](https://arxiv.org/abs/1709.04396) (Choi et al., 2017) on arXiv.\n",
    "- TensorFlow also has additional support for [audio data preparation and augmentation](https://www.tensorflow.org/io/tutorials/audio) to help with your own audio-based projects.\n",
    "- Consider using the [librosa](https://librosa.org/) library for music and audio analysis.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "CPU",
  "colab": {
   "collapsed_sections": [],
   "name": "simple_audio.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
