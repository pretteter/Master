{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "# import tensorflow_io as tfio  \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "train_spectrogram_ds = None\n",
    "val_spectrogram_ds = None\n",
    "test_spectrogram_ds = None\n",
    "example_spectrograms = None\n",
    "label_names = None\n",
    "\n",
    "\n",
    "\n",
    "model = None\n",
    "history = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `Dataset.cache` and `Dataset.prefetch` operations to reduce read latency while training the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    global train_spectrogram_ds, val_spectrogram_ds, test_spectrogram_ds    \n",
    "    train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
    "    val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    test_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model, you'll use a simple convolutional neural network (CNN), since you have transformed the audio files into spectrogram images.\n",
    "\n",
    "Your `tf.keras.Sequential` model will use the following Keras preprocessing layers:\n",
    "\n",
    "- `tf.keras.layers.Resizing`: to downsample the input to enable the model to train faster.\n",
    "- `tf.keras.layers.Normalization`: to normalize each pixel in the image based on its mean and standard deviation.\n",
    "\n",
    "For the `Normalization` layer, its `adapt` method would first need to be called on the training data in order to compute aggregate statistics (that is, the mean and the standard deviation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def build_model():    \n",
    "    input_shape = example_spectrograms.shape[1:]\n",
    "    print('Input shape:', input_shape)\n",
    "    num_labels = len(label_names)\n",
    "\n",
    "    # Instantiate the `tf.keras.layers.Normalization` layer.\n",
    "    norm_layer = layers.Normalization()\n",
    "    # Fit the state of the layer to the spectrograms\n",
    "    # with `Normalization.adapt`.\n",
    "    norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
    "    global model\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        # Downsample the input.\n",
    "        layers.Resizing(32, 32),\n",
    "        # Normalize.\n",
    "        norm_layer,\n",
    "        layers.Conv2D(32, 3, activation='relu'),\n",
    "        layers.Conv2D(64, 3, activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_labels),\n",
    "    ])\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_GPT(): \n",
    "    input_shape = example_spectrograms.shape[1:]\n",
    "    print('Input shape:', input_shape)\n",
    "    num_labels = len(label_names)\n",
    "\n",
    "    norm_layer = layers.Normalization()\n",
    "    norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
    "    \n",
    "    global model\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Resizing(32, 32),\n",
    "        norm_layer,\n",
    "        layers.Conv2D(32, 3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, 3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Conv2D(128, 3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_labels),\n",
    "    ])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # # Compile the model\n",
    "    # model.compile(\n",
    "    #     optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # Adjusted learning rate\n",
    "    #     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    #     metrics=['accuracy'],\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the Keras model with the Adam optimizer and the cross-entropy loss:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile():\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model over 10 epochs for demonstration purposes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    EPOCHS = 500\n",
    "    global history\n",
    "    \n",
    "    # Berechnung der Anzahl der Trainings- und Validierungsdateien\n",
    "    num_train_files = sum(1 for _ in train_spectrogram_ds.unbatch())  # Unbatch für die Gesamtzahl\n",
    "    num_val_files = sum(1 for _ in val_spectrogram_ds.unbatch())\n",
    "    \n",
    "      # Ausgabe der Anzahl der Dateien\n",
    "    print(f\"Number of training files: {num_train_files}\")\n",
    "    print(f\"Number of validation files: {num_val_files}\")\n",
    "    \n",
    "    class TimeHistory(tf.keras.callbacks.Callback):\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.start_time = time.time()  # Zeit zu Beginn des Trainings\n",
    "            self.epoch_times = []  # Liste zur Speicherung der Epochendauern\n",
    "\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.epoch_start_time = time.time()  # Zeit zu Beginn der Epoche\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            epoch_time = time.time() - self.epoch_start_time  # Zeit für die aktuelle Epoche\n",
    "            self.epoch_times.append(epoch_time)\n",
    "\n",
    "            avg_epoch_time = sum(self.epoch_times) / len(self.epoch_times)\n",
    "\n",
    "            remaining_epochs = self.params['epochs'] - (epoch + 1)\n",
    "            estimated_remaining_time = remaining_epochs * avg_epoch_time\n",
    "\n",
    "            hours, rem = divmod(estimated_remaining_time, 3600)\n",
    "            minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "            print(f\"\\nEpoch {epoch + 1}/{self.params['epochs']} - Estimated time until finished: \"\n",
    "                f\"{int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\")\n",
    "\n",
    "    time_callback = TimeHistory()\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_spectrogram_ds,\n",
    "        validation_data=val_spectrogram_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[time_callback, early_stopping, reduce_lr]\n",
    "        # callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(_train_spectrogram_ds,_val_spectrogram_ds,_test_spectrogram_ds, _example_spectrograms, _label_names):\n",
    "    global train_spectrogram_ds,val_spectrogram_ds,test_spectrogram_ds, example_spectrograms, label_names, model, history\n",
    "    \n",
    "    train_spectrogram_ds = _train_spectrogram_ds\n",
    "    val_spectrogram_ds =_val_spectrogram_ds\n",
    "    test_spectrogram_ds=_test_spectrogram_ds\n",
    "    label_names = _label_names\n",
    "    \n",
    "    example_spectrograms = _example_spectrograms\n",
    "    \n",
    "    init()\n",
    "    build_model()\n",
    "    # build_model_GPT()\n",
    "    compile()\n",
    "    train()\n",
    "    \n",
    "    return train_spectrogram_ds,val_spectrogram_ds,test_spectrogram_ds, example_spectrograms, label_names, model, history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
