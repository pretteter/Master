{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.utils import plot_model\n",
    "# import tensorflow_io as tfio  \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import io\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "from types import SimpleNamespace\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import ipynb.fs.defs.handle_ai_model as handle_ai_model\n",
    "\n",
    "\n",
    "\n",
    "train_spectrogram_ds = None\n",
    "val_spectrogram_ds = None\n",
    "test_spectrogram_ds = None\n",
    "example_spectrograms = None\n",
    "label_names = None\n",
    "\n",
    "\n",
    "\n",
    "model = None\n",
    "history = None\n",
    "session = None\n",
    "\n",
    "EPOCHS = None\n",
    "RUNS_FIND_GOOD_AI = None\n",
    "\n",
    "\n",
    "# Define HParams\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.5))\n",
    "HP_REGULARIZATION = hp.HParam('regularization', hp.RealInterval(0.0001, 0.01))\n",
    "\n",
    "# Log Directory for HParams\n",
    "LOG_DIR = 'logs/hparam_tuning'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_model(number=1, dropout_rate=0.5, regularization_rate=0.001):\n",
    "    input_shape = example_spectrograms.shape[1:]\n",
    "    print('Input shape:', input_shape)\n",
    "    num_labels = len(label_names)\n",
    "    print(f\"num_labels: {num_labels}\")\n",
    "\n",
    "    # Instantiate the `tf.keras.layers.Normalization` layer.\n",
    "    norm_layer = layers.Normalization()\n",
    "    # Fit the state of the layer to the spectrograms\n",
    "    # with `Normalization.adapt`.\n",
    "    norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
    "    \n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        layers.RandomZoom(0.1),\n",
    "        layers.RandomContrast(0.2),\n",
    "    ])\n",
    "    \n",
    "    if number == 1:\n",
    "        model = models.Sequential([\n",
    "            layers.Input(shape=input_shape),\n",
    "            layers.Resizing(32, 32),\n",
    "            norm_layer,\n",
    "            layers.Conv2D(8, 3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Conv2D(16, 3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(32, 3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(64, 3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(num_labels, activation='softmax'),\n",
    "        ])\n",
    "    elif number == 2:\n",
    "        model = models.Sequential([\n",
    "            layers.Input(shape=input_shape),\n",
    "            # data_augmentation,\n",
    "            layers.Resizing(32, 32),\n",
    "            norm_layer,\n",
    "            layers.Conv2D(16, 3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D(),\n",
    "            # layers.Dropout(0.2),\n",
    "            layers.Conv2D(32, 3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D(),\n",
    "            # layers.Dropout(0.3),\n",
    "            layers.Conv2D(64, 3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D(),\n",
    "            # layers.Dropout(0.4),\n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(num_labels, activation='softmax'),\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(f\"Unbekanntes Modell Nummer: {number}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `Dataset.cache` and `Dataset.prefetch` operations to reduce read latency while training the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    global train_spectrogram_ds, val_spectrogram_ds, test_spectrogram_ds\n",
    "    # for example_spectrogram, label in train_spectrogram_ds.take(1):\n",
    "    #     print(\"Beispiel-Mel-Spektrogramm Shape:\", example_spectrogram.shape)\n",
    "    #     print(\"Label:\", label)\n",
    "    # Batching, Caching, Prefetching\n",
    "    train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
    "    val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    test_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model, you'll use a simple convolutional neural network (CNN), since you have transformed the audio files into spectrogram images.\n",
    "\n",
    "Your `tf.keras.Sequential` model will use the following Keras preprocessing layers:\n",
    "\n",
    "- `tf.keras.layers.Resizing`: to downsample the input to enable the model to train faster.\n",
    "- `tf.keras.layers.Normalization`: to normalize each pixel in the image based on its mean and standard deviation.\n",
    "\n",
    "For the `Normalization` layer, its `adapt` method would first need to be called on the training data in order to compute aggregate statistics (that is, the mean and the standard deviation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def build_model():      \n",
    "    global model\n",
    "    model = use_model(1)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the Keras model with the Adam optimizer and the cross-entropy loss:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile():\n",
    "\n",
    "    class WarmUpSchedule(LearningRateSchedule):\n",
    "        def __init__(self, initial_lr, warmup_steps):\n",
    "            self.initial_lr = initial_lr\n",
    "            self.warmup_steps = warmup_steps\n",
    "        \n",
    "        def __call__(self, step):\n",
    "            if step < self.warmup_steps:\n",
    "                return self.initial_lr * (step / self.warmup_steps)\n",
    "            return self.initial_lr\n",
    "\n",
    "    warmup_schedule = WarmUpSchedule(initial_lr=1e-4, warmup_steps=1000)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=warmup_schedule)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),  # Wichtig: from_logits=False\n",
    "        metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_hparams(hparams):\n",
    "    global model\n",
    "    model = use_model(number=1, dropout_rate=hparams[HP_DROPOUT], regularization_rate=hparams[HP_REGULARIZATION])\n",
    "    compile()\n",
    "\n",
    "    # TensorBoard Logging\n",
    "    log_dir = f\"{LOG_DIR}/run-{hparams[HP_DROPOUT]}-{hparams[HP_REGULARIZATION]}\"\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    hparams_cb = hp.KerasCallback(log_dir, hparams)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_spectrogram_ds,\n",
    "        validation_data=val_spectrogram_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[tensorboard_cb, hparams_cb],\n",
    "    )\n",
    "    print(history.history)\n",
    "    param = history.history['accuracy'][-1]\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hparam_tuning():\n",
    "    session_results = []\n",
    "\n",
    "    for dropout_rate in [0.2, 0.3, 0.4, 0.5]:\n",
    "        for regularization_rate in [0.001, 0.005, 0.01]:\n",
    "            hparams = {\n",
    "                HP_DROPOUT: dropout_rate,\n",
    "                HP_REGULARIZATION: regularization_rate,\n",
    "            }\n",
    "            print(f\"Testing HParams: {hparams}\")\n",
    "            param = train_with_hparams(hparams)\n",
    "            session_results.append((hparams, param))\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    print(\"HParam tuning completed.\")\n",
    "    return session_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model over 10 epochs for demonstration purposes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingMultipleMetrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, patience, monitor_metrics=['val_loss', 'val_accuracy']):\n",
    "        super().__init__()\n",
    "        self.patience = patience\n",
    "        self.monitor_metrics = monitor_metrics\n",
    "        self.best_values = {metric: float('inf') for metric in monitor_metrics}  # Initialisiere mit hohen Werten\n",
    "        self.best_weights = None  # FÃ¼r das Speichern der besten Gewichtungen\n",
    "        self.best_epoch = 0  # Speichert die Epoche mit den besten Gewichtungen\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        stop_training = False\n",
    "        for metric in self.monitor_metrics:\n",
    "            current_value = logs.get(metric)\n",
    "            if current_value is None:\n",
    "                continue\n",
    "            if current_value < self.best_values[metric]:\n",
    "                self.best_values[metric] = current_value\n",
    "                self.best_epoch = epoch  # Speichern der aktuellen Epoche\n",
    "                self.wait = 0  # Reset wait, weil eine Verbesserung stattgefunden hat\n",
    "                self.best_weights = self.model.get_weights()  # Speichere die besten Gewichtungen\n",
    "            else:\n",
    "                self.wait += 1\n",
    "\n",
    "            if self.wait >= self.patience:\n",
    "                stop_training = True\n",
    "\n",
    "        if stop_training:\n",
    "            self.model.stop_training = True\n",
    "            # Wiederherstellen der besten Gewichtungen\n",
    "            self.model.set_weights(self.best_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()  \n",
    "        self.epoch_times = [] \n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = time.time() - self.epoch_start_time \n",
    "        self.epoch_times.append(epoch_time)\n",
    "\n",
    "        avg_epoch_time = sum(self.epoch_times) / len(self.epoch_times)\n",
    "\n",
    "        remaining_epochs = self.params['epochs'] - (epoch + 1)\n",
    "        estimated_remaining_time = remaining_epochs * avg_epoch_time\n",
    "\n",
    "        hours, rem = divmod(estimated_remaining_time, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{self.params['epochs']} - Estimated time until finished: \"\n",
    "            f\"{int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def train():\n",
    "    # EPOCHS = 5000\n",
    "    global history\n",
    "    global session\n",
    "    \n",
    "    num_train_files = sum(1 for _ in train_spectrogram_ds.unbatch()) \n",
    "    num_val_files = sum(1 for _ in val_spectrogram_ds.unbatch())\n",
    "    \n",
    "    print(f\"Number of training files: {num_train_files}\")\n",
    "    print(f\"Number of validation files: {num_val_files}\")\n",
    "    \n",
    "    MAX_VAL_ACCURACY = 0.8\n",
    "    MAX_VAL_LOSS = 0.2\n",
    "    MAX_TRAIN_ACCURACY = 0.8\n",
    "    MAX_TRAIN_LOSS = 0.2\n",
    "    \n",
    "    max_runs = RUNS_FIND_GOOD_AI  \n",
    "    run = 0  \n",
    "    \n",
    "    time_callback = TimeHistory()\n",
    "    # early_stopping = EarlyStoppingMultipleMetrics(patience=EPOCHS/4, monitor_metrics=['val_loss', 'val_accuracy'])\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=EPOCHS/4, restore_best_weights=True)\n",
    "    # reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "    \n",
    "    \n",
    "    while run < max_runs:\n",
    "        print(f\"Start of run {run+1}/{max_runs}\")\n",
    "        history = model.fit(\n",
    "            train_spectrogram_ds,\n",
    "            validation_data=val_spectrogram_ds,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[time_callback, early_stopping],\n",
    "            # callbacks=[time_callback, early_stopping, reduce_lr]\n",
    "        )\n",
    "        \n",
    "        trained_epochs = len(history.epoch)\n",
    "        print(f\"Das Training wurde nach {trained_epochs} Epochen gestoppt.\")\n",
    "        best_epoch = early_stopping.best_epoch\n",
    "        print(f\"Das beste Modell wurde in Epoche {best_epoch} gefunden.\")\n",
    "        \n",
    "        \n",
    "        val_loss = history.history['val_loss'][best_epoch]\n",
    "        val_accuracy = history.history['val_accuracy'][best_epoch]\n",
    "        train_loss = history.history['loss'][best_epoch]\n",
    "        train_accuracy = history.history['accuracy'][best_epoch]\n",
    "        \n",
    "        batch_size = None\n",
    "        for element in train_spectrogram_ds.take(1):\n",
    "            batch_size = element[0].shape[0] \n",
    "            break\n",
    "        \n",
    "        session = SimpleNamespace(\n",
    "            model=model,\n",
    "            history=history,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[time_callback, early_stopping],\n",
    "            model_values = [val_loss, val_accuracy, train_loss, train_accuracy],\n",
    "            model_batch_size = batch_size,\n",
    "            best_model_values = None\n",
    "        )\n",
    "    \n",
    "        print(f\"Evaluated model with best weights: val_loss={val_loss}, val_accuracy={val_accuracy}\")\n",
    "    \n",
    "\n",
    "        if train_accuracy > MAX_TRAIN_ACCURACY and train_loss < MAX_TRAIN_LOSS and val_loss < MAX_VAL_LOSS and val_accuracy > MAX_VAL_ACCURACY:\n",
    "            print(f\"Run {run+1} successful with val_loss={val_loss} and val_accuracy={val_accuracy}\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Run {run+1} not successful. val_loss={val_loss}, val_accuracy={val_accuracy} - Restarting training...\")\n",
    "            run += 1\n",
    "    else:\n",
    "        print(\"Maximum number of runs reached. Best model from the last run will be used.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def run(_train_spectrogram_ds,_val_spectrogram_ds,_test_spectrogram_ds, _label_names, _EPOCHS, _RUNS_FIND_GOOD_AI, _model_optimization = False):\n",
    "    global train_spectrogram_ds,val_spectrogram_ds,test_spectrogram_ds, example_spectrograms, label_names, model, history, EPOCHS, RUNS_FIND_GOOD_AI\n",
    "    \n",
    "    train_spectrogram_ds = _train_spectrogram_ds\n",
    "    val_spectrogram_ds =_val_spectrogram_ds\n",
    "    test_spectrogram_ds=_test_spectrogram_ds\n",
    "    label_names = _label_names\n",
    "    EPOCHS = _EPOCHS\n",
    "    RUNS_FIND_GOOD_AI = _RUNS_FIND_GOOD_AI\n",
    "    \n",
    "    for example_spectrograms,_ in train_spectrogram_ds.take(1):\n",
    "        break\n",
    "    \n",
    "    init()\n",
    "    model = use_model(\n",
    "        number=1,\n",
    "        dropout_rate=0.5,\n",
    "        regularization_rate=0.001\n",
    "    )\n",
    "    \n",
    "    if(_model_optimization):\n",
    "        compile()\n",
    "        train()\n",
    "        handle_ai_model.run(pathlib.Path('data/medium_test_ds'), test_spectrogram_ds, val_spectrogram_ds, train_spectrogram_ds, session, label_names, 1, True)\n",
    "        \n",
    "        session_results = hparam_tuning()\n",
    "        best_hparams, best_val_accuracy = sorted(session_results, key=lambda x: x[1], reverse=True)[0]\n",
    "        clear_output(wait=True)\n",
    "        model = use_model(\n",
    "            number=1,\n",
    "            dropout_rate=best_hparams[HP_DROPOUT],\n",
    "            regularization_rate=best_hparams[HP_REGULARIZATION]\n",
    "        )\n",
    "        compile()\n",
    "        train()\n",
    "        session.best_model_values = best_hparams\n",
    "        print(f\"Best HParams for training: {session.best_model_values} with validation accuracy: {best_val_accuracy}\")\n",
    "    else:\n",
    "        compile()\n",
    "        train()\n",
    "\n",
    "\n",
    "    return train_spectrogram_ds,val_spectrogram_ds,test_spectrogram_ds, session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
