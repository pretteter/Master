{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pydot pydot-ng graphviz\n",
    "# %pip install ann_visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shap\n",
    "import gc\n",
    "import zipfile\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from collections import Counter\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_silence\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # Kein Speicherverbrauch für Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from types import SimpleNamespace\n",
    "import traceback\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "from tensorflow.keras.applications import ResNet50, ResNet101, ResNet152\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import pandas as pd\n",
    "\n",
    "import visualkeras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "# from ann_visualizer.visualize import ann_viz\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import csv\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "img_height, img_width = 128, 128\n",
    "BATCH_SIZE = 16\n",
    "epochs = 20\n",
    "stop = False\n",
    "\n",
    "use_resnet = True\n",
    "model_optimization = False\n",
    "\n",
    "use_siamese = False\n",
    "\n",
    "use_shap_values = False\n",
    "\n",
    "check_image = False\n",
    "max_length = 0\n",
    "downsize = True\n",
    "\n",
    "\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.0, 0.5))\n",
    "HP_LEARNING = hp.HParam('learning', hp.RealInterval(0.00005, 0.0005))\n",
    "# HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu', 'tanh', 'sigmoid']))\n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu']))\n",
    "# Log Directory for HParams\n",
    "LOG_DIR = 'logs/hparam_tuning'\n",
    "\n",
    "\n",
    "\n",
    "ROOT_DIR = Path('../').resolve() \n",
    "ZIP_DIR = ROOT_DIR / 'data'  \n",
    "UNZIP_DIR = ROOT_DIR / 'Unzipped_Data_Picture' \n",
    "TEST_DIR = UNZIP_DIR / 'new_test_ds'\n",
    "\n",
    "# # Small Dataset\n",
    "TRAIN_DIR = UNZIP_DIR / 'small_train_ds'\n",
    "VAL_DIR = UNZIP_DIR / 'small_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / 'small_test_ds'\n",
    "\n",
    "# # Small 3 Labels Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / '3_small_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / '3_small_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / '3_small_test_ds'\n",
    "\n",
    "# Medium Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / 'medium_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / 'medium_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / 'medium_test_ds'\n",
    "\n",
    "# # Large Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / 'large_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / 'large_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / 'medium_test_ds'\n",
    "\n",
    "# # Large 3 Labels Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / '3_large_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / '3_large_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / '3_large_test_ds'\n",
    "\n",
    "# # No_mod Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / 'no_mod_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / 'no_mod_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / 'no_mod_test_ds'\n",
    "\n",
    "# New_Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / 'new_large_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / 'new_large_val_ds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip(zip_path, extract_to):\n",
    "    zip_path_str = str(zip_path)\n",
    "    \n",
    "    if not zip_path_str.endswith('.zip'):\n",
    "        zip_path_str += '.zip'\n",
    "    \n",
    "    zip_file_path = pathlib.Path(zip_path_str)\n",
    "    \n",
    "    folder_name = zip_file_path.stem \n",
    "    target_folder = pathlib.Path(extract_to) / folder_name\n",
    "    \n",
    "    if target_folder.exists():\n",
    "        print(f\"Das Verzeichnis {target_folder} existiert bereits. Überspringe das Extrahieren.\")\n",
    "    else:\n",
    "        if zip_file_path.exists():\n",
    "            print(f\"Extrahiere die Zip-Datei {zip_file_path} nach {extract_to}.\")\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_to)\n",
    "            print(f\"Zip-Datei {zip_file_path} erfolgreich extrahiert.\")\n",
    "        else:\n",
    "            print(f\"Die Zip-Datei {zip_file_path} existiert nicht.\")\n",
    "\n",
    "def rename_audio_files(root_path):\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        parent_folder = os.path.basename(root)\n",
    "        for file in files:\n",
    "            if not file.startswith(f\"{parent_folder}_\"):\n",
    "                if file.endswith(('.wav', '.mp3')):  \n",
    "                    \n",
    "                    old_file_path = os.path.join(root, file)\n",
    "                    new_file_name = f\"{parent_folder}_{file}\"\n",
    "                    new_file_path = os.path.join(root, new_file_name)\n",
    "                        \n",
    "                    os.rename(old_file_path, new_file_path)\n",
    "        print(f\"renaming of {root_path}/{parent_folder} complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resize Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_max_length():\n",
    "    global max_length\n",
    "    max_length_local = 0\n",
    "    \n",
    "    for subdir, _, files in os.walk(TRAIN_DIR):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                input_path = Path(subdir) / file\n",
    "                audio = AudioSegment.from_file(input_path)\n",
    "                duration = len(audio)\n",
    "                max_length_local = max(max_length_local, duration)\n",
    "    \n",
    "    max_length = max_length_local\n",
    "\n",
    "def normalize_audio_length(input_dir):\n",
    "    global max_length\n",
    "    output_dir = input_dir.parent / f\"{input_dir.name}_normalized\"\n",
    "    if output_dir.exists():\n",
    "        print(f\"Überspringe Verarbeitung, da {output_dir} bereits existiert.\")\n",
    "        return output_dir\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    audio_files = []\n",
    "    \n",
    "    if max_length == 0:\n",
    "        determine_max_length()\n",
    "    \n",
    "    print(f\"Maximale Länge: {max_length / 1000} Sekunden\")\n",
    "    \n",
    "    for subdir, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                input_path = Path(subdir) / file\n",
    "                audio = AudioSegment.from_file(input_path)\n",
    "                if len(audio) > max_length:\n",
    "                    print(f\"⚠️ {input_path.name} ist länger als {max_length / 1000} Sekunden. Kürze Datei!\")\n",
    "                    audio = audio[:max_length]\n",
    "\n",
    "                padded_audio = audio + AudioSegment.silent(duration=max(0, max_length - len(audio)))\n",
    "                \n",
    "                relative_path = input_path.parent.relative_to(input_dir)\n",
    "                target_dir = output_dir / relative_path\n",
    "                os.makedirs(target_dir, exist_ok=True)\n",
    "                output_path = target_dir / input_path.name\n",
    "                \n",
    "                padded_audio.export(output_path, format=\"wav\")\n",
    "                print(f\"Processed {input_path.name}: expanded to {max_length / 1000} seconds\")\n",
    "    \n",
    "    print(f\"Processing complete. Normalized files saved in {output_dir}\")\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel-Spektogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_file(audio_file, input_dir, output_dir, n_mels, fmin, fmax):\n",
    "    \"\"\"\n",
    "    Diese Funktion verarbeitet eine einzelne Audiodatei und berechnet das Mel-Spektrogramm.\n",
    "    \"\"\"\n",
    "    relative_path = audio_file.relative_to(input_dir)\n",
    "    \n",
    "    target_dir = output_dir / relative_path.parent\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "\n",
    "    y, sr = librosa.load(audio_file, sr=44100)\n",
    "\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
    "\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    return relative_path, mel_spectrogram, sr, target_dir, audio_file.stem\n",
    "\n",
    "def generate_mel_spectrograms_with_structure(input_dir, output_dir, n_mels=256, fmin=20, fmax=44100, batch_size=25, square = False):\n",
    "    \"\"\"\n",
    "    Optimierte Funktion für die Verarbeitung von Mel-Spektrogrammen:\n",
    "    1. Berechnung wird parallelisiert.\n",
    "    2. Ergebnisse werden sequentiell geplottet, um Thread-Sicherheitsprobleme zu vermeiden.\n",
    "    3. Batches werden verwendet, um den Speicherverbrauch zu kontrollieren.\n",
    "    \"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "\n",
    "    if output_dir.exists() and any(output_dir.rglob(\"*.png\")):\n",
    "        print(f\"Überspringe Verarbeitung, da {output_dir} bereits Mel-Spektrogramme enthält.\")\n",
    "        return\n",
    "\n",
    "    audio_files = list(input_dir.rglob(\"*.wav\"))\n",
    "\n",
    "    if not audio_files:\n",
    "        print(\"Keine Audiodateien gefunden.\")\n",
    "        return\n",
    "\n",
    "    total_files = len(audio_files)\n",
    "    print(f\"{total_files} Audiodateien gefunden. Verarbeitung startet.\")\n",
    "\n",
    "    for batch_start in range(0, total_files, batch_size):\n",
    "        batch_files = audio_files[batch_start:batch_start + batch_size]\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        gc.collect()\n",
    "        print(f\"Verarbeite Batch {batch_start // batch_size + 1} von {total_files // batch_size + 1}\")\n",
    "\n",
    "        results = []\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [\n",
    "                executor.submit(process_audio_file, audio_file, input_dir, output_dir, n_mels, fmin, fmax)\n",
    "                for audio_file in batch_files\n",
    "            ]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                results.append(future.result())\n",
    "\n",
    "        for relative_path, mel_spectrogram_db, sr, target_dir, audio_file_stem in results:\n",
    "            mel_spectrogram_path = target_dir / f\"{audio_file_stem}_mel_spectrogram.png\"\n",
    "\n",
    "            if mel_spectrogram_path.exists():\n",
    "                print(f\"Spektrogramm {mel_spectrogram_path} existiert bereits. Überspringen.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if square:\n",
    "                    plt.figure(figsize=(2, 2))\n",
    "                    librosa.display.specshow(mel_spectrogram_db, x_axis='time', y_axis='mel', sr=sr, cmap='magma', fmin=fmin, fmax=fmax)\n",
    "                    plt.axis('off')\n",
    "\n",
    "                    plt.savefig(mel_spectrogram_path, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "                    plt.close()\n",
    "                    print(f\"{batch_start // batch_size + 1} von {total_files // batch_size + 1}__Mel-Spektrogramm für {audio_file_stem} gespeichert in {mel_spectrogram_path}\")\n",
    "                else:\n",
    "                    height = 333 if downsize else 2000  \n",
    "                    width = height * 30  \n",
    "                    dpi = 100  \n",
    "                    figsize = (width / dpi, height / dpi)\n",
    "\n",
    "                    fig, ax = plt.subplots(figsize=figsize, dpi=dpi, frameon=False)\n",
    "\n",
    "                    librosa.display.specshow(mel_spectrogram_db, x_axis='time', y_axis='mel', sr=sr, cmap='magma', fmin=fmin, fmax=fmax)\n",
    "                    ax.set_axis_off()\n",
    "\n",
    "                    plt.savefig(mel_spectrogram_path, bbox_inches='tight', pad_inches=0, dpi=dpi)\n",
    "                    plt.close(fig)\n",
    "                    print(f\"{batch_start // batch_size + 1} von {total_files // batch_size + 1}__Mel-Spektrogramm für {audio_file_stem} gespeichert in {mel_spectrogram_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Plotten von {audio_file_stem}: {e}\")\n",
    "            finally:\n",
    "                del mel_spectrogram_db, fig, ax\n",
    "\n",
    "    print(f\"Alle Mel-Spektrogramme gespeichert in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_spectrogram(image_path, output_dir):\n",
    "    \"\"\"\n",
    "    Schneidet ein Spektrogramm in gleich große Quadrate.\n",
    "    :param image_path: Pfad zum Spektrogramm (PNG)\n",
    "    :param output_dir: Ordner zum Speichern der Segmente\n",
    "    :param segment_size: Größe jedes quadratischen Segments (Standard: 924x924)\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trash_dir = output_dir.parent.parent / f\"{output_dir.parent.name}_trash\"\n",
    "    os.makedirs(trash_dir, exist_ok=True)\n",
    "\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "    segment_size= height\n",
    "\n",
    "    num_segments = width // segment_size -1\n",
    "    for i in range(1, num_segments):\n",
    "        left = i * segment_size\n",
    "        right = left + segment_size\n",
    "        segment = img.crop((left, 0, right, segment_size))\n",
    "        segment_array = np.array(segment)\n",
    "        \n",
    "        silence_threshold = 10  \n",
    "        silence_ratio = np.mean(segment_array < silence_threshold)\n",
    "\n",
    "        transparency_ratio = 0\n",
    "        if segment.mode == 'RGBA':\n",
    "            alpha_channel = segment_array[:, :, 3] \n",
    "            transparency_ratio = np.mean(alpha_channel == 0)  \n",
    "\n",
    "        output_path = Path(output_dir) / f\"{Path(image_path).stem}_part{i}.png\"\n",
    "        if silence_ratio >= 0.7 or transparency_ratio > 0.2:\n",
    "            output_path = trash_dir / f\"{Path(image_path).stem}_part{i}.png\"\n",
    "            print(f\"Segment {i} enthält {silence_ratio * 100:.2f}% Stille und wird in den Trash {output_path} verschoben.\")\n",
    "        \n",
    "        try:\n",
    "            segment.save(output_path)\n",
    "        except IOError:\n",
    "            print(f\"Fehler beim Speichern des Segments: {output_path}\")\n",
    "            continue\n",
    "\n",
    "    \n",
    "    print(f\"Spektrogramm in {num_segments} Segmente geschnitten und gespeichert in {output_dir}\")\n",
    "\n",
    "def process_spectrograms(input_dir):\n",
    "    \"\"\"\n",
    "    Durchsucht rekursiv alle Unterordner eines Verzeichnisses nach Spektrogrammen und schneidet sie in Segmente.\n",
    "    Die Ordnerstruktur des Eingabeverzeichnisses wird im Ausgabeordner beibehalten.\n",
    "    :param input_dir: Verzeichnis mit Unterordnern, die Spektrogramme enthalten\n",
    "    :param output_dir: Verzeichnis zum Speichern der Segmente\n",
    "    \"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = input_dir.parent / f\"{input_dir.name}_splits\"\n",
    "    if output_dir.exists():\n",
    "        print(f\"Output directory already exists: {output_dir}. Skipping splitting.\")\n",
    "        return output_dir\n",
    "    \n",
    "    for subdir in input_dir.iterdir():\n",
    "        if subdir.is_dir(): \n",
    "            for image_path in subdir.glob(\"*.png\"):  \n",
    "                relative_path = subdir.relative_to(input_dir)\n",
    "                target_dir = output_dir / relative_path\n",
    "                try:\n",
    "                    img = Image.open(image_path)\n",
    "                    img.verify()\n",
    "                    split_spectrogram(image_path, target_dir) \n",
    "                except (IOError, SyntaxError):\n",
    "                    print(f\"Fehler: Beschädigtes oder ungültiges Bild übersprungen: {image_path}\")\n",
    "                    continue\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"Hilfsfunktion zum Laden und Vorverarbeiten eines Bildes\"\"\"\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, (256, 256)) \n",
    "    img = img / 255.0  # Normalisierung\n",
    "    return img\n",
    "\n",
    "def create_siamese_dataset(directory, batch_size=32):\n",
    "    \"\"\" Erstellt ein TensorFlow Dataset mit dynamischer Paarbildung für das Siamese Network \"\"\"\n",
    "    \n",
    "    image_paths = list(Path(directory).glob(\"*.png\"))\n",
    "    \n",
    "    paired_data = {}\n",
    "    for path in image_paths:\n",
    "        base_name = re.sub(r'(upscale-from-mp3-128|orig-16-44-mono)_', '', path.stem) \n",
    "        \n",
    "        if base_name not in paired_data:\n",
    "            paired_data[base_name] = {\"original\": None, \"upscaled\": None}\n",
    "        \n",
    "        if \"orig-16-44-mono\" in path.stem: \n",
    "            paired_data[base_name][\"original\"] = str(path)\n",
    "        elif \"upscale-from-mp3-128\" in path.stem:  \n",
    "            paired_data[base_name][\"upscaled\"] = str(path)\n",
    "\n",
    "    pairs = []\n",
    "    for data in paired_data.values():\n",
    "        if data[\"original\"] and data[\"upscaled\"]:\n",
    "            pairs.append((load_image(data[\"original\"]), load_image(data[\"upscaled\"])))\n",
    "\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(pairs)\n",
    "\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def build_siamese_model(input_shape=(128, 128, 3)):\n",
    "    \"\"\" Erstellt ein Siamese CNN mit Normalisierung \"\"\"\n",
    "    print(\"use siamese model\")\n",
    "    norm_layer = layers.Normalization()\n",
    "\n",
    "    base_model = models.Sequential([\n",
    "        layers.Conv2D(64, 3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu')\n",
    "    ])\n",
    "    print(base_model)\n",
    "    input1 = layers.Input(shape=input_shape)\n",
    "    input2 = layers.Input(shape=input_shape)\n",
    "\n",
    "    encoded1 = base_model(input1)\n",
    "    encoded2 = base_model(input2)\n",
    "\n",
    "    distance = layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))([encoded1, encoded2])\n",
    "\n",
    "    output = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "    model = models.Model([input1, input2], output)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Contrastive Loss\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    \"\"\" Contrastive Loss Funktion \"\"\"\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "\n",
    "\n",
    "def build_resnet_model(input_shape=(128, 128, 3), num_classes=2, dropout_rate=0.2, learning_rate=0.00005, activation='relu', fine_tune_at=None):\n",
    "\n",
    "    print(\"use Resnet_Model\")\n",
    "    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    base_model.trainable = True\n",
    "\n",
    "    \n",
    "    # Optional: freeze base-model\n",
    "    if fine_tune_at is not None:\n",
    "        for layer in base_model.layers[:fine_tune_at]:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    # add classification header\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)  \n",
    "    x = Dropout(dropout_rate)(x)  \n",
    "    x = Dense(512, activation=activation)(x)  \n",
    "    x = Dropout(dropout_rate)(x)  \n",
    "    predictions = Dense(num_classes, activation='softmax',  dtype='float32')(x)  \n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def calculate_shap_values(model, test_ds, num_samples=1, num_images_to_explain=1):\n",
    "    \"\"\"\n",
    "    Berechnet Shapley-Werte für eine angegebene Anzahl von Bildern aus dem Testdatensatz.\n",
    "\n",
    "    Args:\n",
    "        model: Das zu erklärende Modell.\n",
    "        test_ds: Der Testdatensatz.\n",
    "        num_samples: Anzahl der Batches, die aus dem Testdatensatz genommen werden.\n",
    "        num_images_to_explain: Anzahl der Bilder, für die Shapley-Werte berechnet werden sollen.\n",
    "\n",
    "    Returns:\n",
    "        shap_values_list: Liste der Shapley-Werte für jedes Bild.\n",
    "        test_images_list: Liste der Testbilder, für die die Shapley-Werte berechnet wurden.\n",
    "    \"\"\"\n",
    "    test_images = []\n",
    "    for images, _ in test_ds.take(num_samples):\n",
    "        test_images.append(images)\n",
    "    test_images = np.concatenate(test_images, axis=0)\n",
    "\n",
    "    if num_images_to_explain > test_images.shape[0]:\n",
    "        print(f\"Nur {test_images.shape[0]} Bilder verfügbar, anstatt {num_images_to_explain}.\")\n",
    "        num_images_to_explain = test_images.shape[0]\n",
    "\n",
    "    shap_values_list = []\n",
    "    test_images_list = []\n",
    "\n",
    "    for i in range(num_images_to_explain):\n",
    "        test_image = test_images[i:i+1] \n",
    "\n",
    "        explainer = shap.GradientExplainer(model, test_image)\n",
    "\n",
    "        shap_values = explainer.shap_values(test_image)\n",
    "\n",
    "        shap_values_list.append(shap_values)\n",
    "        test_images_list.append(test_image)\n",
    "        print(\"Shapley-Werte Form:\", shap_values[0].shape)\n",
    "        print(\"Bild Form:\", test_image.shape)\n",
    "\n",
    "    return shap_values_list, test_images_list\n",
    "\n",
    "def plot_shap_values(shap_values_list, test_images_list):\n",
    "    \"\"\"\n",
    "    Visualisiert die Shapley-Werte für mehrere Bilder.\n",
    "\n",
    "    Args:\n",
    "        shap_values_list: Liste der Shapley-Werte für jedes Bild.\n",
    "        test_images_list: Liste der Testbilder, die visualisiert werden sollen.\n",
    "    \"\"\"\n",
    "    for i, (shap_values, test_image) in enumerate(zip(shap_values_list, test_images_list)):\n",
    "        print(f\"Shapley-Werte für Bild {i+1}:\")\n",
    "        test_image = test_image / 255.0\n",
    "        shap.image_plot(shap_values, test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_results(session):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    history = session.history\n",
    "    metrics = history.history\n",
    "    \n",
    "    early_stopping = session.callbacks[0]  \n",
    "    best_epoch = early_stopping.best_epoch  \n",
    "    \n",
    "    batch_size = session.model_batch_size\n",
    "    \n",
    "    epochs = np.array(history.epoch)\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, metrics[\"loss\"], label=f\"Train Loss {session.model_values[2]:.3f}\")\n",
    "    plt.plot(epochs, metrics[\"val_loss\"], label=f\"Val Loss {session.model_values[0]:.3f}\")\n",
    "\n",
    "    if best_epoch is not None:\n",
    "        plt.axvline(\n",
    "            x=best_epoch,  \n",
    "            color=\"green\", \n",
    "            linestyle=\"--\",\n",
    "            label=f\"Best Weights Epoch {best_epoch}\"\n",
    "        )\n",
    "\n",
    "    print(session.best_model_values)  \n",
    "\n",
    "    if session.best_model_values is not None:\n",
    "        dropout_value = 'N/A'\n",
    "        learning_value = 'N/A'\n",
    "        activation_function = 'N/A'\n",
    "\n",
    "        for param, value in session.best_model_values.items():\n",
    "            if param.name == 'dropout':\n",
    "                dropout_value = value \n",
    "            elif param.name == 'learning':\n",
    "                learning_value = value  \n",
    "            elif param.name == 'activation':\n",
    "                activation_function = value\n",
    "    else:\n",
    "        dropout_value = 'N/A'\n",
    "        learning_value = 'N/A'\n",
    "        activation_function = 'N/A'\n",
    "\n",
    "    print(f\"dropout: {dropout_value}, learning: {learning_value}, activation: {activation_function}\")\n",
    "\n",
    "    plt.subplots_adjust(bottom=0.65)\n",
    "    plt.figtext(\n",
    "        0.5, 0.01,\n",
    "        f\"Batch Size: {session.model_batch_size}\\n\"\n",
    "        f\"HParams: {'Default' if session.best_model_values is None else f'dropout: {dropout_value}, learning: {learning_value}, activation: {activation_function}'}\",\n",
    "        fontsize=8, ha=\"center\", va=\"bottom\", color=\"black\"\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylim([0, max(plt.ylim())])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss [CrossEntropy]\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, 100 * np.array(metrics[\"accuracy\"]), label=f\"Train Accuracy {session.model_values[3]:.3f}\")\n",
    "    plt.plot(epochs, 100 * np.array(metrics[\"val_accuracy\"]), label=f\"Val Accuracy {session.model_values[1]:.3f}\")\n",
    "\n",
    "    if best_epoch is not None:\n",
    "        plt.axvline(\n",
    "            x=best_epoch,  \n",
    "            color=\"green\", \n",
    "            linestyle=\"--\",\n",
    "            label=f\"Best Weights Epoch {best_epoch}\"\n",
    "        )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 100])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy [%]\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_hparams(train_ds, val_ds, hparams):\n",
    "    model = build_resnet_model(input_shape=(img_height, img_width, 3), num_classes=2, fine_tune_at=100, dropout_rate=hparams[HP_DROPOUT], learning_rate=hparams[HP_LEARNING], activation = hparams[HP_ACTIVATION])\n",
    "\n",
    "\n",
    "    # TensorBoard Logging\n",
    "    log_dir = f\"{LOG_DIR}/run-{hparams[HP_DROPOUT]}-{hparams[HP_LEARNING]}-{hparams[HP_ACTIVATION]}\"\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    hparams_cb = hp.KerasCallback(log_dir, hparams)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=[tensorboard_cb, hparams_cb],\n",
    "    )\n",
    "    print(history.history)\n",
    "    param = history.history['accuracy'][-1]\n",
    "    return param\n",
    "\n",
    "def hparam_tuning(train_ds, val_ds):\n",
    "    session_results = []\n",
    "\n",
    "    for activation in HP_ACTIVATION.domain.values:\n",
    "        for dropout_rate in [0.2, 0.3, 0.4]:\n",
    "            for learning_rate in [0.0001, 0.00025, 0.0005]:\n",
    "                hparams = {\n",
    "                    HP_DROPOUT: dropout_rate,\n",
    "                    HP_LEARNING: learning_rate,\n",
    "                    HP_ACTIVATION: activation,\n",
    "                }\n",
    "                print(f\"Testing HParams: {hparams}\")\n",
    "                param = train_with_hparams(train_ds, val_ds, hparams)\n",
    "                session_results.append((hparams, param))\n",
    "                clear_output(wait=True)\n",
    "\n",
    "    print(\"HParam tuning completed.\")\n",
    "    return session_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_session_as_zip(session, train_ds, zip_dir=\"model_results\"):\n",
    "\n",
    "    os.makedirs(zip_dir, exist_ok=True)\n",
    "\n",
    "    num_files = sum(1 for _ in train_ds) * session.model_batch_size  \n",
    "\n",
    "    val_loss, val_acc, train_loss, train_acc = session.model_values\n",
    "    val_loss, val_acc = round(val_loss, 3), round(val_acc, 3)\n",
    "    train_loss, train_acc = round(train_loss, 3), round(train_acc, 3)\n",
    "\n",
    "    name_to_save = f\"({num_files}_{img_height}px-{'resnet_model'if use_resnet else 'own_model'})_loss_{train_loss}_acc_{train_acc}_val_loss_{val_loss}_val_acc_{val_acc}\"\n",
    "\n",
    "    zip_filename = f\"model_{name_to_save}.zip\"\n",
    "    zip_path = os.path.join(zip_dir, zip_filename)\n",
    "\n",
    "    model_path = os.path.join(zip_dir, \"model.h5\")\n",
    "    session.model.save(model_path)\n",
    "\n",
    "    plot = model_train_results(session)\n",
    "    plot_path = os.path.join(zip_dir, f\"history_{name_to_save}.png\")\n",
    "    plot.savefig(plot_path)\n",
    "    model_image_path = f\"{zip_dir}/model.png\"\n",
    "    visualkeras.layered_view(session.model, legend=True, show_dimension=True, to_file=model_image_path)\n",
    "    model_image_path_2 = f\"{zip_dir}/model_2.png\"\n",
    "    plot_model(session.model, to_file=model_image_path_2, show_shapes=True, show_layer_names=True, expand_nested=True, dpi=96)\n",
    "\n",
    "    plot.show()\n",
    "    plot.close()\n",
    "\n",
    "\n",
    "    session_path = os.path.join(zip_dir, \"session_data.json\")\n",
    "\n",
    "    print(\"History wurde als JSON und CSV gespeichert.\")\n",
    "    \n",
    "    max_length = 0\n",
    "    for subdir, _, files in os.walk(TRAIN_DIR):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                input_path = Path(subdir) / file\n",
    "                audio = AudioSegment.from_file(input_path)\n",
    "                duration = len(audio)\n",
    "                max_length = max(max_length, duration)\n",
    "        \n",
    "\n",
    "        \n",
    "    session_data = {\n",
    "        \"model_values\": session.model_values,\n",
    "        \"model_batch_size\": session.model_batch_size,\n",
    "        \"best_model_values\": {str(k): v for k, v in session.best_model_values.items()} if session.best_model_values else None,\n",
    "        \"history\": session.history.history,\n",
    "        \"softmax_values\": [[[float(v) for v in sample] for sample in epoch] for epoch in session.softmax_values], \n",
    "        \"max_length\": max_length,\n",
    "        \"img_height\": img_height\n",
    "    }\n",
    "    print(\"save\")\n",
    "    print(session.best_model_values)\n",
    "    print(session_data[\"best_model_values\"])\n",
    "    with open(session_path, \"w\") as f:\n",
    "        json.dump(session_data, f, indent=4, default=lambda o: float(o) if isinstance(o, np.float32) else o)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"w\") as zipf:\n",
    "        zipf.write(model_path, arcname=\"model.h5\")\n",
    "        zipf.write(plot_path, arcname=f\"history_{name_to_save}.png\")\n",
    "        zipf.write(session_path, arcname=\"session_data.json\")\n",
    "        zipf.write(model_image_path, arcname=\"model.png\")\n",
    "        zipf.write(model_image_path_2, arcname=\"model_2.png\")\n",
    "\n",
    "    os.remove(model_path)\n",
    "    os.remove(plot_path)\n",
    "    os.remove(session_path)\n",
    "    os.remove(model_image_path)\n",
    "    os.remove(model_image_path_2)\n",
    "\n",
    "    print(f\"Session-Daten erfolgreich in {zip_path} gespeichert.\")\n",
    "    return zip_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, dataset, num_samples=5):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.num_samples = num_samples\n",
    "        self.softmax_history = [] \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        softmax_values_epoch = []\n",
    "        print(f\"\\nSoftmax-Werte nach Epoche {epoch+1}:\")\n",
    "        for images, labels in self.dataset.take(1):\n",
    "            predictions = self.model.predict(images[:self.num_samples], verbose=0)\n",
    "            softmax_values = tf.nn.softmax(predictions).numpy()\n",
    "            for i in range(self.num_samples):\n",
    "                softmax_values_epoch.append(softmax_values[i]) \n",
    "                print(f\"Sample {i+1}: {softmax_values[i]} (Label: {labels[i].numpy()})\")\n",
    "            break\n",
    "        self.softmax_history.append(softmax_values_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_size_from_dir(directory):\n",
    "    \"\"\"Liest die Bildgröße aus der ersten Datei im Verzeichnis.\"\"\"\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    return img.size[::-1]\n",
    "    raise ValueError(\"Keine Bilder im Verzeichnis gefunden!\")\n",
    "\n",
    "def preprocess_data(train_dir, val_dir, test_dir, dropout_rate=0.2, regularization_rate=0.0001, activation='relu'):\n",
    "    global img_height, img_width\n",
    "    img_height, img_width = get_image_size_from_dir(train_dir)\n",
    "    print(f\"Ermittelte Bildgröße: {img_height} x {img_width}\")\n",
    "\n",
    "    if not use_siamese:\n",
    "        train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            train_dir,\n",
    "            image_size=(img_height, img_width),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            val_dir,\n",
    "            image_size=(img_height, img_width),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            test_dir,\n",
    "            image_size=(img_height, img_width),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        train_ds = create_siamese_dataset(train_dir, batch_size=BATCH_SIZE)\n",
    "        val_ds = create_siamese_dataset(val_dir, batch_size=BATCH_SIZE)\n",
    "        test_ds = create_siamese_dataset(test_dir, batch_size=BATCH_SIZE)\n",
    "        print(\"siamese DS Created\")\n",
    "\n",
    "    # Preprocessing\n",
    "    train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    for batch in train_ds.take(1):\n",
    "        images, labels = batch\n",
    "        print(images.numpy().min(), images.numpy().max()) \n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def build_model(input_shape=(128, 128, 3), num_classes=2, dropout_rate=0.2, regularization_rate=0.0001, activation='relu'):\n",
    "\n",
    "    print(f\"num_labels: {num_classes}\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "        \n",
    "\n",
    "    print(\"Build Model with:\")\n",
    "    print (f\"dropout_rate = {dropout_rate}\")\n",
    "    print (f\"regularization_rate = {regularization_rate}\")\n",
    "    print (f\"activation = {activation}\")\n",
    "    print(f\"input_shape: {input_shape}\")\n",
    "    print(\"\")\n",
    "    norm_layer = layers.Normalization()\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        norm_layer,\n",
    "        layers.Conv2D(16, 3, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(32, 3, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "\n",
    "        layers.Conv2D(64, 3, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(128, 3, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "\n",
    "        layers.Dense(128, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(num_classes, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_and_result():\n",
    "\n",
    "    train_ds, val_ds, test_ds = preprocess_data(train_mel_dir, val_mel_dir, test_mel_dir)\n",
    "\n",
    "    if use_resnet:\n",
    "        if model_optimization:\n",
    "            session_results = hparam_tuning(train_ds, val_ds)\n",
    "            best_hparams, best_val_accuracy = sorted(session_results, key=lambda x: x[1], reverse=True)[0]\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Results: {session_results}\")\n",
    "            print(f\"Best Params: {best_hparams}\")\n",
    "            print(f\"Best val_acc: {best_val_accuracy}\")\n",
    "\n",
    "            model = build_resnet_model(input_shape=(img_height, img_width, 3), num_classes=2, fine_tune_at=100, dropout_rate=best_hparams[HP_DROPOUT], learning_rate=best_hparams[HP_LEARNING], activation = best_hparams[HP_ACTIVATION])\n",
    "        else:\n",
    "            model = build_resnet_model(input_shape=(img_height, img_width, 3), num_classes=2, fine_tune_at=100)\n",
    "    else:\n",
    "        if use_siamese:\n",
    "            model = build_siamese_model(input_shape=(img_height, img_width, 3))\n",
    "            print(\"builded siamese model\")\n",
    "            model.compile(loss=contrastive_loss, optimizer='adam', metrics=['accuracy'])\n",
    "            print(\"compiled\")\n",
    "        else:\n",
    "            model = build_model(input_shape=(img_height, img_width, 3), num_classes=2)\n",
    "        \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=(epochs // 4) if stop else epochs , restore_best_weights=True)\n",
    "    softmax_logger = SoftmaxLogger(val_ds, num_samples=5)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_accuracy', \n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=0.00001 \n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping,  softmax_logger, reduce_lr]\n",
    "    )\n",
    "\n",
    "    trained_epochs = len(history.epoch)\n",
    "    print(f\"Das Training wurde nach {trained_epochs} Epochen gestoppt.\")\n",
    "    best_epoch = early_stopping.best_epoch\n",
    "    print(f\"Das beste Modell wurde in Epoche {best_epoch} gefunden.\")\n",
    "        \n",
    "        \n",
    "    val_loss = history.history['val_loss'][best_epoch]\n",
    "    val_accuracy = history.history['val_accuracy'][best_epoch]\n",
    "    train_loss = history.history['loss'][best_epoch]\n",
    "    train_accuracy = history.history['accuracy'][best_epoch]\n",
    "\n",
    "    batch_size = None\n",
    "    for element in train_ds.take(1):\n",
    "        batch_size = element[0].shape[0]\n",
    "        break\n",
    "        \n",
    "    session = SimpleNamespace(\n",
    "        model=model,\n",
    "        history=history,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping],\n",
    "        model_values = [val_loss, val_accuracy, train_loss, train_accuracy],\n",
    "        model_batch_size = batch_size,\n",
    "        best_model_values = None,\n",
    "        softmax_values=softmax_logger.softmax_history, \n",
    "        img_height = img_height\n",
    "    )\n",
    "    if(model_optimization and best_hparams is not None):\n",
    "        session.best_model_values = best_hparams\n",
    "        print(f\"Best HParams for training: {session.best_model_values} with validation accuracy: {best_val_accuracy}\")\n",
    "\n",
    "    path = save_session_as_zip(session, train_ds)\n",
    "    print(f\"Evaluated model with best weights: val_loss={val_loss}, val_accuracy={val_accuracy}\")\n",
    "\n",
    "    if use_shap_values:\n",
    "        print(\"calculate_shap_values\")\n",
    "        if downsize is None:\n",
    "            print(\"pictures > 256px => abbord\")\n",
    "            return path, session, test_ds\n",
    "        shap_values_list, test_images_list = calculate_shap_values(model, test_ds, num_samples=1, num_images_to_explain=3)\n",
    "        plot_shap_values(shap_values_list, test_images_list)\n",
    "\n",
    "\n",
    "    return path, session, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_images(directory):\n",
    "    print(f\"check images in: {directory}\")\n",
    "    trash_dir = os.path.join(directory, '..', 'trash') \n",
    "    \n",
    "    os.makedirs(trash_dir, exist_ok=True)\n",
    "\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.lower().endswith('.png'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                try:\n",
    "                    with Image.open(file_path) as img:\n",
    "                        img.verify()\n",
    "                except (IOError, SyntaxError):\n",
    "                    print(f\"Fehler bei {filename}, verschiebe nach trash\")\n",
    "                    trash_file_path = os.path.join(trash_dir, os.path.relpath(file_path, directory))\n",
    "                    os.makedirs(os.path.dirname(trash_file_path), exist_ok=True)\n",
    "                    shutil.move(file_path, trash_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_segments(output_dir):\n",
    "    \"\"\"\n",
    "    Diese Funktion vergleicht Mel-Spektrogramme aus zwei verschiedenen Label-Unterordnern, \n",
    "    indem sie die ersten fünf Dateien paarweise nach ihrem Index abgleicht und als Grafik anzeigt.\n",
    "\n",
    "    :param output_dir: Verzeichnis, das die Label-Unterordner mit Mel-Spektrogrammen enthält\n",
    "    \"\"\"\n",
    "\n",
    "    output_dir = Path(output_dir)\n",
    "    label_dirs = [d for d in output_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "    if len(label_dirs) < 2:\n",
    "        raise ValueError(\"Es müssen mindestens zwei Label-Unterordner vorhanden sein.\")\n",
    "\n",
    "    label_dirs.sort()\n",
    "\n",
    "    files_per_label = [sorted(label_dir.glob(\"*.png\")) for label_dir in label_dirs]\n",
    "\n",
    "    pairs = list(zip(*[files[::30] for files in files_per_label]))[:5]\n",
    "\n",
    "    for idx, (file1, file2) in enumerate(pairs):\n",
    "        img1 = plt.imread(file1)\n",
    "        img2 = plt.imread(file2)\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img1)\n",
    "        plt.title(f\"Label 1: {file1.stem[:80]}...\", fontsize=6)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(img2)\n",
    "        plt.title(f\"Label 2: {file2.stem[:80]}...\", fontsize=6) \n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Vergleich {idx + 1}\")\n",
    "        plt.show()\n",
    "        plt.savefig(f\"./compare/{idx + 1}_compare.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_predictions(model, test_ds, max_images=None):\n",
    "    results = []\n",
    "    class_names = test_ds.class_names if hasattr(test_ds, \"class_names\") else [str(i) for i in range(len(test_ds))]\n",
    "    \n",
    "    if max_images is not None:\n",
    "        test_ds = test_ds.shuffle(buffer_size=1000) \n",
    "\n",
    "    image_count = 0\n",
    "    for images, labels in test_ds:\n",
    "        predictions = model.predict(images)\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            if max_images is not None and image_count >= max_images:\n",
    "                break\n",
    "            true_label = class_names[labels[i].numpy()]\n",
    "            predicted_label = class_names[predicted_labels[i]]\n",
    "            results.append({\n",
    "                'True Class': true_label,\n",
    "                'Predicted Class': predicted_label\n",
    "            })\n",
    "            image_count += 1\n",
    "\n",
    "        if max_images is not None and image_count >= max_images:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_summary(results):\n",
    "    total_predictions = len(results)\n",
    "    correct_predictions = sum(1 for result in results if result['True Class'] == result['Predicted Class']) \n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "    summary_data = {\n",
    "        'Total Correct': f\"{correct_predictions} / {total_predictions} ({accuracy:.2f}%)\",\n",
    "        'Classwise Accuracy': {}\n",
    "    }\n",
    "\n",
    "    for label in set(result['True Class'] for result in results):\n",
    "        correct_for_class = sum(1 for result in results if result['True Class'] == label and result['True Class'] == result['Predicted Class'])\n",
    "        total_for_class = sum(1 for result in results if result['True Class'] == label)\n",
    "        class_accuracy = (correct_for_class / total_for_class * 100) if total_for_class > 0 else 0\n",
    "        summary_data['Classwise Accuracy'][label] = f\"{class_accuracy:.2f}%\"\n",
    "\n",
    "    return summary_data\n",
    "\n",
    "def display_summary(summary, save_path=\"model_results\"):\n",
    "    summary_data = [\n",
    "        [\"Total Correct\", summary[\"Total Correct\"]],\n",
    "    ]\n",
    "    \n",
    "    for label, accuracy in summary['Classwise Accuracy'].items():\n",
    "        summary_data.append([f\"{label} Accuracy\", accuracy])\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data, columns=[\"Metric\", \"Value\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 2))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "\n",
    "    table = ax.table(cellText=summary_df.values, colLabels=summary_df.columns, cellLoc=\"center\", loc=\"center\", colWidths=[0.5, 0.5])\n",
    "    \n",
    "    plt.show()\n",
    "    table_filename = \"summary_table.png\"\n",
    "    table_path = os.path.join(save_path, table_filename)\n",
    "    fig.savefig(table_path, bbox_inches='tight', pad_inches=0.1)\n",
    "    print(f\"Table saved as {table_path}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "def visualize_and_summarize_predictions(model, test_ds, max_images=10):\n",
    "\n",
    "    results = collect_predictions(model, test_ds, max_images=max_images)\n",
    "    \n",
    "    summary = generate_summary(results)\n",
    "    \n",
    "    display_summary(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    extract_zip(TRAIN_DIR, UNZIP_DIR)\n",
    "    extract_zip(TEST_DIR, UNZIP_DIR)\n",
    "    extract_zip(VAL_DIR, UNZIP_DIR)\n",
    "    rename_audio_files(UNZIP_DIR)\n",
    "\n",
    "    TRAIN_DIR=normalize_audio_length(TRAIN_DIR)\n",
    "    VAL_DIR=normalize_audio_length(VAL_DIR)\n",
    "    TEST_DIR=normalize_audio_length(TEST_DIR)\n",
    "\n",
    "\n",
    "    train_mel_dir = Path(f\"{TRAIN_DIR.stem}_mel_spectrograms\" + (\"_downsize\" if downsize else \"\"))  # Der Zielordner für die Mel-Spektrogramme\n",
    "    generate_mel_spectrograms_with_structure(TRAIN_DIR.resolve(), train_mel_dir)\n",
    "    train_mel_dir = process_spectrograms(train_mel_dir)\n",
    "\n",
    "    val_mel_dir = Path(f\"{VAL_DIR.stem}_mel_spectrograms\" + (\"_downsize\" if downsize else \"\"))  # Der Zielordner für die Mel-Spektrogramme\n",
    "    generate_mel_spectrograms_with_structure(VAL_DIR, val_mel_dir)\n",
    "    val_mel_dir = process_spectrograms(val_mel_dir)\n",
    "\n",
    "    test_mel_dir = Path(f\"{TEST_DIR.stem}_mel_spectrograms\" + (\"_downsize\" if downsize else \"\"))  # Der Zielordner für die Mel-Spektrogramme\n",
    "    generate_mel_spectrograms_with_structure(TEST_DIR, test_mel_dir)\n",
    "    test_mel_dir = process_spectrograms(test_mel_dir)\n",
    "\n",
    "    if check_image:\n",
    "        check_images(train_mel_dir)\n",
    "        check_images(val_mel_dir)\n",
    "        check_images(test_mel_dir)\n",
    "\n",
    "\n",
    "    # import ipynb.fs.defs.Programm_audioToPicture as old_Programm\n",
    "\n",
    "    # wav_dir = Path(\"../picture_model/compare/orig_vs_split\")\n",
    "    # wav_files = list(wav_dir.glob(\"*.wav\"))\n",
    "\n",
    "    # for wav_file in wav_files:\n",
    "    #     splits_dir = wav_dir / f\"{wav_file.stem}_splits\"\n",
    "    #     print(f\"wav_file: {wav_file}\")\n",
    "    #     old_Programm.process_and_compare_spectrograms(wav_file, splits_dir, input_file=Path(wav_file))\n",
    "\n",
    "\n",
    "    # old_Programm.process_and_compare_spectrograms(\"../picture_model/compare/orig_vs_split/orig-16-44-mono_Burglar Bob.wav\", \"../picture_model/compare/orig_vs_split/splits/\")\n",
    "    # compare_segments(train_mel_dir)\n",
    "\n",
    "    path, session, test_ds = train_and_result()\n",
    "\n",
    "    # Funktion ausführen\n",
    "    # visualize_and_summarize_predictions(session.model, test_ds)\n",
    "\n",
    "    import ipynb.fs.defs.use_model_picture as use_model\n",
    "    use_model.run(path, downsize)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda_Enviroment_Backup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
