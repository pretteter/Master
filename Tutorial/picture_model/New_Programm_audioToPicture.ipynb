{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pydot pydot-ng graphviz\n",
    "# %pip install ann_visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shap\n",
    "import gc\n",
    "import zipfile\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from collections import Counter\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_silence\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # Kein Speicherverbrauch für Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from types import SimpleNamespace\n",
    "import traceback\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "from tensorflow.keras.applications import ResNet50, ResNet101, ResNet152\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "import pandas as pd\n",
    "\n",
    "import visualkeras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "# from ann_visualizer.visualize import ann_viz\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "# from tensorflow.keras.mixed_precision import set_global_policy\n",
    "# set_global_policy('mixed_float16')  # Nutzt FP16 statt FP32 für Berechnungen\n",
    "\n",
    "\n",
    "img_height, img_width = 128, 128\n",
    "BATCH_SIZE = 16\n",
    "epochs = 20\n",
    "stop = False\n",
    "use_resnet = True\n",
    "use_siamese = False\n",
    "use_shap_values = False\n",
    "check_image = False\n",
    "max_length = 0\n",
    "\n",
    "# Globale Pfade für die Daten\n",
    "ROOT_DIR = Path('../').resolve()  # Hauptverzeichnis\n",
    "ZIP_DIR = ROOT_DIR / 'data'  # Ordner, der die ZIP-Dateien enthält\n",
    "UNZIP_DIR = ROOT_DIR / 'Unzipped_Data_Picture'  # Zielordner für entpackte Dateien\n",
    "TEST_DIR = UNZIP_DIR / 'new_test_ds'\n",
    "\n",
    "# # Small Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / 'small_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / 'small_val_ds'\n",
    "# # TEST_DIR = UNZIP_DIR / 'small_test_ds'\n",
    "\n",
    "# # Small 3 Labels Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / '3_small_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / '3_small_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / '3_small_test_ds'\n",
    "\n",
    "# Medium Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / 'medium_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / 'medium_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / 'medium_test_ds'\n",
    "\n",
    "# # Large Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / 'large_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / 'large_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / 'medium_test_ds'\n",
    "\n",
    "# # Large 3 Labels Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / '3_large_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / '3_large_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / '3_large_test_ds'\n",
    "\n",
    "# # No_mod Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / 'no_mod_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / 'no_mod_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / 'no_mod_test_ds'\n",
    "\n",
    "# New_Dataset\n",
    "TRAIN_DIR = UNZIP_DIR / 'new_large_train_ds'\n",
    "VAL_DIR = UNZIP_DIR / 'new_large_val_ds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip(zip_path, extract_to):\n",
    "    zip_path_str = str(zip_path)\n",
    "    \n",
    "    if not zip_path_str.endswith('.zip'):\n",
    "        zip_path_str += '.zip'\n",
    "    \n",
    "    zip_file_path = pathlib.Path(zip_path_str)\n",
    "    \n",
    "    folder_name = zip_file_path.stem \n",
    "    target_folder = pathlib.Path(extract_to) / folder_name\n",
    "    \n",
    "    if target_folder.exists():\n",
    "        print(f\"Das Verzeichnis {target_folder} existiert bereits. Überspringe das Extrahieren.\")\n",
    "    else:\n",
    "        if zip_file_path.exists():\n",
    "            print(f\"Extrahiere die Zip-Datei {zip_file_path} nach {extract_to}.\")\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_to)\n",
    "            print(f\"Zip-Datei {zip_file_path} erfolgreich extrahiert.\")\n",
    "        else:\n",
    "            print(f\"Die Zip-Datei {zip_file_path} existiert nicht.\")\n",
    "\n",
    "def rename_audio_files(root_path):\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        parent_folder = os.path.basename(root)\n",
    "        for file in files:\n",
    "            if not file.startswith(f\"{parent_folder}_\"):\n",
    "                if file.endswith(('.wav', '.mp3')):  \n",
    "                    \n",
    "                    old_file_path = os.path.join(root, file)\n",
    "                    new_file_name = f\"{parent_folder}_{file}\"\n",
    "                    new_file_path = os.path.join(root, new_file_name)\n",
    "                        \n",
    "                    os.rename(old_file_path, new_file_path)\n",
    "        print(f\"renaming of {root_path}/{parent_folder} complete\")\n",
    "\n",
    "\n",
    "# extract_zip(TRAIN_DIR, UNZIP_DIR)\n",
    "# extract_zip(TEST_DIR, UNZIP_DIR)\n",
    "# extract_zip(VAL_DIR, UNZIP_DIR)\n",
    "# rename_audio_files(UNZIP_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resize Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_audio_files(input_dir, min_length_sec=100):\n",
    "    min_length_ms = min_length_sec * 1000  # Umrechnung in Millisekunden\n",
    "    \n",
    "    output_dir = input_dir.parent / f\"{input_dir.name}_trimmed\"\n",
    "    # Überprüfen, ob die Ordnerstruktur bereits existiert\n",
    "    if output_dir.exists():\n",
    "        print(f\"Überspringe Verarbeitung, da {output_dir} bereits existiert.\")\n",
    "        return output_dir\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for subdir, _, files in os.walk(input_dir):\n",
    "        relative_path = Path(subdir).relative_to(input_dir)  # Beibehaltung der Verzeichnisstruktur\n",
    "        target_dir = output_dir / relative_path\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):  # Falls andere Formate unterstützt werden sollen, hier anpassen\n",
    "                input_path = Path(subdir) / file\n",
    "                audio = AudioSegment.from_file(input_path)\n",
    "                duration = len(audio)  # Länge der Audiodatei in Millisekunden\n",
    "                \n",
    "                if duration < min_length_ms:\n",
    "                    print(f\"Skipping {file}: shorter than {min_length_sec} seconds\")\n",
    "                    continue  # Datei ignorieren, wenn sie zu kurz ist\n",
    "                \n",
    "                trimmed_audio = audio[:min_length_ms]  # Schneide auf genau die Mindestlänge\n",
    "                output_path = target_dir / file\n",
    "                trimmed_audio.export(output_path, format=\"wav\")\n",
    "                print(f\"Processed {file}: trimmed to {min_length_sec} seconds\")\n",
    "    \n",
    "    print(f\"Processing complete. Trimmed files saved in {output_dir}\")\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_max_length():\n",
    "    global max_length\n",
    "    max_length_local = 0\n",
    "    \n",
    "    for subdir, _, files in os.walk(TRAIN_DIR):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                input_path = Path(subdir) / file\n",
    "                audio = AudioSegment.from_file(input_path)\n",
    "                duration = len(audio)\n",
    "                max_length_local = max(max_length_local, duration)\n",
    "    \n",
    "    max_length = max_length_local\n",
    "\n",
    "def normalize_audio_length(input_dir):\n",
    "    global max_length\n",
    "    output_dir = input_dir.parent / f\"{input_dir.name}_normalized\"\n",
    "    if output_dir.exists():\n",
    "        print(f\"Überspringe Verarbeitung, da {output_dir} bereits existiert.\")\n",
    "        return output_dir\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    audio_files = []\n",
    "    \n",
    "    # Falls max_length noch nicht bestimmt wurde, berechne es von TRAIN_DIR\n",
    "    if max_length == 0:\n",
    "        determine_max_length()\n",
    "    \n",
    "    print(f\"Maximale Länge: {max_length / 1000} Sekunden\")\n",
    "    \n",
    "    # Normalisiere alle Dateien auf die maximale Länge\n",
    "    for subdir, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                input_path = Path(subdir) / file\n",
    "                audio = AudioSegment.from_file(input_path)\n",
    "                if len(audio) > max_length:\n",
    "                    print(f\"⚠️ {input_path.name} ist länger als {max_length / 1000} Sekunden. Kürze Datei!\")\n",
    "                    audio = audio[:max_length]  # Trimme das Audio auf max_length\n",
    "\n",
    "                padded_audio = audio + AudioSegment.silent(duration=max(0, max_length - len(audio)))\n",
    "                \n",
    "                relative_path = input_path.parent.relative_to(input_dir)\n",
    "                target_dir = output_dir / relative_path\n",
    "                os.makedirs(target_dir, exist_ok=True)\n",
    "                output_path = target_dir / input_path.name\n",
    "                \n",
    "                padded_audio.export(output_path, format=\"wav\")\n",
    "                print(f\"Processed {input_path.name}: expanded to {max_length / 1000} seconds\")\n",
    "    \n",
    "    print(f\"Processing complete. Normalized files saved in {output_dir}\")\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel-Spektogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_file(audio_file, input_dir, output_dir, n_mels, fmin, fmax):\n",
    "    \"\"\"\n",
    "    Diese Funktion verarbeitet eine einzelne Audiodatei und berechnet das Mel-Spektrogramm.\n",
    "    \"\"\"\n",
    "    # Relativer Pfad zur Eingabedatei\n",
    "    relative_path = audio_file.relative_to(input_dir)\n",
    "    \n",
    "    # Zielpfad basierend auf der ursprünglichen Ordnerstruktur\n",
    "    target_dir = output_dir / relative_path.parent\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Lade die Audiodatei mit librosa\n",
    "    y, sr = librosa.load(audio_file, sr=44100)\n",
    "\n",
    "    # Berechne das Mel-Spektrogramm\n",
    "    # padding = 1024\n",
    "    # y = np.pad(y, (padding, padding), mode='constant')\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
    "\n",
    "    # Konvertiere das Mel-Spektrogramm in dB (logarithmische Skala)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Rückgabe der Daten ohne das Plotten\n",
    "    return relative_path, mel_spectrogram, sr, target_dir, audio_file.stem\n",
    "\n",
    "def generate_mel_spectrograms_with_structure(input_dir, output_dir, n_mels=256, fmin=20, fmax=44100, batch_size=25, square = False):\n",
    "    \"\"\"\n",
    "    Optimierte Funktion für die Verarbeitung von Mel-Spektrogrammen:\n",
    "    1. Berechnung wird parallelisiert.\n",
    "    2. Ergebnisse werden sequentiell geplottet, um Thread-Sicherheitsprobleme zu vermeiden.\n",
    "    3. Batches werden verwendet, um den Speicherverbrauch zu kontrollieren.\n",
    "    \"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "\n",
    "    # Überprüfen, ob die Ordnerstruktur bereits existiert\n",
    "    if output_dir.exists() and any(output_dir.rglob(\"*.png\")):\n",
    "        print(f\"Überspringe Verarbeitung, da {output_dir} bereits Mel-Spektrogramme enthält.\")\n",
    "        return\n",
    "\n",
    "    # Liste der .wav-Dateien im input_dir\n",
    "    audio_files = list(input_dir.rglob(\"*.wav\"))\n",
    "\n",
    "    if not audio_files:\n",
    "        print(\"Keine Audiodateien gefunden.\")\n",
    "        return\n",
    "\n",
    "    total_files = len(audio_files)\n",
    "    print(f\"{total_files} Audiodateien gefunden. Verarbeitung startet.\")\n",
    "\n",
    "    # Verarbeite die Dateien in Batches\n",
    "    for batch_start in range(0, total_files, batch_size):\n",
    "        batch_files = audio_files[batch_start:batch_start + batch_size]\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        gc.collect()\n",
    "        print(f\"Verarbeite Batch {batch_start // batch_size + 1} von {total_files // batch_size + 1}\")\n",
    "\n",
    "        # Parallele Berechnung der Mel-Spektrogramme\n",
    "        results = []\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [\n",
    "                executor.submit(process_audio_file, audio_file, input_dir, output_dir, n_mels, fmin, fmax)\n",
    "                for audio_file in batch_files\n",
    "            ]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                results.append(future.result())\n",
    "\n",
    "        # Sequentielles Plotten und Speichern\n",
    "        for relative_path, mel_spectrogram_db, sr, target_dir, audio_file_stem in results:\n",
    "            mel_spectrogram_path = target_dir / f\"{audio_file_stem}_mel_spectrogram.png\"\n",
    "\n",
    "            # Überspringen, wenn das Spektrogramm bereits existiert\n",
    "            if mel_spectrogram_path.exists():\n",
    "                print(f\"Spektrogramm {mel_spectrogram_path} existiert bereits. Überspringen.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Erstelle das Bild des Mel-Spektrogramms\n",
    "                if square:\n",
    "                    plt.figure(figsize=(2, 2))\n",
    "                    librosa.display.specshow(mel_spectrogram_db, x_axis='time', y_axis='mel', sr=sr, cmap='magma', fmin=fmin, fmax=fmax)\n",
    "                    plt.axis('off')\n",
    "\n",
    "                # Speichern des Bildes als PNG\n",
    "                    plt.savefig(mel_spectrogram_path, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "                    plt.close()\n",
    "                    print(f\"{batch_start // batch_size + 1} von {total_files // batch_size + 1}__Mel-Spektrogramm für {audio_file_stem} gespeichert in {mel_spectrogram_path}\")\n",
    "                else:\n",
    "                    height = 2000  # Höhe in Pixel\n",
    "                    width = height * 30  # Breite als Vielfaches der Höhe\n",
    "                    dpi = 100  # Auflösung\n",
    "                    figsize = (width / dpi, height / dpi)\n",
    "\n",
    "                    # Erstelle die Figure mit exakt berechneter Größe\n",
    "                    fig, ax = plt.subplots(figsize=figsize, dpi=dpi, frameon=False)\n",
    "\n",
    "                    # Spektrogramm anzeigen\n",
    "                    librosa.display.specshow(mel_spectrogram_db, x_axis='time', y_axis='mel', sr=sr, cmap='magma', fmin=fmin, fmax=fmax)\n",
    "                    ax.set_axis_off()\n",
    "\n",
    "                    # Speichern des Bildes ohne Padding oder Verzerrung\n",
    "                    plt.savefig(mel_spectrogram_path, bbox_inches='tight', pad_inches=0, dpi=dpi)\n",
    "                    plt.close(fig)\n",
    "                    print(f\"{batch_start // batch_size + 1} von {total_files // batch_size + 1}__Mel-Spektrogramm für {audio_file_stem} gespeichert in {mel_spectrogram_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Plotten von {audio_file_stem}: {e}\")\n",
    "            finally:\n",
    "                # Speicher freigeben\n",
    "                del mel_spectrogram_db, fig, ax\n",
    "                # gc.collect()\n",
    "                # print(\"batch complete\")\n",
    "\n",
    "    print(f\"Alle Mel-Spektrogramme gespeichert in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_spectrogram(image_path, output_dir):\n",
    "    \"\"\"\n",
    "    Schneidet ein Spektrogramm in gleich große Quadrate.\n",
    "    :param image_path: Pfad zum Spektrogramm (PNG)\n",
    "    :param output_dir: Ordner zum Speichern der Segmente\n",
    "    :param segment_size: Größe jedes quadratischen Segments (Standard: 924x924)\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trash_dir = output_dir.parent.parent / f\"{output_dir.parent.name}_trash\"\n",
    "    os.makedirs(trash_dir, exist_ok=True)\n",
    "\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "    segment_size= height\n",
    "\n",
    "    num_segments = width // segment_size -1\n",
    "    for i in range(1, num_segments):\n",
    "        left = i * segment_size\n",
    "        right = left + segment_size\n",
    "        segment = img.crop((left, 0, right, segment_size))\n",
    "        segment_array = np.array(segment)\n",
    "        # print(segment.mode)\n",
    "        \n",
    "        # Berechne den Anteil an Stille (Helligkeit über einem Schwellenwert)\n",
    "        silence_threshold = 10  \n",
    "        silence_ratio = np.mean(segment_array < silence_threshold)\n",
    "        # print(silence_ratio)\n",
    "\n",
    "         # Überprüfen, ob das Bild Transparenz enthält (falls RGBA-Modus)\n",
    "        transparency_ratio = 0\n",
    "        if segment.mode == 'RGBA':\n",
    "            alpha_channel = segment_array[:, :, 3]  # Der Alpha-Kanal ist der 4. Kanal (Index 3)\n",
    "            transparency_ratio = np.mean(alpha_channel == 0)  # Anteil der transparenten Pixel (Alpha == 0)\n",
    "\n",
    "        output_path = Path(output_dir) / f\"{Path(image_path).stem}_part{i}.png\"\n",
    "        if silence_ratio >= 0.7 or transparency_ratio > 0.2:\n",
    "            output_path = trash_dir / f\"{Path(image_path).stem}_part{i}.png\"\n",
    "            print(f\"Segment {i} enthält {silence_ratio * 100:.2f}% Stille und wird in den Trash {output_path} verschoben.\")\n",
    "        \n",
    "        try:\n",
    "            segment.save(output_path)\n",
    "        except IOError:\n",
    "            print(f\"Fehler beim Speichern des Segments: {output_path}\")\n",
    "            continue\n",
    "\n",
    "    \n",
    "    print(f\"Spektrogramm in {num_segments} Segmente geschnitten und gespeichert in {output_dir}\")\n",
    "\n",
    "def process_spectrograms(input_dir):\n",
    "    \"\"\"\n",
    "    Durchsucht rekursiv alle Unterordner eines Verzeichnisses nach Spektrogrammen und schneidet sie in Segmente.\n",
    "    Die Ordnerstruktur des Eingabeverzeichnisses wird im Ausgabeordner beibehalten.\n",
    "    :param input_dir: Verzeichnis mit Unterordnern, die Spektrogramme enthalten\n",
    "    :param output_dir: Verzeichnis zum Speichern der Segmente\n",
    "    \"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = input_dir.parent / f\"{input_dir.name}_splits\"\n",
    "    if output_dir.exists():\n",
    "        print(f\"Output directory already exists: {output_dir}. Skipping splitting.\")\n",
    "        return output_dir\n",
    "    \n",
    "    for subdir in input_dir.iterdir():\n",
    "        if subdir.is_dir():  # Nur Unterverzeichnisse durchsuchen\n",
    "            for image_path in subdir.glob(\"*.png\"):  # Nur PNG-Dateien verarbeiten\n",
    "                relative_path = subdir.relative_to(input_dir)\n",
    "                target_dir = output_dir / relative_path\n",
    "                try:\n",
    "                    img = Image.open(image_path)\n",
    "                    img.verify()  # Überprüft, ob das Bild beschädigt ist\n",
    "                    split_spectrogram(image_path, target_dir)  # Segmente für das Spektrogramm speichern\n",
    "                except (IOError, SyntaxError):\n",
    "                    print(f\"Fehler: Beschädigtes oder ungültiges Bild übersprungen: {image_path}\")\n",
    "                    continue\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"Hilfsfunktion zum Laden und Vorverarbeiten eines Bildes\"\"\"\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3)  # Oder .decode_jpeg\n",
    "    # img = tf.image.resize(img, (img_height, img_width))  # Bildgröße anpassen\n",
    "    img = tf.image.resize(img, (256, 256)) \n",
    "    img = img / 255.0  # Normalisierung\n",
    "    return img\n",
    "\n",
    "def create_siamese_dataset(directory, batch_size=32):\n",
    "    \"\"\" Erstellt ein TensorFlow Dataset mit dynamischer Paarbildung für das Siamese Network \"\"\"\n",
    "    \n",
    "    # 1️⃣ Alle Bilddateien laden\n",
    "    image_paths = list(Path(directory).glob(\"*.png\"))  # Hier nach PNG-Dateien suchen\n",
    "    \n",
    "    # 2️⃣ Paare zuordnen (Basename nach Tag extrahieren)\n",
    "    paired_data = {}\n",
    "    for path in image_paths:\n",
    "        # Extrahiere den Teil des Dateinamens ohne die Tags und ohne die Dateiendung\n",
    "        base_name = re.sub(r'(upscale-from-mp3-128|orig-16-44-mono)_', '', path.stem)  # Entfernt die Tags\n",
    "        \n",
    "        if base_name not in paired_data:\n",
    "            paired_data[base_name] = {\"original\": None, \"upscaled\": None}\n",
    "        \n",
    "        # Ordne die Dateipfade dem richtigen Tag zu\n",
    "        if \"orig-16-44-mono\" in path.stem:  # Original-Dateien\n",
    "            paired_data[base_name][\"original\"] = str(path)\n",
    "        elif \"upscale-from-mp3-128\" in path.stem:  # Upscaled-Dateien\n",
    "            paired_data[base_name][\"upscaled\"] = str(path)\n",
    "\n",
    "    # 3️⃣ Nur Paare mit beiden Dateien (Original und Upscaled) behalten\n",
    "    pairs = []\n",
    "    for data in paired_data.values():\n",
    "        if data[\"original\"] and data[\"upscaled\"]:  # Beide Dateien müssen vorhanden sein\n",
    "            pairs.append((load_image(data[\"original\"]), load_image(data[\"upscaled\"])))\n",
    "        # Falls nur eine Datei vorhanden ist, überspringen wir sie\n",
    "\n",
    "    # 4️⃣ Dataset erstellen\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(pairs)\n",
    "    # dataset = dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def build_siamese_model(input_shape=(128, 128, 3)):\n",
    "    \"\"\" Erstellt ein Siamese CNN mit Normalisierung \"\"\"\n",
    "    print(\"use siamese model\")\n",
    "    norm_layer = layers.Normalization()\n",
    "\n",
    "    # Gemeinsame CNN Architektur für beide Bilder\n",
    "    base_model = models.Sequential([\n",
    "        # layers.Conv2D(32, 3, activation='relu', input_shape=input_shape),\n",
    "        # layers.BatchNormalization(),\n",
    "        # layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu')\n",
    "    ])\n",
    "    print(base_model)\n",
    "    # Zwei Eingänge für das Siamese Network\n",
    "    input1 = layers.Input(shape=input_shape)\n",
    "    input2 = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Gemeinsames CNN auf beide Eingänge anwenden\n",
    "    encoded1 = base_model(input1)\n",
    "    encoded2 = base_model(input2)\n",
    "\n",
    "    # Berechnung des Abstands\n",
    "    distance = layers.Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))([encoded1, encoded2])\n",
    "\n",
    "    # Klassifikationsschicht\n",
    "    output = layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "    model = models.Model([input1, input2], output)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Contrastive Loss\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    \"\"\" Contrastive Loss Funktion \"\"\"\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "\n",
    "\n",
    "def build_resnet_model(input_shape=(128, 128, 3), num_classes=2, dropout_rate=0.2, learning_rate=0.00005, fine_tune_at=None):\n",
    "\n",
    "    print(\"use Resnet_Model\")\n",
    "    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    # base_model = ResNet101(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    # base_model = ResNet152(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    base_model.trainable = True  # Standard: Alles trainierbar\n",
    "\n",
    "    \n",
    "    # Optional: freeze base-model\n",
    "    if fine_tune_at is not None:\n",
    "        for layer in base_model.layers[:fine_tune_at]:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    # add classification header\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)  \n",
    "    x = Dropout(dropout_rate)(x)  \n",
    "    x = Dense(512, activation='relu')(x)  \n",
    "    x = Dropout(dropout_rate)(x)  \n",
    "    predictions = Dense(num_classes, activation='softmax',  dtype='float32')(x)  \n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def calculate_shap_values(model, test_ds, num_samples=1, num_images_to_explain=1):\n",
    "    \"\"\"\n",
    "    Berechnet Shapley-Werte für eine angegebene Anzahl von Bildern aus dem Testdatensatz.\n",
    "\n",
    "    Args:\n",
    "        model: Das zu erklärende Modell.\n",
    "        test_ds: Der Testdatensatz.\n",
    "        num_samples: Anzahl der Batches, die aus dem Testdatensatz genommen werden.\n",
    "        num_images_to_explain: Anzahl der Bilder, für die Shapley-Werte berechnet werden sollen.\n",
    "\n",
    "    Returns:\n",
    "        shap_values_list: Liste der Shapley-Werte für jedes Bild.\n",
    "        test_images_list: Liste der Testbilder, für die die Shapley-Werte berechnet wurden.\n",
    "    \"\"\"\n",
    "    # Testdaten aufbereiten\n",
    "    test_images = []\n",
    "    for images, _ in test_ds.take(num_samples):\n",
    "        test_images.append(images)\n",
    "    test_images = np.concatenate(test_images, axis=0)\n",
    "\n",
    "    # Anzahl der Bilder begrenzen\n",
    "    if num_images_to_explain > test_images.shape[0]:\n",
    "        print(f\"Nur {test_images.shape[0]} Bilder verfügbar, anstatt {num_images_to_explain}.\")\n",
    "        num_images_to_explain = test_images.shape[0]\n",
    "\n",
    "    # Shapley-Werte für jedes Bild berechnen\n",
    "    shap_values_list = []\n",
    "    test_images_list = []\n",
    "\n",
    "    for i in range(num_images_to_explain):\n",
    "        test_image = test_images[i:i+1]  # Einzelnes Bild auswählen\n",
    "\n",
    "        # Explainer initialisieren\n",
    "        explainer = shap.GradientExplainer(model, test_image)\n",
    "\n",
    "        # Shapley-Werte berechnen\n",
    "        shap_values = explainer.shap_values(test_image)\n",
    "\n",
    "        shap_values_list.append(shap_values)\n",
    "        test_images_list.append(test_image)\n",
    "        print(\"Shapley-Werte Form:\", shap_values[0].shape)\n",
    "        print(\"Bild Form:\", test_image.shape)\n",
    "\n",
    "    return shap_values_list, test_images_list\n",
    "\n",
    "def plot_shap_values(shap_values_list, test_images_list):\n",
    "    \"\"\"\n",
    "    Visualisiert die Shapley-Werte für mehrere Bilder.\n",
    "\n",
    "    Args:\n",
    "        shap_values_list: Liste der Shapley-Werte für jedes Bild.\n",
    "        test_images_list: Liste der Testbilder, die visualisiert werden sollen.\n",
    "    \"\"\"\n",
    "    for i, (shap_values, test_image) in enumerate(zip(shap_values_list, test_images_list)):\n",
    "        print(f\"Shapley-Werte für Bild {i+1}:\")\n",
    "        test_image = test_image / 255.0\n",
    "        shap.image_plot(shap_values, test_image)\n",
    "\n",
    "\n",
    "# # Testen mit Dummy-Daten\n",
    "# input_shape = (128, 128, 3)\n",
    "# dummy_input = np.random.random((1, 128, 128, 3))  # Dummy-Bild\n",
    "\n",
    "# # Modell erstellen\n",
    "# model = build_siamese_model(input_shape)\n",
    "# model.summary()\n",
    "\n",
    "# # Vorhersage machen\n",
    "# output = model.predict([dummy_input, dummy_input])\n",
    "# print(\"Predicted Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_results(session):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Extrahieren der Daten aus dem Session-Objekt\n",
    "    history = session.history\n",
    "    metrics = history.history\n",
    "    \n",
    "    # Zugriff auf EarlyStopping Callback und Best-Weight Epoche\n",
    "    early_stopping = session.callbacks[0]  # EarlyStopping Callback\n",
    "    best_epoch = early_stopping.best_epoch  # Epoche des besten Modells (mit restore_best_weights)\n",
    "    \n",
    "    batch_size = session.model_batch_size\n",
    "    \n",
    "    epochs = np.array(history.epoch)\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    # Plot für den Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, metrics[\"loss\"], label=f\"Train Loss {session.model_values[2]:.3f}\")\n",
    "    plt.plot(epochs, metrics[\"val_loss\"], label=f\"Val Loss {session.model_values[0]:.3f}\")\n",
    "\n",
    "    # Vertikale Linie bei der Best-Weights Epoche\n",
    "    if best_epoch is not None:\n",
    "        plt.axvline(\n",
    "            x=best_epoch,  # 1-basierte Epoche\n",
    "            color=\"green\", \n",
    "            linestyle=\"--\",\n",
    "            label=f\"Best Weights Epoch {best_epoch}\"\n",
    "        )\n",
    "\n",
    "    print(session.best_model_values)  \n",
    "\n",
    "    # Überprüfen, ob 'session.best_model_values' nicht None ist\n",
    "    if session.best_model_values is not None:\n",
    "        # Initialisiere die Variablen mit 'N/A'\n",
    "        dropout_value = 'N/A'\n",
    "        regularization_value = 'N/A'\n",
    "        activation_function = 'N/A'\n",
    "\n",
    "        # Iteriere über das Dictionary, um den richtigen Wert für dropout und regularization zu extrahieren\n",
    "        for param, value in session.best_model_values.items():\n",
    "            if param.name == 'dropout':\n",
    "                dropout_value = value  # Der Wert von dropout\n",
    "            elif param.name == 'regularization':\n",
    "                regularization_value = value  # Der Wert von regularization\n",
    "            elif param.name == 'activation':\n",
    "                activation_function = value\n",
    "    else:\n",
    "        dropout_value = 'N/A'\n",
    "        regularization_value = 'N/A'\n",
    "        activation_function = 'N/A'\n",
    "\n",
    "    # Überprüfe die Ausgaben\n",
    "    print(f\"dropout: {dropout_value}, regularization: {regularization_value}, activation: {activation_function}\")\n",
    "\n",
    "    # Jetzt korrektes Anzeigen im Plot\n",
    "    plt.subplots_adjust(bottom=0.65)\n",
    "    plt.figtext(\n",
    "        0.5, 0.01,\n",
    "        f\"Batch Size: {session.model_batch_size}\\n\"\n",
    "        # f\"Training with: {'Mel_Spectogram' if mel_spectogram else 'Spectogram'}\\n\"\n",
    "        f\"HParams: {'Default' if session.best_model_values is None else f'dropout: {dropout_value}, regularization: {regularization_value}, activation: {activation_function}'}\",\n",
    "        fontsize=8, ha=\"center\", va=\"bottom\", color=\"black\"\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylim([0, max(plt.ylim())])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss [CrossEntropy]\")\n",
    "\n",
    "    # Plot für die Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, 100 * np.array(metrics[\"accuracy\"]), label=f\"Train Accuracy {session.model_values[3]:.3f}\")\n",
    "    plt.plot(epochs, 100 * np.array(metrics[\"val_accuracy\"]), label=f\"Val Accuracy {session.model_values[1]:.3f}\")\n",
    "\n",
    "    # Vertikale Linie bei der Best-Weights Epoche\n",
    "    if best_epoch is not None:\n",
    "        plt.axvline(\n",
    "            x=best_epoch,  # 1-basierte Epoche\n",
    "            color=\"green\", \n",
    "            linestyle=\"--\",\n",
    "            label=f\"Best Weights Epoch {best_epoch}\"\n",
    "        )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 100])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy [%]\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"./saved/{adjust_zip_file_path(base_name='history')}.png\")\n",
    "    # plt.savefig(f\"./saved_models/{adjust_zip_file_path(base_name='history')}.png\")\n",
    "    # plt.show()\n",
    "\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_session_as_zip(session, train_ds, zip_dir=\"model_results\"):\n",
    "\n",
    "    # Erstelle das Verzeichnis, falls es nicht existiert\n",
    "    os.makedirs(zip_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Extrahiere Daten für den Dateinamen\n",
    "    num_files = sum(1 for _ in train_ds) * session.model_batch_size  # Gesamtanzahl der Dateien\n",
    "\n",
    "    val_loss, val_acc, train_loss, train_acc = session.model_values\n",
    "    val_loss, val_acc = round(val_loss, 3), round(val_acc, 3)\n",
    "    train_loss, train_acc = round(train_loss, 3), round(train_acc, 3)\n",
    "\n",
    "    zip_filename = f\"model_({num_files}-{'resnet_model'if use_resnet else 'own_model'})_loss_{train_loss}_acc_{train_acc}_val_loss_{val_loss}_val_acc_{val_acc}.zip\"\n",
    "    zip_path = os.path.join(zip_dir, zip_filename)\n",
    "\n",
    "    # 2. Speichere das Modell\n",
    "    model_path = os.path.join(zip_dir, \"model.h5\")\n",
    "    session.model.save(model_path)\n",
    "\n",
    "    # 3. Generiere und speichere den Plot mit deiner Funktion\n",
    "    plot = model_train_results(session)\n",
    "    plot_path = os.path.join(zip_dir, f\"history_({num_files}-{'resnet_model'if use_resnet else 'own_model'})_loss_{train_loss}_acc_{train_acc}_val_loss_{val_loss}_val_acc_{val_acc}.png\")\n",
    "    plot.savefig(plot_path)\n",
    "    model_image_path = f\"{zip_dir}/model.png\"\n",
    "    visualkeras.layered_view(session.model, legend=True, show_dimension=True, to_file=model_image_path)\n",
    "    model_image_path_2 = f\"{zip_dir}/model_2.png\"\n",
    "    plot_model(session.model, to_file=model_image_path_2, show_shapes=True, show_layer_names=True, expand_nested=True, dpi=96)\n",
    "    # model_image_path_3 = f\"{zip_dir}/model_3.png\"\n",
    "    # ann_viz(session.model, filename=model_image_path_3, view=False)\n",
    "    plot.show()\n",
    "    plot.close()\n",
    "\n",
    "    # Nur das history-Attribut extrahieren\n",
    "    # history_dict = session.history.history\n",
    "    session_path = os.path.join(zip_dir, \"session_data.json\")\n",
    "\n",
    "    # # Speichern als JSON\n",
    "    # with open(session_path, 'w') as json_file:\n",
    "    #     json.dump(history_dict, json_file)\n",
    "\n",
    "    # # Speichern als CSV\n",
    "    # with open(os.path.join(zip_dir,'history.csv'), 'w', newline='') as csv_file:\n",
    "    #     writer = csv.writer(csv_file)\n",
    "    #     # Header schreiben (Schlüssel der Metriken)\n",
    "    #     writer.writerow(['epoch'] + list(history_dict.keys()))\n",
    "    #     # Zeilen schreiben (Epochenwerte)\n",
    "    #     for i in range(len(history_dict['loss'])):\n",
    "    #         row = [i + 1] + [history_dict[key][i] for key in history_dict.keys()]\n",
    "    #         writer.writerow(row)\n",
    "\n",
    "    print(\"History wurde als JSON und CSV gespeichert.\")\n",
    "    \n",
    "    max_length = 0\n",
    "    for subdir, _, files in os.walk(TRAIN_DIR):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                input_path = Path(subdir) / file\n",
    "                audio = AudioSegment.from_file(input_path)\n",
    "                duration = len(audio)\n",
    "                max_length = max(max_length, duration)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    # 4. Speichere Variablen in session\n",
    "    session_data = {\n",
    "        \"model_values\": session.model_values,\n",
    "        \"model_batch_size\": session.model_batch_size,\n",
    "        \"best_model_values\": session.best_model_values,\n",
    "        \"history\": session.history.history,\n",
    "        \"softmax_values\": [[[float(v) for v in sample] for sample in epoch] for epoch in session.softmax_values], \n",
    "        \"max_length\":  max_length\n",
    "    }\n",
    "\n",
    "    with open(session_path, \"w\") as f:\n",
    "        # json.dump(session_data, f, indent=4)\n",
    "        json.dump(session_data, f, indent=4, default=lambda o: float(o) if isinstance(o, np.float32) else o)\n",
    "\n",
    "    # 5. Packe alles in eine ZIP-Datei\n",
    "    with zipfile.ZipFile(zip_path, \"w\") as zipf:\n",
    "        zipf.write(model_path, arcname=\"model.h5\")\n",
    "        zipf.write(plot_path, arcname=f\"history_({num_files}-{'resnet_model'if use_resnet else 'own_model'})_loss_{train_loss}_acc_{train_acc}_val_loss_{val_loss}_val_acc_{val_acc}.png\")\n",
    "        zipf.write(session_path, arcname=\"session_data.json\")\n",
    "        zipf.write(model_image_path, arcname=\"model.png\")\n",
    "        zipf.write(model_image_path_2, arcname=\"model_2.png\")\n",
    "        # zipf.write(model_image_path_3, arcname=\"model_3.png\")\n",
    "\n",
    "    # Aufräumen\n",
    "    os.remove(model_path)\n",
    "    os.remove(plot_path)\n",
    "    os.remove(session_path)\n",
    "    os.remove(model_image_path)\n",
    "    os.remove(model_image_path_2)\n",
    "\n",
    "    print(f\"Session-Daten erfolgreich in {zip_path} gespeichert.\")\n",
    "    return zip_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, dataset, num_samples=5):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.num_samples = num_samples\n",
    "        self.softmax_history = []  # Liste zur Speicherung der Softmax-Werte pro Epoche\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        softmax_values_epoch = []\n",
    "        print(f\"\\nSoftmax-Werte nach Epoche {epoch+1}:\")\n",
    "        for images, labels in self.dataset.take(1):  # Nur eine Batch nehmen\n",
    "            predictions = self.model.predict(images[:self.num_samples], verbose=0)\n",
    "            softmax_values = tf.nn.softmax(predictions).numpy()  # Softmax explizit berechnen\n",
    "            for i in range(self.num_samples):\n",
    "                softmax_values_epoch.append(softmax_values[i])  # Speichern\n",
    "                print(f\"Sample {i+1}: {softmax_values[i]} (Label: {labels[i].numpy()})\")\n",
    "            break\n",
    "        self.softmax_history.append(softmax_values_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_size_from_dir(directory):\n",
    "    \"\"\"Liest die Bildgröße aus der ersten Datei im Verzeichnis.\"\"\"\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    return img.size[::-1]  # (Höhe, Breite)\n",
    "    raise ValueError(\"Keine Bilder im Verzeichnis gefunden!\")\n",
    "\n",
    "def preprocess_data(train_dir, val_dir, test_dir, dropout_rate=0.2, regularization_rate=0.0001, activation='relu'):\n",
    "    global img_height, img_width\n",
    "    img_height, img_width = get_image_size_from_dir(train_dir)\n",
    "    print(f\"Ermittelte Bildgröße: {img_height} x {img_width}\")\n",
    "\n",
    "    if not use_siamese:\n",
    "        train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            train_dir,\n",
    "            image_size=(img_height, img_width),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            val_dir,\n",
    "            image_size=(img_height, img_width),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            test_dir,\n",
    "            image_size=(img_height, img_width),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "\n",
    "    # 2️⃣ Siamese Netzwerk Ansatz: Paare aus Bildern erstellen\n",
    "    else:\n",
    "        train_ds = create_siamese_dataset(train_dir, batch_size=BATCH_SIZE)\n",
    "        val_ds = create_siamese_dataset(val_dir, batch_size=BATCH_SIZE)\n",
    "        test_ds = create_siamese_dataset(test_dir, batch_size=BATCH_SIZE)\n",
    "        print(\"siamese DS Created\")\n",
    "\n",
    "    # Preprocessing\n",
    "    train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    for batch in train_ds.take(1):\n",
    "        images, labels = batch\n",
    "        print(images.numpy().min(), images.numpy().max()) \n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def build_model(input_shape=(128, 128, 3), num_classes=2, dropout_rate=0.2, regularization_rate=0.0001, activation='relu'):\n",
    "\n",
    "    print(f\"num_labels: {num_classes}\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "        \n",
    "\n",
    "    print(\"Build Model with:\")\n",
    "    print (f\"dropout_rate = {dropout_rate}\")\n",
    "    print (f\"regularization_rate = {regularization_rate}\")\n",
    "    print (f\"activation = {activation}\")\n",
    "    print(f\"input_shape: {input_shape}\")\n",
    "    print(\"\")\n",
    "    norm_layer = layers.Normalization()\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        norm_layer,\n",
    "        layers.Conv2D(16, 3, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(32, 3, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        # layers.Dropout(dropout_rate),  # Dropout bleibt hier\n",
    "        layers.Conv2D(64, 3, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(128, 3, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        # layers.Dropout(dropout_rate),\n",
    "        layers.Dense(128, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),  # Kleinere Dense-Schicht\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(num_classes, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_and_result():\n",
    "    # # Define the paths for the mel-spectrogram directories\n",
    "    # train_mel_dir = Path(f\"{TRAIN_DIR.stem}_mel_spectrograms\")\n",
    "    # val_mel_dir = Path(f\"{VAL_DIR.stem}_mel_spectrograms\")\n",
    "    # test_mel_dir = Path(f\"{TEST_DIR.stem}_mel_spectrograms\")\n",
    "    \n",
    "    # Load datasets\n",
    "    train_ds, val_ds, test_ds = preprocess_data(train_mel_dir, val_mel_dir, test_mel_dir)\n",
    "\n",
    "    # Build the model\n",
    "    if use_resnet:\n",
    "        model = build_resnet_model(input_shape=(img_height, img_width, 3), num_classes=2, fine_tune_at=100)\n",
    "    else:\n",
    "        if use_siamese:\n",
    "            model = build_siamese_model(input_shape=(img_height, img_width, 3))  # Siamese-Modell für Spektrogramme\n",
    "            print(\"builded siamese model\")\n",
    "            model.compile(loss=contrastive_loss, optimizer='adam', metrics=['accuracy'])\n",
    "            print(\"compiled\")\n",
    "        else:\n",
    "            model = build_model(input_shape=(img_height, img_width, 3), num_classes=2)\n",
    "        \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=(epochs // 4) if stop else epochs , restore_best_weights=True)\n",
    "    softmax_logger = SoftmaxLogger(val_ds, num_samples=5)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_accuracy', \n",
    "        factor=0.5,  # Learning Rate halbieren\n",
    "        patience=3,  # Wenn sich _monitor_ 3 Epochen nicht verbessert\n",
    "        min_lr=0.00001  # Untere Grenze für die LR\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping,  softmax_logger, reduce_lr]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    # test_loss, test_acc = model.evaluate(test_ds, verbose=2)\n",
    "    trained_epochs = len(history.epoch)\n",
    "    print(f\"Das Training wurde nach {trained_epochs} Epochen gestoppt.\")\n",
    "    best_epoch = early_stopping.best_epoch\n",
    "    print(f\"Das beste Modell wurde in Epoche {best_epoch} gefunden.\")\n",
    "        \n",
    "        \n",
    "    val_loss = history.history['val_loss'][best_epoch]\n",
    "    val_accuracy = history.history['val_accuracy'][best_epoch]\n",
    "    train_loss = history.history['loss'][best_epoch]\n",
    "    train_accuracy = history.history['accuracy'][best_epoch]\n",
    "\n",
    "    batch_size = None\n",
    "    for element in train_ds.take(1):\n",
    "        batch_size = element[0].shape[0] \n",
    "        break\n",
    "        \n",
    "    session = SimpleNamespace(\n",
    "        model=model,\n",
    "        history=history,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping],\n",
    "        model_values = [val_loss, val_accuracy, train_loss, train_accuracy],\n",
    "        model_batch_size = batch_size,\n",
    "        best_model_values = None,\n",
    "        softmax_values=softmax_logger.softmax_history\n",
    "    )\n",
    "\n",
    "    path = save_session_as_zip(session, train_ds)\n",
    "    print(f\"Evaluated model with best weights: val_loss={val_loss}, val_accuracy={val_accuracy}\")\n",
    "\n",
    "    if use_shap_values:\n",
    "        print(\"calculate_shap_values\")\n",
    "        shap_values_list, test_images_list = calculate_shap_values(model, test_ds, num_samples=1, num_images_to_explain=3)\n",
    "        plot_shap_values(shap_values_list, test_images_list)\n",
    "\n",
    "    # plt = model_train_results(session)\n",
    "\n",
    "    return path, session, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_images(directory):\n",
    "    print(f\"check images in: {directory}\")\n",
    "    trash_dir = os.path.join(directory, '..', 'trash')  # Trash-Ordner auf gleicher Ebene wie directory\n",
    "    \n",
    "    # Sicherstellen, dass der Trash-Ordner existiert\n",
    "    os.makedirs(trash_dir, exist_ok=True)\n",
    "\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.lower().endswith('.png'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                try:\n",
    "                    with Image.open(file_path) as img:\n",
    "                        img.verify()\n",
    "                except (IOError, SyntaxError):\n",
    "                    print(f\"Fehler bei {filename}, verschiebe nach trash\")\n",
    "                    trash_file_path = os.path.join(trash_dir, os.path.relpath(file_path, directory))\n",
    "                    os.makedirs(os.path.dirname(trash_file_path), exist_ok=True)\n",
    "                    shutil.move(file_path, trash_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_segments(output_dir):\n",
    "    \"\"\"\n",
    "    Diese Funktion vergleicht Mel-Spektrogramme aus zwei verschiedenen Label-Unterordnern, \n",
    "    indem sie die ersten fünf Dateien paarweise nach ihrem Index abgleicht und als Grafik anzeigt.\n",
    "\n",
    "    :param output_dir: Verzeichnis, das die Label-Unterordner mit Mel-Spektrogrammen enthält\n",
    "    \"\"\"\n",
    "\n",
    "    output_dir = Path(output_dir)\n",
    "    label_dirs = [d for d in output_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "    if len(label_dirs) < 2:\n",
    "        raise ValueError(\"Es müssen mindestens zwei Label-Unterordner vorhanden sein.\")\n",
    "\n",
    "    # Sortiere die Label-Unterordner alphabetisch\n",
    "    label_dirs.sort()\n",
    "\n",
    "    # Liste der Dateien in jedem Label-Unterordner\n",
    "    files_per_label = [sorted(label_dir.glob(\"*.png\")) for label_dir in label_dirs]\n",
    "\n",
    "    # Paare von Dateien basierend auf ihrem Index erstellen (maximal 5 Paare)\n",
    "    pairs = list(zip(*[files[::30] for files in files_per_label]))[:5]\n",
    "\n",
    "    # Vergleiche und zeige die Paare als Grafik an\n",
    "    for idx, (file1, file2) in enumerate(pairs):\n",
    "        img1 = plt.imread(file1)\n",
    "        img2 = plt.imread(file2)\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img1)\n",
    "        plt.title(f\"Label 1: {file1.stem[:80]}...\", fontsize=6)  # Verkürzt und kleinere Schriftgröße\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(img2)\n",
    "        plt.title(f\"Label 2: {file2.stem[:80]}...\", fontsize=6)  # Verkürzt und kleinere Schriftgröße\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Vergleich {idx + 1}\")\n",
    "        plt.show()\n",
    "        plt.savefig(f\"./compare/{idx + 1}_compare.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_spectrogram_compare(mel_spectrogram, num_segments):\n",
    "#     \"\"\"\n",
    "#     Zerlegt das Original-Spektrogramm in Segmente.\n",
    "#     \"\"\"\n",
    "#     segment_length = mel_spectrogram.shape[1] // num_segments\n",
    "#     segments = [mel_spectrogram[:, i * segment_length:(i + 1) * segment_length] for i in range(num_segments)]\n",
    "#     return segments\n",
    "\n",
    "# def process_and_compare_spectrograms(original_file, output_base_dir, n_mels=256, fmin=20, fmax=44100):\n",
    "#     \"\"\"\n",
    "#     Verarbeitet eine einzelne Audiodatei:\n",
    "#     - Erstellt das Spektrogramm der Originaldatei\n",
    "#     - Splittet die Datei in nicht überlappende Segmente\n",
    "#     - Erstellt Spektrogramme für die Splits\n",
    "#     - Vergleicht und visualisiert alle Spektrogramme\n",
    "#     \"\"\"\n",
    "#     original_file = Path(original_file)\n",
    "#     output_base_dir = Path(output_base_dir)\n",
    "    \n",
    "#     # Erstelle das Spektrogramm der Originaldatei\n",
    "#     _, mel_orig_db, sr_orig, _, _ = process_audio_file(original_file, original_file.parent, output_base_dir, n_mels, fmin, fmax)\n",
    "    \n",
    "#     # Splitte die Datei\n",
    "#     split_dir = split_audio_dataset(original_file.parent)\n",
    "    \n",
    "#     # Erstelle Spektrogramme für die Splits\n",
    "#     generate_mel_spectrograms_with_structure(split_dir, output_base_dir, n_mels, fmin, fmax)\n",
    "    \n",
    "#     # Lade die gesplitteten Dateien, aber nur nicht-überlappende Segmente\n",
    "#     split_files = sorted([f for f in split_dir.rglob(\"*.wav\") if \"_segment_\" in f.stem and \"_segment_0.\" in f.stem or int(f.stem.split(\"_segment_\")[-1]) % 2 == 0], key=lambda x: int(x.stem.split(\"_segment_\")[-1]))\n",
    "#     split_spectrograms = []\n",
    "    \n",
    "#     for split_file in split_files:\n",
    "#         _, mel_split_db, sr_split, _, _ = process_audio_file(split_file, split_dir, output_base_dir, n_mels, fmin, fmax)\n",
    "#         split_spectrograms.append(mel_split_db)\n",
    "    \n",
    "#     # Neue Matrix für zusammengefügte Splits erzeugen\n",
    "#     reconstructed_spectrogram = np.zeros_like(mel_orig_db)\n",
    "#     segment_length = mel_orig_db.shape[1] // len(split_files) if split_files else mel_orig_db.shape[1]\n",
    "    \n",
    "#     for i, mel_split_db in enumerate(split_spectrograms):\n",
    "#         start_col = i * segment_length\n",
    "#         end_col = start_col + mel_split_db.shape[1]\n",
    "#         if end_col > reconstructed_spectrogram.shape[1]:\n",
    "#             end_col = reconstructed_spectrogram.shape[1]\n",
    "#         reconstructed_spectrogram[:, start_col:end_col] = mel_split_db[:, :end_col - start_col]\n",
    "    \n",
    "#     # Original-Spektrogramm in Segmente teilen\n",
    "#     original_segments = split_spectrogram_compare(mel_orig_db, len(split_files))\n",
    "    \n",
    "#     # Vergleich der Spektrogramme plotten\n",
    "#     fig, axes = plt.subplots(3, 1, figsize=(40, 6))\n",
    "    \n",
    "#     axes[0].set_title(\"Original Spektrogramm\")\n",
    "#     librosa.display.specshow(mel_orig_db, x_axis='time', y_axis='mel', sr=sr_orig, cmap='magma', fmin=fmin, fmax=fmax, ax=axes[0])\n",
    "#     axes[0].axis('off')\n",
    "        \n",
    "#     axes[1].set_title(\"Original-Spektrogramm in Segmente zerlegt\")\n",
    "#     combined_segments = np.hstack(original_segments)\n",
    "#     librosa.display.specshow(combined_segments, x_axis='time', y_axis='mel', sr=sr_orig, cmap='magma', fmin=fmin, fmax=fmax, ax=axes[1])\n",
    "#     axes[1].axis('off')\n",
    "\n",
    "#     axes[2].set_title(\"Rekonstruiertes Spektrogramm aus Splits\")\n",
    "#     librosa.display.specshow(reconstructed_spectrogram, x_axis='time', y_axis='mel', sr=sr_orig, cmap='magma', fmin=fmin, fmax=fmax, ax=axes[2])\n",
    "#     axes[2].axis('off')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_predictions(model, test_ds, max_images=None):\n",
    "    results = []\n",
    "    class_names = test_ds.class_names if hasattr(test_ds, \"class_names\") else [str(i) for i in range(len(test_ds))]\n",
    "    \n",
    "    # Mische das Dataset nur, wenn max_images gesetzt ist\n",
    "    if max_images is not None:\n",
    "        test_ds = test_ds.shuffle(buffer_size=1000)  # Puffergröße je nach Bedarf anpassen\n",
    "\n",
    "    image_count = 0\n",
    "    for images, labels in test_ds:\n",
    "        predictions = model.predict(images)\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            if max_images is not None and image_count >= max_images:\n",
    "                break\n",
    "            true_label = class_names[labels[i].numpy()]\n",
    "            predicted_label = class_names[predicted_labels[i]]\n",
    "            results.append({\n",
    "                'True Class': true_label,\n",
    "                'Predicted Class': predicted_label\n",
    "            })\n",
    "            image_count += 1\n",
    "\n",
    "        if max_images is not None and image_count >= max_images:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_summary(results):\n",
    "    total_predictions = len(results)\n",
    "    correct_predictions = sum(1 for result in results if result['True Class'] == result['Predicted Class'])  # Vergleiche nur die Klassennamen\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "    # Für jede Klasse\n",
    "    summary_data = {\n",
    "        'Total Correct': f\"{correct_predictions} / {total_predictions} ({accuracy:.2f}%)\",\n",
    "        'Classwise Accuracy': {}\n",
    "    }\n",
    "\n",
    "    # Berechne die Genauigkeit pro Klasse\n",
    "    for label in set(result['True Class'] for result in results):\n",
    "        correct_for_class = sum(1 for result in results if result['True Class'] == label and result['True Class'] == result['Predicted Class'])\n",
    "        total_for_class = sum(1 for result in results if result['True Class'] == label)\n",
    "        class_accuracy = (correct_for_class / total_for_class * 100) if total_for_class > 0 else 0\n",
    "        summary_data['Classwise Accuracy'][label] = f\"{class_accuracy:.2f}%\"\n",
    "\n",
    "    return summary_data\n",
    "\n",
    "# Funktion zur Erstellung der Zusammenfassungs-Tabelle\n",
    "def display_summary(summary, save_path=\"model_results\"):\n",
    "    summary_data = [\n",
    "        [\"Total Correct\", summary[\"Total Correct\"]],\n",
    "    ]\n",
    "    \n",
    "    for label, accuracy in summary['Classwise Accuracy'].items():\n",
    "        summary_data.append([f\"{label} Accuracy\", accuracy])\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data, columns=[\"Metric\", \"Value\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 2))  # Größe der Tabelle\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "\n",
    "    table = ax.table(cellText=summary_df.values, colLabels=summary_df.columns, cellLoc=\"center\", loc=\"center\", colWidths=[0.5, 0.5])\n",
    "    \n",
    "    # Optional: Hier könntest du Farben anpassen\n",
    "    # color_table(table)\n",
    "\n",
    "    plt.show()\n",
    "    table_filename = \"summary_table.png\"\n",
    "    table_path = os.path.join(save_path, table_filename)\n",
    "    fig.savefig(table_path, bbox_inches='tight', pad_inches=0.1)  # Speichern als Bild\n",
    "    print(f\"Table saved as {table_path}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "def visualize_and_summarize_predictions(model, test_ds, max_images=10):\n",
    "    # Ergebnisse sammeln\n",
    "    results = collect_predictions(model, test_ds, max_images=max_images)\n",
    "    \n",
    "    # Zusammenfassung der Ergebnisse generieren\n",
    "    summary = generate_summary(results)\n",
    "    \n",
    "    # Anzeige der Zusammenfassungstabelle\n",
    "    display_summary(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    extract_zip(TRAIN_DIR, UNZIP_DIR)\n",
    "    extract_zip(TEST_DIR, UNZIP_DIR)\n",
    "    extract_zip(VAL_DIR, UNZIP_DIR)\n",
    "    rename_audio_files(UNZIP_DIR)\n",
    "\n",
    "    # TRAIN_DIR=trim_audio_files(TRAIN_DIR)\n",
    "    # VAL_DIR=trim_audio_files(VAL_DIR)\n",
    "    # TEST_DIR=trim_audio_files(TEST_DIR)\n",
    "\n",
    "    TRAIN_DIR=normalize_audio_length(TRAIN_DIR)\n",
    "    VAL_DIR=normalize_audio_length(VAL_DIR)\n",
    "    TEST_DIR=normalize_audio_length(TEST_DIR)\n",
    "\n",
    "\n",
    "    train_mel_dir = Path(f\"{TRAIN_DIR.stem}_mel_spectrograms\")  # Der Zielordner für die Mel-Spektrogramme\n",
    "    generate_mel_spectrograms_with_structure(TRAIN_DIR.resolve(), train_mel_dir)\n",
    "    train_mel_dir = process_spectrograms(train_mel_dir)\n",
    "\n",
    "    val_mel_dir = Path(f\"{VAL_DIR.stem}_mel_spectrograms\")  # Der Zielordner für die Mel-Spektrogramme\n",
    "    generate_mel_spectrograms_with_structure(VAL_DIR, val_mel_dir)\n",
    "    val_mel_dir = process_spectrograms(val_mel_dir)\n",
    "\n",
    "    test_mel_dir = Path(f\"{TEST_DIR.stem}_mel_spectrograms\")  # Der Zielordner für die Mel-Spektrogramme\n",
    "    generate_mel_spectrograms_with_structure(TEST_DIR, test_mel_dir)\n",
    "    test_mel_dir = process_spectrograms(test_mel_dir)\n",
    "\n",
    "    if check_image:\n",
    "        check_images(train_mel_dir)\n",
    "        check_images(val_mel_dir)\n",
    "        check_images(test_mel_dir)\n",
    "\n",
    "\n",
    "    import ipynb.fs.defs.Programm_audioToPicture as old_Programm\n",
    "\n",
    "    wav_dir = Path(\"../picture_model/compare/orig_vs_split\")\n",
    "    wav_files = list(wav_dir.glob(\"*.wav\"))\n",
    "\n",
    "    for wav_file in wav_files:\n",
    "        splits_dir = wav_dir / f\"{wav_file.stem}_splits\"\n",
    "        print(f\"wav_file: {wav_file}\")\n",
    "        old_Programm.process_and_compare_spectrograms(wav_file, splits_dir, input_file=Path(wav_file))\n",
    "\n",
    "\n",
    "    # old_Programm.process_and_compare_spectrograms(\"../picture_model/compare/orig_vs_split/orig-16-44-mono_Burglar Bob.wav\", \"../picture_model/compare/orig_vs_split/splits/\")\n",
    "    # compare_segments(train_mel_dir)\n",
    "\n",
    "    # path, session, test_ds = train_and_result()\n",
    "\n",
    "    # Funktion ausführen\n",
    "    # visualize_and_summarize_predictions(session.model, test_ds)\n",
    "\n",
    "    # import ipynb.fs.defs.use_model_picture as use_model\n",
    "    # use_model.run(path)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
