{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "Your GPUs may run slowly with dtype policy mixed_float16 because they do not have compute capability of at least 7.0. Your GPUs:\n",
      "  DML, no compute capability (probably not an Nvidia GPU) (x2)\n",
      "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shap\n",
    "import gc\n",
    "import zipfile\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from collections import Counter\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_silence\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from types import SimpleNamespace\n",
    "import traceback\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "# from tensorflow.keras.mixed_precision import set_global_policy\n",
    "# set_global_policy('mixed_float16')  # Nutzt FP16 statt FP32 für Berechnungen\n",
    "\n",
    "\n",
    "img_height, img_width = 128, 128\n",
    "BATCH_SIZE=4\n",
    "epochs=20\n",
    "use_resnet = False  \n",
    "use_shap_values = True\n",
    "check_image = False\n",
    "\n",
    "# Globale Pfade für die Daten\n",
    "ROOT_DIR = Path('../').resolve()  # Hauptverzeichnis\n",
    "ZIP_DIR = ROOT_DIR / 'data'  # Ordner, der die ZIP-Dateien enthält\n",
    "UNZIP_DIR = ROOT_DIR / 'Unzipped_Data_Picture'  # Zielordner für entpackte Dateien\n",
    "\n",
    "# # Small Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / 'small_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / 'small_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / 'small_test_ds'\n",
    "\n",
    "# # Small 3 Labels Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / '3_small_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / '3_small_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / '3_small_test_ds'\n",
    "\n",
    "# Medium Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / 'medium_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / 'medium_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / 'medium_test_ds'\n",
    "\n",
    "# # Large Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / 'large_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / 'large_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / 'medium_test_ds'\n",
    "\n",
    "# # Large 3 Labels Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / '3_large_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / '3_large_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / '3_large_test_ds'\n",
    "\n",
    "# # No_mod Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / 'no_mod_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / 'no_mod_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / 'no_mod_test_ds'\n",
    "\n",
    "# New_Dataset\n",
    "TRAIN_DIR = UNZIP_DIR / 'new_large_train_ds'\n",
    "VAL_DIR = UNZIP_DIR / 'new_large_val_ds'\n",
    "TEST_DIR = UNZIP_DIR / 'medium_test_ds'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip(zip_path, extract_to):\n",
    "    zip_path_str = str(zip_path)\n",
    "    \n",
    "    if not zip_path_str.endswith('.zip'):\n",
    "        zip_path_str += '.zip'\n",
    "    \n",
    "    zip_file_path = pathlib.Path(zip_path_str)\n",
    "    \n",
    "    folder_name = zip_file_path.stem \n",
    "    target_folder = pathlib.Path(extract_to) / folder_name\n",
    "    \n",
    "    if target_folder.exists():\n",
    "        print(f\"Das Verzeichnis {target_folder} existiert bereits. Überspringe das Extrahieren.\")\n",
    "    else:\n",
    "        if zip_file_path.exists():\n",
    "            print(f\"Extrahiere die Zip-Datei {zip_file_path} nach {extract_to}.\")\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_to)\n",
    "            print(f\"Zip-Datei {zip_file_path} erfolgreich extrahiert.\")\n",
    "        else:\n",
    "            print(f\"Die Zip-Datei {zip_file_path} existiert nicht.\")\n",
    "\n",
    "def rename_audio_files(root_path):\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        parent_folder = os.path.basename(root)\n",
    "        for file in files:\n",
    "            if not file.startswith(f\"{parent_folder}_\"):\n",
    "                if file.endswith(('.wav', '.mp3')):  \n",
    "                    \n",
    "                    old_file_path = os.path.join(root, file)\n",
    "                    new_file_name = f\"{parent_folder}_{file}\"\n",
    "                    new_file_path = os.path.join(root, new_file_name)\n",
    "                        \n",
    "                    os.rename(old_file_path, new_file_path)\n",
    "        print(f\"renaming of {root_path}/{parent_folder} complete\")\n",
    "\n",
    "\n",
    "# extract_zip(TRAIN_DIR, UNZIP_DIR)\n",
    "# extract_zip(TEST_DIR, UNZIP_DIR)\n",
    "# extract_zip(VAL_DIR, UNZIP_DIR)\n",
    "# rename_audio_files(UNZIP_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statistics(total_segments, retaining_segments, discarded_segments, trash_dir):\n",
    "    # Create a bar chart for the statistics\n",
    "    labels = [f'Total Segments: {total_segments}', f'Valid Segments: {retaining_segments}', f'Discarded Segments: {discarded_segments}']\n",
    "    values = [total_segments, retaining_segments, discarded_segments]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(labels, values, color=['blue', 'green', 'red'])\n",
    "    plt.title('Audio Splitting Statistics')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(trash_dir / 'splitting_statistics.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Statistics saved as PNG in {trash_dir}\")\n",
    "\n",
    "def contains_completely_silent_part(segment, silence_threshold=-90.0, min_silence_len=100):\n",
    "    \"\"\"Check if a segment contains any completely silent part.\"\"\"\n",
    "    silence_ranges = detect_silence(\n",
    "        segment, min_silence_len=min_silence_len, silence_thresh=silence_threshold\n",
    "    )\n",
    "    # If any silence range exists that is equal to or exceeds min_silence_len, return True\n",
    "    return any(end - start >= min_silence_len for start, end in silence_ranges)\n",
    "\n",
    "def plot_spectrogram(wav_path, output_dir):\n",
    "    \"\"\"Plot and save the spectrogram of a wav file.\"\"\"\n",
    "    # Load the audio file using librosa (this also returns the sample rate)\n",
    "    samples, sample_rate = librosa.load(wav_path, sr=None)\n",
    "    \n",
    "    # Generate the spectrogram (using a Short-Time Fourier Transform, STFT)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Compute the spectrogram (logarithmic scale for better visualization)\n",
    "    D = librosa.amplitude_to_db(librosa.stft(samples), ref=np.max)\n",
    "    \n",
    "    # Display the spectrogram\n",
    "    librosa.display.specshow(D, sr=sample_rate, x_axis='time', y_axis='log', cmap='viridis')\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title(f\"Spectrogram of {Path(wav_path).name}\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.colorbar(label=\"Intensity (dB)\")\n",
    "\n",
    "    # Save the plot\n",
    "    spectrogram_path = output_dir / f\"{Path(wav_path).stem}_spectrogram.png\"\n",
    "    plt.savefig(spectrogram_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Spectrogram saved: {spectrogram_path}\")\n",
    "\n",
    "def split_audio_dataset(input_dir):\n",
    "        segment_length_ms = 1000\n",
    "        overlap_ms = 500  # 50% Überlappung\n",
    "\n",
    "        output_dir = input_dir.parent / f\"{input_dir.name}_splits\"\n",
    "        trash_dir = input_dir.parent / f\"{input_dir.name}_trash\"\n",
    "        \n",
    "        if output_dir.exists():\n",
    "            print(f\"Output directory already exists: {output_dir}. Skipping splitting.\")\n",
    "            return output_dir\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(trash_dir, exist_ok=True)\n",
    "\n",
    "        total_segments = 0\n",
    "        retaining_segments = 0\n",
    "        discarded_segments = 0\n",
    "\n",
    "        # Traverse the input directory and split files\n",
    "        for subdir, _, files in os.walk(input_dir):\n",
    "            relative_path = Path(subdir).relative_to(input_dir)  # Maintain subdirectory structure\n",
    "            target_dir = output_dir / relative_path\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):  # Adjust formats as needed\n",
    "                    input_path = Path(subdir) / file\n",
    "                    audio = AudioSegment.from_file(input_path)\n",
    "                    duration = len(audio)  # Total length of the audio in milliseconds\n",
    "\n",
    "                    # Split and export segments\n",
    "                    for i, start_time in enumerate(range(0, duration, segment_length_ms - overlap_ms)):\n",
    "                        end_time = min(start_time + segment_length_ms, duration)\n",
    "                        segment = audio[start_time:end_time]\n",
    "                        total_segments +=1\n",
    "                        file_path = Path(file)  # Convert file string to Path\n",
    "                        # segment_filename = target_dir / f\"{file_path.stem}_segment_{i}.wav\"\n",
    "                        if contains_completely_silent_part(segment):\n",
    "                            discarded_segments += 1\n",
    "                            segment_filename = trash_dir / f\"{file_path.stem}_segment_{i}.wav\"\n",
    "                            segment.export(segment_filename, format=\"wav\")\n",
    "                            plot_spectrogram(segment_filename, trash_dir)\n",
    "                        else:\n",
    "                            retaining_segments += 1\n",
    "                            segment_filename = target_dir / f\"{file_path.stem}_segment_{i}.wav\"\n",
    "                            segment.export(segment_filename, format=\"wav\")\n",
    "                            \n",
    "                    print(f\"Processed {file} into {i+1} segments in {target_dir}\")\n",
    "\n",
    "        generate_statistics(total_segments, retaining_segments, discarded_segments, trash_dir)\n",
    "        print(f\"All files processed. Split dataset saved in {output_dir}\")\n",
    "        return output_dir\n",
    "\n",
    "\n",
    "# print(f\"Split Audio Data in {1000}ms\")\n",
    "# TRAIN_DIR=split_audio_dataset(TRAIN_DIR)\n",
    "# VAL_DIR=split_audio_dataset(VAL_DIR)\n",
    "# TEST_DIR=split_audio_dataset(TEST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel-Spektogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_file(audio_file, input_dir, output_dir, n_mels, fmin, fmax):\n",
    "    \"\"\"\n",
    "    Diese Funktion verarbeitet eine einzelne Audiodatei und berechnet das Mel-Spektrogramm.\n",
    "    \"\"\"\n",
    "    # Relativer Pfad zur Eingabedatei\n",
    "    relative_path = audio_file.relative_to(input_dir)\n",
    "    \n",
    "    # Zielpfad basierend auf der ursprünglichen Ordnerstruktur\n",
    "    target_dir = output_dir / relative_path.parent\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # def trim_to_zero_crossings(y, sr):\n",
    "    #     start_sample = librosa.zero_crossings(y, pad=False).argmax()\n",
    "    #     end_sample = len(y) - librosa.zero_crossings(y[::-1], pad=False).argmax()\n",
    "    #     return y[start_sample:end_sample]\n",
    "    # Lade die Audiodatei mit librosa\n",
    "    y, sr = librosa.load(audio_file, sr=44100)\n",
    "    # y = trim_to_zero_crossings(y, sr)\n",
    "\n",
    "    # Berechne das Mel-Spektrogramm\n",
    "    # padding = 1024\n",
    "    # y = np.pad(y, (padding, padding), mode='constant')\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
    "\n",
    "    # Konvertiere das Mel-Spektrogramm in dB (logarithmische Skala)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Rückgabe der Daten ohne das Plotten\n",
    "    return relative_path, mel_spectrogram, sr, target_dir, audio_file.stem\n",
    "\n",
    "def generate_mel_spectrograms_with_structure(input_dir, output_dir, n_mels=256, fmin=20, fmax=44100, batch_size=100):\n",
    "    \"\"\"\n",
    "    Optimierte Funktion für die Verarbeitung von Mel-Spektrogrammen:\n",
    "    1. Berechnung wird parallelisiert.\n",
    "    2. Ergebnisse werden sequentiell geplottet, um Thread-Sicherheitsprobleme zu vermeiden.\n",
    "    3. Batches werden verwendet, um den Speicherverbrauch zu kontrollieren.\n",
    "    \"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "\n",
    "    # Überprüfen, ob die Ordnerstruktur bereits existiert\n",
    "    if output_dir.exists() and any(output_dir.rglob(\"*.png\")):\n",
    "        print(f\"Überspringe Verarbeitung, da {output_dir} bereits Mel-Spektrogramme enthält.\")\n",
    "        return\n",
    "\n",
    "    # Liste der .wav-Dateien im input_dir\n",
    "    audio_files = list(input_dir.rglob(\"*.wav\"))\n",
    "\n",
    "    if not audio_files:\n",
    "        print(\"Keine Audiodateien gefunden.\")\n",
    "        return\n",
    "\n",
    "    total_files = len(audio_files)\n",
    "    print(f\"{total_files} Audiodateien gefunden. Verarbeitung startet.\")\n",
    "\n",
    "    # Verarbeite die Dateien in Batches\n",
    "    for batch_start in range(0, total_files, batch_size):\n",
    "        batch_files = audio_files[batch_start:batch_start + batch_size]\n",
    "        gc.collect()\n",
    "        print(f\"Verarbeite Batch {batch_start // batch_size + 1} von {total_files // batch_size + 1}\")\n",
    "\n",
    "        # Parallele Berechnung der Mel-Spektrogramme\n",
    "        results = []\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [\n",
    "                executor.submit(process_audio_file, audio_file, input_dir, output_dir, n_mels, fmin, fmax)\n",
    "                for audio_file in batch_files\n",
    "            ]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                results.append(future.result())\n",
    "\n",
    "        # Sequentielles Plotten und Speichern\n",
    "        for relative_path, mel_spectrogram_db, sr, target_dir, audio_file_stem in results:\n",
    "            mel_spectrogram_path = target_dir / f\"{audio_file_stem}_mel_spectrogram.png\"\n",
    "\n",
    "            # Überspringen, wenn das Spektrogramm bereits existiert\n",
    "            if mel_spectrogram_path.exists():\n",
    "                print(f\"Spektrogramm {mel_spectrogram_path} existiert bereits. Überspringen.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Erstelle das Bild des Mel-Spektrogramms\n",
    "                plt.figure(figsize=(20, 2))\n",
    "                librosa.display.specshow(mel_spectrogram_db, x_axis='time', y_axis='mel', sr=sr, cmap='magma', fmin=fmin, fmax=fmax)\n",
    "                plt.axis('off')\n",
    "\n",
    "                # Speichern des Bildes als PNG\n",
    "                plt.savefig(mel_spectrogram_path, bbox_inches='tight', pad_inches=0, dpi=600)\n",
    "                plt.close()\n",
    "                print(f\"{batch_start // batch_size + 1} von {total_files // batch_size + 1}__Mel-Spektrogramm für {audio_file_stem} gespeichert in {mel_spectrogram_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Plotten von {audio_file_stem}: {e}\")\n",
    "            finally:\n",
    "                # Speicher freigeben\n",
    "                del mel_spectrogram_db\n",
    "                # gc.collect()\n",
    "                # print(\"batch complete\")\n",
    "\n",
    "    print(f\"Alle Mel-Spektrogramme gespeichert in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_segments(output_dir):\n",
    "    \"\"\"\n",
    "    Diese Funktion vergleicht Mel-Spektrogramme aus zwei verschiedenen Label-Unterordnern, \n",
    "    indem sie die ersten fünf Dateien paarweise nach ihrem Index abgleicht und als Grafik anzeigt.\n",
    "\n",
    "    :param output_dir: Verzeichnis, das die Label-Unterordner mit Mel-Spektrogrammen enthält\n",
    "    \"\"\"\n",
    "\n",
    "    output_dir = Path(output_dir)\n",
    "    label_dirs = [d for d in output_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "    if len(label_dirs) < 2:\n",
    "        raise ValueError(\"Es müssen mindestens zwei Label-Unterordner vorhanden sein.\")\n",
    "\n",
    "    # Sortiere die Label-Unterordner alphabetisch\n",
    "    label_dirs.sort()\n",
    "\n",
    "    # Liste der Dateien in jedem Label-Unterordner\n",
    "    files_per_label = [sorted(label_dir.glob(\"*.png\")) for label_dir in label_dirs]\n",
    "\n",
    "    # Paare von Dateien basierend auf ihrem Index erstellen (maximal 5 Paare)\n",
    "    pairs = list(zip(*[files[::30] for files in files_per_label]))[:5]\n",
    "\n",
    "    # Vergleiche und zeige die Paare als Grafik an\n",
    "    for idx, (file1, file2) in enumerate(pairs):\n",
    "        img1 = plt.imread(file1)\n",
    "        img2 = plt.imread(file2)\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img1)\n",
    "        plt.title(f\"Label 1: {file1.stem[:80]}...\", fontsize=6)  # Verkürzt und kleinere Schriftgröße\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(img2)\n",
    "        plt.title(f\"Label 2: {file2.stem[:80]}...\", fontsize=6)  # Verkürzt und kleinere Schriftgröße\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Vergleich {idx + 1}\")\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "# compare_segments(train_mel_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet_model(input_shape=(128, 128, 3), num_classes=2, dropout_rate=0.2, learning_rate=0.0001, fine_tune_at=None):\n",
    "    #  pretrained ResNet50-Model without classification header\n",
    "    print(\"use Resnet_Model\")\n",
    "    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    \n",
    "    # Optional: freeze base-model\n",
    "    if fine_tune_at is not None:\n",
    "        for layer in base_model.layers[:fine_tune_at]:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    # add classification header\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)  \n",
    "    x = Dropout(dropout_rate)(x)  \n",
    "    x = Dense(128, activation='relu')(x)  \n",
    "    x = Dropout(dropout_rate)(x)  \n",
    "    predictions = Dense(num_classes, activation='softmax')(x)  \n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def calculate_shap_values(model, test_ds, num_samples=1, num_images_to_explain=1):\n",
    "    \"\"\"\n",
    "    Berechnet Shapley-Werte für eine angegebene Anzahl von Bildern aus dem Testdatensatz.\n",
    "\n",
    "    Args:\n",
    "        model: Das zu erklärende Modell.\n",
    "        test_ds: Der Testdatensatz.\n",
    "        num_samples: Anzahl der Batches, die aus dem Testdatensatz genommen werden.\n",
    "        num_images_to_explain: Anzahl der Bilder, für die Shapley-Werte berechnet werden sollen.\n",
    "\n",
    "    Returns:\n",
    "        shap_values_list: Liste der Shapley-Werte für jedes Bild.\n",
    "        test_images_list: Liste der Testbilder, für die die Shapley-Werte berechnet wurden.\n",
    "    \"\"\"\n",
    "    # Testdaten aufbereiten\n",
    "    test_images = []\n",
    "    for images, _ in test_ds.take(num_samples):\n",
    "        test_images.append(images)\n",
    "    test_images = np.concatenate(test_images, axis=0)\n",
    "\n",
    "    # Anzahl der Bilder begrenzen\n",
    "    if num_images_to_explain > test_images.shape[0]:\n",
    "        print(f\"Nur {test_images.shape[0]} Bilder verfügbar, anstatt {num_images_to_explain}.\")\n",
    "        num_images_to_explain = test_images.shape[0]\n",
    "\n",
    "    # Shapley-Werte für jedes Bild berechnen\n",
    "    shap_values_list = []\n",
    "    test_images_list = []\n",
    "\n",
    "    for i in range(num_images_to_explain):\n",
    "        test_image = test_images[i:i+1]  # Einzelnes Bild auswählen\n",
    "\n",
    "        # Explainer initialisieren\n",
    "        explainer = shap.GradientExplainer(model, test_image)\n",
    "\n",
    "        # Shapley-Werte berechnen\n",
    "        shap_values = explainer.shap_values(test_image)\n",
    "\n",
    "        shap_values_list.append(shap_values)\n",
    "        test_images_list.append(test_image)\n",
    "        print(\"Shapley-Werte Form:\", shap_values[0].shape)\n",
    "        print(\"Bild Form:\", test_image.shape)\n",
    "\n",
    "    return shap_values_list, test_images_list\n",
    "\n",
    "def plot_shap_values(shap_values_list, test_images_list):\n",
    "    \"\"\"\n",
    "    Visualisiert die Shapley-Werte für mehrere Bilder.\n",
    "\n",
    "    Args:\n",
    "        shap_values_list: Liste der Shapley-Werte für jedes Bild.\n",
    "        test_images_list: Liste der Testbilder, die visualisiert werden sollen.\n",
    "    \"\"\"\n",
    "    for i, (shap_values, test_image) in enumerate(zip(shap_values_list, test_images_list)):\n",
    "        print(f\"Shapley-Werte für Bild {i+1}:\")\n",
    "        test_image = test_image / 255.0\n",
    "        shap.image_plot(shap_values, test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_results(session):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Extrahieren der Daten aus dem Session-Objekt\n",
    "    history = session.history\n",
    "    metrics = history.history\n",
    "    \n",
    "    # Zugriff auf EarlyStopping Callback und Best-Weight Epoche\n",
    "    early_stopping = session.callbacks[0]  # EarlyStopping Callback\n",
    "    best_epoch = early_stopping.best_epoch  # Epoche des besten Modells (mit restore_best_weights)\n",
    "    \n",
    "    batch_size = session.model_batch_size\n",
    "    \n",
    "    epochs = np.array(history.epoch)\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    # Plot für den Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, metrics[\"loss\"], label=f\"Train Loss {session.model_values[2]:.3f}\")\n",
    "    plt.plot(epochs, metrics[\"val_loss\"], label=f\"Val Loss {session.model_values[0]:.3f}\")\n",
    "\n",
    "    # Vertikale Linie bei der Best-Weights Epoche\n",
    "    if best_epoch is not None:\n",
    "        plt.axvline(\n",
    "            x=best_epoch,  # 1-basierte Epoche\n",
    "            color=\"green\", \n",
    "            linestyle=\"--\",\n",
    "            label=f\"Best Weights Epoch {best_epoch}\"\n",
    "        )\n",
    "\n",
    "    print(session.best_model_values)  \n",
    "\n",
    "    # Überprüfen, ob 'session.best_model_values' nicht None ist\n",
    "    if session.best_model_values is not None:\n",
    "        # Initialisiere die Variablen mit 'N/A'\n",
    "        dropout_value = 'N/A'\n",
    "        regularization_value = 'N/A'\n",
    "        activation_function = 'N/A'\n",
    "\n",
    "        # Iteriere über das Dictionary, um den richtigen Wert für dropout und regularization zu extrahieren\n",
    "        for param, value in session.best_model_values.items():\n",
    "            if param.name == 'dropout':\n",
    "                dropout_value = value  # Der Wert von dropout\n",
    "            elif param.name == 'regularization':\n",
    "                regularization_value = value  # Der Wert von regularization\n",
    "            elif param.name == 'activation':\n",
    "                activation_function = value\n",
    "    else:\n",
    "        dropout_value = 'N/A'\n",
    "        regularization_value = 'N/A'\n",
    "        activation_function = 'N/A'\n",
    "\n",
    "    # Überprüfe die Ausgaben\n",
    "    print(f\"dropout: {dropout_value}, regularization: {regularization_value}, activation: {activation_function}\")\n",
    "\n",
    "    # Jetzt korrektes Anzeigen im Plot\n",
    "    plt.subplots_adjust(bottom=0.65)\n",
    "    plt.figtext(\n",
    "        0.5, 0.01,\n",
    "        f\"Batch Size: {session.model_batch_size}\\n\"\n",
    "        # f\"Training with: {'Mel_Spectogram' if mel_spectogram else 'Spectogram'}\\n\"\n",
    "        f\"HParams: {'Default' if session.best_model_values is None else f'dropout: {dropout_value}, regularization: {regularization_value}, activation: {activation_function}'}\",\n",
    "        fontsize=8, ha=\"center\", va=\"bottom\", color=\"black\"\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylim([0, max(plt.ylim())])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss [CrossEntropy]\")\n",
    "\n",
    "    # Plot für die Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, 100 * np.array(metrics[\"accuracy\"]), label=f\"Train Accuracy {session.model_values[3]:.3f}\")\n",
    "    plt.plot(epochs, 100 * np.array(metrics[\"val_accuracy\"]), label=f\"Val Accuracy {session.model_values[1]:.3f}\")\n",
    "\n",
    "    # Vertikale Linie bei der Best-Weights Epoche\n",
    "    if best_epoch is not None:\n",
    "        plt.axvline(\n",
    "            x=best_epoch,  # 1-basierte Epoche\n",
    "            color=\"green\", \n",
    "            linestyle=\"--\",\n",
    "            label=f\"Best Weights Epoch {best_epoch}\"\n",
    "        )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 100])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy [%]\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"./saved/{adjust_zip_file_path(base_name='history')}.png\")\n",
    "    # plt.savefig(f\"./saved_models/{adjust_zip_file_path(base_name='history')}.png\")\n",
    "    # plt.show()\n",
    "\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_session_as_zip(session, train_ds, zip_dir=\"model_results\"):\n",
    "\n",
    "    # Erstelle das Verzeichnis, falls es nicht existiert\n",
    "    os.makedirs(zip_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Extrahiere Daten für den Dateinamen\n",
    "    num_files = sum(1 for _ in train_ds) * session.model_batch_size  # Gesamtanzahl der Dateien\n",
    "    file_duration_sec = 1  # Hier solltest du die tatsächliche Dauer einer Datei in Sekunden einfügen.\n",
    "\n",
    "    val_loss, val_acc, train_loss, train_acc = session.model_values\n",
    "    val_loss, val_acc = round(val_loss, 3), round(val_acc, 3)\n",
    "    train_loss, train_acc = round(train_loss, 3), round(train_acc, 3)\n",
    "\n",
    "    zip_filename = f\"model_({num_files}-{file_duration_sec})_loss_{train_loss}_acc_{train_acc}_val_loss_{val_loss}_val_acc_{val_acc}.zip\"\n",
    "    zip_path = os.path.join(zip_dir, zip_filename)\n",
    "\n",
    "    # 2. Speichere das Modell\n",
    "    model_path = os.path.join(zip_dir, \"model.h5\")\n",
    "    session.model.save(model_path)\n",
    "\n",
    "    # 3. Generiere und speichere den Plot mit deiner Funktion\n",
    "    plot = model_train_results(session)\n",
    "    plot_path = os.path.join(zip_dir, \"training_plot.png\")\n",
    "    plot.savefig(plot_path)\n",
    "    plot.show()\n",
    "    plot.close()\n",
    "\n",
    "    # Nur das history-Attribut extrahieren\n",
    "    # history_dict = session.history.history\n",
    "    session_path = os.path.join(zip_dir, \"session_data.json\")\n",
    "\n",
    "    # # Speichern als JSON\n",
    "    # with open(session_path, 'w') as json_file:\n",
    "    #     json.dump(history_dict, json_file)\n",
    "\n",
    "    # # Speichern als CSV\n",
    "    # with open(os.path.join(zip_dir,'history.csv'), 'w', newline='') as csv_file:\n",
    "    #     writer = csv.writer(csv_file)\n",
    "    #     # Header schreiben (Schlüssel der Metriken)\n",
    "    #     writer.writerow(['epoch'] + list(history_dict.keys()))\n",
    "    #     # Zeilen schreiben (Epochenwerte)\n",
    "    #     for i in range(len(history_dict['loss'])):\n",
    "    #         row = [i + 1] + [history_dict[key][i] for key in history_dict.keys()]\n",
    "    #         writer.writerow(row)\n",
    "\n",
    "    print(\"History wurde als JSON und CSV gespeichert.\")\n",
    "\n",
    "    # 4. Speichere Variablen in session\n",
    "    session_data = {\n",
    "        \"model_values\": session.model_values,\n",
    "        \"model_batch_size\": session.model_batch_size,\n",
    "        \"best_model_values\": session.best_model_values,\n",
    "        \"history\": session.history.history\n",
    "    }\n",
    "\n",
    "    with open(session_path, \"w\") as f:\n",
    "        json.dump(session_data, f, indent=4)\n",
    "\n",
    "    # 5. Packe alles in eine ZIP-Datei\n",
    "    with zipfile.ZipFile(zip_path, \"w\") as zipf:\n",
    "        zipf.write(model_path, arcname=\"model.h5\")\n",
    "        zipf.write(plot_path, arcname=\"training_plot.png\")\n",
    "        zipf.write(session_path, arcname=\"session_data.json\")\n",
    "\n",
    "    # Aufräumen\n",
    "    os.remove(model_path)\n",
    "    os.remove(plot_path)\n",
    "    os.remove(session_path)\n",
    "\n",
    "    print(f\"Session-Daten erfolgreich in {zip_path} gespeichert.\")\n",
    "    return zip_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_size_from_dir(directory):\n",
    "    \"\"\"Liest die Bildgröße aus der ersten Datei im Verzeichnis.\"\"\"\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    return img.size[::-1]  # (Höhe, Breite)\n",
    "    raise ValueError(\"Keine Bilder im Verzeichnis gefunden!\")\n",
    "\n",
    "def preprocess_data(train_dir, val_dir, test_dir, dropout_rate=0.2, regularization_rate=0.0001, activation='relu'):\n",
    "    global img_height, img_width\n",
    "    img_height, img_width = get_image_size_from_dir(train_dir)\n",
    "    print(f\"Ermittelte Bildgröße: {img_height} x {img_width}\")\n",
    "\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # train_ds = train_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
    "    # val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    # test_ds = test_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def build_model(input_shape=(128, 128, 3), num_classes=2, dropout_rate=0.2, regularization_rate=0.0001, activation='relu'):\n",
    "\n",
    "    print(f\"num_labels: {num_classes}\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "        \n",
    "\n",
    "    print(\"Build Model with:\")\n",
    "    print (f\"dropout_rate = {dropout_rate}\")\n",
    "    print (f\"regularization_rate = {regularization_rate}\")\n",
    "    print (f\"activation = {activation}\")\n",
    "    print(f\"input_shape: {input_shape}\")\n",
    "    print(\"\")\n",
    "    norm_layer = layers.Normalization()\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Resizing(*input_shape[:2]),\n",
    "        norm_layer,\n",
    "        layers.Conv2D(16, 3, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(32, 3, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        # layers.Dropout(dropout_rate),  # Dropout bleibt hier\n",
    "        layers.Conv2D(64, 3, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(128, 3, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        # layers.Dropout(dropout_rate),\n",
    "        layers.Dense(128, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),  # Kleinere Dense-Schicht\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(num_classes, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_and_result():\n",
    "    # # Define the paths for the mel-spectrogram directories\n",
    "    # train_mel_dir = Path(f\"{TRAIN_DIR.stem}_mel_spectrograms\")\n",
    "    # val_mel_dir = Path(f\"{VAL_DIR.stem}_mel_spectrograms\")\n",
    "    # test_mel_dir = Path(f\"{TEST_DIR.stem}_mel_spectrograms\")\n",
    "    \n",
    "    # Load datasets\n",
    "    train_ds, val_ds, test_ds = preprocess_data(train_mel_dir, val_mel_dir, test_mel_dir)\n",
    "\n",
    "    # Build the model\n",
    "    if use_resnet:\n",
    "        model = build_resnet_model(input_shape=(img_height, img_width, 3), num_classes=2, fine_tune_at=100)\n",
    "    else:\n",
    "        model = build_model(input_shape=(img_height, img_width, 3), num_classes=2)\n",
    "        \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=epochs // 4, restore_best_weights=True)\n",
    "    model.summary()\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    # test_loss, test_acc = model.evaluate(test_ds, verbose=2)\n",
    "    trained_epochs = len(history.epoch)\n",
    "    print(f\"Das Training wurde nach {trained_epochs} Epochen gestoppt.\")\n",
    "    best_epoch = early_stopping.best_epoch\n",
    "    print(f\"Das beste Modell wurde in Epoche {best_epoch} gefunden.\")\n",
    "        \n",
    "        \n",
    "    val_loss = history.history['val_loss'][best_epoch]\n",
    "    val_accuracy = history.history['val_accuracy'][best_epoch]\n",
    "    train_loss = history.history['loss'][best_epoch]\n",
    "    train_accuracy = history.history['accuracy'][best_epoch]\n",
    "\n",
    "    batch_size = None\n",
    "    for element in train_ds.take(1):\n",
    "        batch_size = element[0].shape[0] \n",
    "        break\n",
    "        \n",
    "    session = SimpleNamespace(\n",
    "        model=model,\n",
    "        history=history,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping],\n",
    "        model_values = [val_loss, val_accuracy, train_loss, train_accuracy],\n",
    "        model_batch_size = batch_size,\n",
    "        best_model_values = None\n",
    "    )\n",
    "\n",
    "    if use_shap_values:\n",
    "        shap_values_list, test_images_list = calculate_shap_values(model, test_ds, num_samples=1, num_images_to_explain=3)\n",
    "        plot_shap_values(shap_values_list, test_images_list)\n",
    "\n",
    "    # plt = model_train_results(session)\n",
    "    path = save_session_as_zip(session, train_ds)\n",
    "    print(f\"Evaluated model with best weights: val_loss={val_loss}, val_accuracy={val_accuracy}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_images(directory):\n",
    "    print(f\"check images in: {directory}\")\n",
    "    trash_dir = os.path.join(directory, '..', 'trash')  # Trash-Ordner auf gleicher Ebene wie directory\n",
    "    \n",
    "    # Sicherstellen, dass der Trash-Ordner existiert\n",
    "    os.makedirs(trash_dir, exist_ok=True)\n",
    "\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.lower().endswith('.png'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                try:\n",
    "                    with Image.open(file_path) as img:\n",
    "                        img.verify()\n",
    "                except (IOError, SyntaxError):\n",
    "                    print(f\"Fehler bei {filename}, verschiebe nach trash\")\n",
    "                    trash_file_path = os.path.join(trash_dir, os.path.relpath(file_path, directory))\n",
    "                    os.makedirs(os.path.dirname(trash_file_path), exist_ok=True)\n",
    "                    shutil.move(file_path, trash_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Verzeichnis I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture\\new_large_train_ds existiert bereits. Überspringe das Extrahieren.\n",
      "Das Verzeichnis I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture\\medium_test_ds existiert bereits. Überspringe das Extrahieren.\n",
      "Das Verzeichnis I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture\\new_large_val_ds existiert bereits. Überspringe das Extrahieren.\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/Unzipped_Data_Picture complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/large_train_ds complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig-16-44-mono complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale-from-mp3-128 complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/large_train_ds_splits complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig-16-44-mono complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale-from-mp3-128 complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/large_train_ds_trash complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/large_val_ds complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig-16-44-mono complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale-from-mp3-128 complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/large_val_ds_splits complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig-16-44-mono complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale-from-mp3-128 complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/large_val_ds_trash complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/medium_test_ds complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig-16-44-mono complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale-from-mp3-128 complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/medium_test_ds_splits complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig-16-44-mono complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale-from-mp3-128 complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/medium_test_ds_trash complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/medium_train_ds complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig-16-44-mono complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale-from-mp3-128 complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/medium_train_ds_splits complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig-16-44-mono complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale-from-mp3-128 complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/medium_train_ds_trash complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/medium_val_ds complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig-16-44-mono complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale-from-mp3-128 complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/medium_val_ds_splits complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig-16-44-mono complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale-from-mp3-128 complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/medium_val_ds_trash complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/new_large_train_ds complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig-16-44-mono complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale-from-mp3-128 complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/new_large_val_ds complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig-16-44-mono complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale-from-mp3-128 complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/small_test_ds complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/small_test_ds_splits complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/small_test_ds_trash complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/small_train_ds complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig-16-44-mono complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale-from-mp3-128 complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/small_train_ds_splits complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig-16-44-mono complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale-from-mp3-128 complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/small_train_ds_trash complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/small_val_ds complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig-16-44-mono complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale-from-mp3-128 complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/small_val_ds_splits complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/orig-16-44-mono complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/upscale-from-mp3-128 complete\n",
      "renaming of I:\\Uni-Git\\Master\\Tutorial\\Unzipped_Data_Picture/small_val_ds_trash complete\n",
      "Überspringe Verarbeitung, da new_large_train_ds_mel_spectrograms bereits Mel-Spektrogramme enthält.\n",
      "Überspringe Verarbeitung, da new_large_val_ds_mel_spectrograms bereits Mel-Spektrogramme enthält.\n",
      "Überspringe Verarbeitung, da medium_test_ds_mel_spectrograms bereits Mel-Spektrogramme enthält.\n",
      "Ermittelte Bildgröße: 924 x 9300\n",
      "Found 1727 files belonging to 2 classes.\n",
      "Found 506 files belonging to 2 classes.\n",
      "Found 7 files belonging to 2 classes.\n",
      "num_labels: 2\n",
      "\n",
      "\n",
      "Build Model with:\n",
      "dropout_rate = 0.2\n",
      "regularization_rate = 0.0001\n",
      "activation = relu\n",
      "input_shape: (924, 9300, 3)\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resizing (Resizing)         (None, 924, 9300, 3)      0         \n",
      "                                                                 \n",
      " normalization (Normalizatio  (None, 924, 9300, 3)     7         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 922, 9298, 16)     448       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 922, 9298, 16)    64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 461, 4649, 16)    0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 459, 4647, 32)     4640      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 459, 4647, 32)    128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 229, 2323, 32)    0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 227, 2321, 64)     18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 227, 2321, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 113, 1160, 64)    0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 111, 1158, 128)    73856     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 111, 1158, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 55, 579, 128)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4076160)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               521748608 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 521,847,273\n",
      "Trainable params: 521,846,786\n",
      "Non-trainable params: 487\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/sequential/batch_normalization/FusedBatchNormGradV3' defined at (most recent call last):\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\prett\\AppData\\Local\\Temp\\ipykernel_23684\\3900093336.py\", line 30, in <module>\n      path = train_and_result()\n    File \"C:\\Users\\prett\\AppData\\Local\\Temp\\ipykernel_23684\\2024440038.py\", line 106, in train_and_result\n      history = model.fit(\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 576, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\mixed_precision\\loss_scale_optimizer.py\", line 763, in _compute_gradients\n      grads_and_vars = self._optimizer._compute_gradients(\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 634, in _compute_gradients\n      grads_and_vars = self._get_gradients(\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 510, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/sequential/batch_normalization/FusedBatchNormGradV3'\nOOM when allocating a buffer of 7681458176 bytes\n\t [[{{node gradient_tape/sequential/batch_normalization/FusedBatchNormGradV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_2540]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 30\u001b[0m\n\u001b[0;32m     24\u001b[0m     check_images(test_mel_dir)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# compare_segments(train_mel_dir)\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipynb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muse_model_picture\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01muse_model\u001b[39;00m\n\u001b[0;32m     33\u001b[0m use_model\u001b[38;5;241m.\u001b[39mrun(path)\n",
      "Cell \u001b[1;32mIn[9], line 106\u001b[0m, in \u001b[0;36mtrain_and_result\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# test_loss, test_acc = model.evaluate(test_ds, verbose=2)\u001b[39;00m\n\u001b[0;32m    115\u001b[0m trained_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(history\u001b[38;5;241m.\u001b[39mepoch)\n",
      "File \u001b[1;32mc:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/sequential/batch_normalization/FusedBatchNormGradV3' defined at (most recent call last):\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\prett\\AppData\\Local\\Temp\\ipykernel_23684\\3900093336.py\", line 30, in <module>\n      path = train_and_result()\n    File \"C:\\Users\\prett\\AppData\\Local\\Temp\\ipykernel_23684\\2024440038.py\", line 106, in train_and_result\n      history = model.fit(\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 576, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\mixed_precision\\loss_scale_optimizer.py\", line 763, in _compute_gradients\n      grads_and_vars = self._optimizer._compute_gradients(\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 634, in _compute_gradients\n      grads_and_vars = self._get_gradients(\n    File \"c:\\Users\\prett\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 510, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/sequential/batch_normalization/FusedBatchNormGradV3'\nOOM when allocating a buffer of 7681458176 bytes\n\t [[{{node gradient_tape/sequential/batch_normalization/FusedBatchNormGradV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_2540]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    extract_zip(TRAIN_DIR, UNZIP_DIR)\n",
    "    extract_zip(TEST_DIR, UNZIP_DIR)\n",
    "    extract_zip(VAL_DIR, UNZIP_DIR)\n",
    "    rename_audio_files(UNZIP_DIR)\n",
    "\n",
    "    # print(f\"Split Audio Data in {1000}ms\")\n",
    "    # TRAIN_DIR=split_audio_dataset(TRAIN_DIR)\n",
    "    # VAL_DIR=split_audio_dataset(VAL_DIR)\n",
    "    # TEST_DIR=split_audio_dataset(TEST_DIR)\n",
    "\n",
    "    train_mel_dir = Path(f\"{TRAIN_DIR.stem}_mel_spectrograms\")  # Der Zielordner für die Mel-Spektrogramme\n",
    "    generate_mel_spectrograms_with_structure(TRAIN_DIR.resolve(), train_mel_dir)\n",
    "\n",
    "    val_mel_dir = Path(f\"{VAL_DIR.stem}_mel_spectrograms\")  # Der Zielordner für die Mel-Spektrogramme\n",
    "    generate_mel_spectrograms_with_structure(VAL_DIR, val_mel_dir)\n",
    "\n",
    "    test_mel_dir = Path(f\"{TEST_DIR.stem}_mel_spectrograms\")  # Der Zielordner für die Mel-Spektrogramme\n",
    "    generate_mel_spectrograms_with_structure(TEST_DIR, test_mel_dir)\n",
    "\n",
    "    if check_image:\n",
    "        check_images(train_mel_dir)\n",
    "        check_images(val_mel_dir)\n",
    "        check_images(test_mel_dir)\n",
    "\n",
    "\n",
    "\n",
    "    # compare_segments(train_mel_dir)\n",
    "\n",
    "    path = train_and_result()\n",
    "\n",
    "    import ipynb.fs.defs.use_model_picture as use_model\n",
    "    use_model.run(path)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
