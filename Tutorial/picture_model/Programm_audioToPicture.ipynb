{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import zipfile\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from collections import Counter\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_silence\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from types import SimpleNamespace\n",
    "import traceback\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "img_height, img_width = 128, 128\n",
    "BATCH_SIZE=512\n",
    "epochs=100\n",
    "\n",
    "# Globale Pfade für die Daten\n",
    "ROOT_DIR = Path('../').resolve()  # Hauptverzeichnis\n",
    "ZIP_DIR = ROOT_DIR / 'data'  # Ordner, der die ZIP-Dateien enthält\n",
    "UNZIP_DIR = ROOT_DIR / 'Unzipped_Data_Picture'  # Zielordner für entpackte Dateien\n",
    "\n",
    "# Small Dataset\n",
    "TRAIN_DIR = UNZIP_DIR / 'small_train_ds'\n",
    "VAL_DIR = UNZIP_DIR / 'small_val_ds'\n",
    "TEST_DIR = UNZIP_DIR / 'small_test_ds'\n",
    "\n",
    "# # Small 3 Labels Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / '3_small_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / '3_small_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / '3_small_test_ds'\n",
    "\n",
    "# # Medium Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / 'medium_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / 'medium_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / 'medium_test_ds'\n",
    "\n",
    "# # Large Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / 'large_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / 'large_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / 'medium_test_ds'\n",
    "\n",
    "# # Large 3 Labels Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / '3_large_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / '3_large_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / '3_large_test_ds'\n",
    "\n",
    "# # No_mod Dataset\n",
    "# TRAIN_DIR = UNZIP_DIR / 'no_mod_train_ds'\n",
    "# VAL_DIR = UNZIP_DIR / 'no_mod_val_ds'\n",
    "# TEST_DIR = UNZIP_DIR / 'no_mod_test_ds'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip(zip_path, extract_to):\n",
    "    zip_path_str = str(zip_path)\n",
    "    \n",
    "    if not zip_path_str.endswith('.zip'):\n",
    "        zip_path_str += '.zip'\n",
    "    \n",
    "    zip_file_path = pathlib.Path(zip_path_str)\n",
    "    \n",
    "    folder_name = zip_file_path.stem \n",
    "    target_folder = pathlib.Path(extract_to) / folder_name\n",
    "    \n",
    "    if target_folder.exists():\n",
    "        print(f\"Das Verzeichnis {target_folder} existiert bereits. Überspringe das Extrahieren.\")\n",
    "    else:\n",
    "        if zip_file_path.exists():\n",
    "            print(f\"Extrahiere die Zip-Datei {zip_file_path} nach {extract_to}.\")\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_to)\n",
    "            print(f\"Zip-Datei {zip_file_path} erfolgreich extrahiert.\")\n",
    "        else:\n",
    "            print(f\"Die Zip-Datei {zip_file_path} existiert nicht.\")\n",
    "\n",
    "def rename_audio_files(root_path):\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        parent_folder = os.path.basename(root)\n",
    "        for file in files:\n",
    "            if not file.startswith(f\"{parent_folder}_\"):\n",
    "                if file.endswith(('.wav', '.mp3')):  \n",
    "                    \n",
    "                    old_file_path = os.path.join(root, file)\n",
    "                    new_file_name = f\"{parent_folder}_{file}\"\n",
    "                    new_file_path = os.path.join(root, new_file_name)\n",
    "                        \n",
    "                    os.rename(old_file_path, new_file_path)\n",
    "        print(f\"renaming of {root_path}/{parent_folder} complete\")\n",
    "\n",
    "\n",
    "# extract_zip(TRAIN_DIR, UNZIP_DIR)\n",
    "# extract_zip(TEST_DIR, UNZIP_DIR)\n",
    "# extract_zip(VAL_DIR, UNZIP_DIR)\n",
    "# rename_audio_files(UNZIP_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statistics(total_segments, retaining_segments, discarded_segments, trash_dir):\n",
    "    # Create a bar chart for the statistics\n",
    "    labels = [f'Total Segments: {total_segments}', f'Valid Segments: {retaining_segments}', f'Discarded Segments: {discarded_segments}']\n",
    "    values = [total_segments, retaining_segments, discarded_segments]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(labels, values, color=['blue', 'green', 'red'])\n",
    "    plt.title('Audio Splitting Statistics')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(trash_dir / 'splitting_statistics.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Statistics saved as PNG in {trash_dir}\")\n",
    "\n",
    "def contains_completely_silent_part(segment, silence_threshold=-90.0, min_silence_len=100):\n",
    "    \"\"\"Check if a segment contains any completely silent part.\"\"\"\n",
    "    silence_ranges = detect_silence(\n",
    "        segment, min_silence_len=min_silence_len, silence_thresh=silence_threshold\n",
    "    )\n",
    "    # If any silence range exists that is equal to or exceeds min_silence_len, return True\n",
    "    return any(end - start >= min_silence_len for start, end in silence_ranges)\n",
    "\n",
    "def plot_spectrogram(wav_path, output_dir):\n",
    "    \"\"\"Plot and save the spectrogram of a wav file.\"\"\"\n",
    "    # Load the audio file using librosa (this also returns the sample rate)\n",
    "    samples, sample_rate = librosa.load(wav_path, sr=None)\n",
    "    \n",
    "    # Generate the spectrogram (using a Short-Time Fourier Transform, STFT)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Compute the spectrogram (logarithmic scale for better visualization)\n",
    "    D = librosa.amplitude_to_db(librosa.stft(samples), ref=np.max)\n",
    "    \n",
    "    # Display the spectrogram\n",
    "    librosa.display.specshow(D, sr=sample_rate, x_axis='time', y_axis='log', cmap='viridis')\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title(f\"Spectrogram of {Path(wav_path).name}\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.colorbar(label=\"Intensity (dB)\")\n",
    "\n",
    "    # Save the plot\n",
    "    spectrogram_path = output_dir / f\"{Path(wav_path).stem}_spectrogram.png\"\n",
    "    plt.savefig(spectrogram_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Spectrogram saved: {spectrogram_path}\")\n",
    "\n",
    "def split_audio_dataset(input_dir):\n",
    "        segment_length_ms = 1000\n",
    "        overlap_ms = 500  # 50% Überlappung\n",
    "\n",
    "        output_dir = input_dir.parent / f\"{input_dir.name}_splits\"\n",
    "        trash_dir = input_dir.parent / f\"{input_dir.name}_trash\"\n",
    "        \n",
    "        if output_dir.exists():\n",
    "            print(f\"Output directory already exists: {output_dir}. Skipping splitting.\")\n",
    "            return output_dir\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(trash_dir, exist_ok=True)\n",
    "\n",
    "        total_segments = 0\n",
    "        retaining_segments = 0\n",
    "        discarded_segments = 0\n",
    "\n",
    "        # Traverse the input directory and split files\n",
    "        for subdir, _, files in os.walk(input_dir):\n",
    "            relative_path = Path(subdir).relative_to(input_dir)  # Maintain subdirectory structure\n",
    "            target_dir = output_dir / relative_path\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):  # Adjust formats as needed\n",
    "                    input_path = Path(subdir) / file\n",
    "                    audio = AudioSegment.from_file(input_path)\n",
    "                    duration = len(audio)  # Total length of the audio in milliseconds\n",
    "\n",
    "                    # Split and export segments\n",
    "                    for i, start_time in enumerate(range(0, duration, segment_length_ms - overlap_ms)):\n",
    "                        end_time = min(start_time + segment_length_ms, duration)\n",
    "                        segment = audio[start_time:end_time]\n",
    "                        total_segments +=1\n",
    "                        file_path = Path(file)  # Convert file string to Path\n",
    "                        # segment_filename = target_dir / f\"{file_path.stem}_segment_{i}.wav\"\n",
    "                        if contains_completely_silent_part(segment):\n",
    "                            discarded_segments += 1\n",
    "                            segment_filename = trash_dir / f\"{file_path.stem}_segment_{i}.wav\"\n",
    "                            segment.export(segment_filename, format=\"wav\")\n",
    "                            plot_spectrogram(segment_filename, trash_dir)\n",
    "                        else:\n",
    "                            retaining_segments += 1\n",
    "                            segment_filename = target_dir / f\"{file_path.stem}_segment_{i}.wav\"\n",
    "                            segment.export(segment_filename, format=\"wav\")\n",
    "                            \n",
    "                    print(f\"Processed {file} into {i+1} segments in {target_dir}\")\n",
    "\n",
    "        generate_statistics(total_segments, retaining_segments, discarded_segments, trash_dir)\n",
    "        print(f\"All files processed. Split dataset saved in {output_dir}\")\n",
    "        return output_dir\n",
    "\n",
    "\n",
    "# print(f\"Split Audio Data in {1000}ms\")\n",
    "# TRAIN_DIR=split_audio_dataset(TRAIN_DIR)\n",
    "# VAL_DIR=split_audio_dataset(VAL_DIR)\n",
    "# TEST_DIR=split_audio_dataset(TEST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel-Spektogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_file(audio_file, input_dir, output_dir, n_mels, fmin, fmax):\n",
    "    \"\"\"\n",
    "    Diese Funktion verarbeitet eine einzelne Audiodatei und berechnet das Mel-Spektrogramm.\n",
    "    \"\"\"\n",
    "    # Relativer Pfad zur Eingabedatei\n",
    "    relative_path = audio_file.relative_to(input_dir)\n",
    "    \n",
    "    # Zielpfad basierend auf der ursprünglichen Ordnerstruktur\n",
    "    target_dir = output_dir / relative_path.parent\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def trim_to_zero_crossings(y, sr):\n",
    "        start_sample = librosa.zero_crossings(y, pad=False).argmax()\n",
    "        end_sample = len(y) - librosa.zero_crossings(y[::-1], pad=False).argmax()\n",
    "        return y[start_sample:end_sample]\n",
    "    # Lade die Audiodatei mit librosa\n",
    "    y, sr = librosa.load(audio_file, sr=44100)\n",
    "    y = trim_to_zero_crossings(y, sr)\n",
    "\n",
    "    # Berechne das Mel-Spektrogramm\n",
    "    # padding = 1024\n",
    "    # y = np.pad(y, (padding, padding), mode='constant')\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
    "\n",
    "    # Konvertiere das Mel-Spektrogramm in dB (logarithmische Skala)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Rückgabe der Daten ohne das Plotten\n",
    "    return relative_path, mel_spectrogram, sr, target_dir, audio_file.stem\n",
    "\n",
    "def generate_mel_spectrograms_with_structure(input_dir, output_dir, n_mels=128, fmin=20, fmax=44100, batch_size=200):\n",
    "    \"\"\"\n",
    "    Optimierte Funktion für die Verarbeitung von Mel-Spektrogrammen:\n",
    "    1. Berechnung wird parallelisiert.\n",
    "    2. Ergebnisse werden sequentiell geplottet, um Thread-Sicherheitsprobleme zu vermeiden.\n",
    "    3. Batches werden verwendet, um den Speicherverbrauch zu kontrollieren.\n",
    "    \"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "\n",
    "    # Überprüfen, ob die Ordnerstruktur bereits existiert\n",
    "    if output_dir.exists() and any(output_dir.rglob(\"*.png\")):\n",
    "        print(f\"Überspringe Verarbeitung, da {output_dir} bereits Mel-Spektrogramme enthält.\")\n",
    "        return\n",
    "\n",
    "    # Liste der .wav-Dateien im input_dir\n",
    "    audio_files = list(input_dir.rglob(\"*.wav\"))\n",
    "\n",
    "    if not audio_files:\n",
    "        print(\"Keine Audiodateien gefunden.\")\n",
    "        return\n",
    "\n",
    "    total_files = len(audio_files)\n",
    "    print(f\"{total_files} Audiodateien gefunden. Verarbeitung startet.\")\n",
    "\n",
    "    # Verarbeite die Dateien in Batches\n",
    "    for batch_start in range(0, total_files, batch_size):\n",
    "        batch_files = audio_files[batch_start:batch_start + batch_size]\n",
    "        print(f\"Verarbeite Batch {batch_start // batch_size + 1} von {total_files // batch_size + 1}\")\n",
    "\n",
    "        # Parallele Berechnung der Mel-Spektrogramme\n",
    "        results = []\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [\n",
    "                executor.submit(process_audio_file, audio_file, input_dir, output_dir, n_mels, fmin, fmax)\n",
    "                for audio_file in batch_files\n",
    "            ]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                results.append(future.result())\n",
    "\n",
    "        # Sequentielles Plotten und Speichern\n",
    "        for relative_path, mel_spectrogram_db, sr, target_dir, audio_file_stem in results:\n",
    "            mel_spectrogram_path = target_dir / f\"{audio_file_stem}_mel_spectrogram.png\"\n",
    "\n",
    "            # Überspringen, wenn das Spektrogramm bereits existiert\n",
    "            if mel_spectrogram_path.exists():\n",
    "                print(f\"Spektrogramm {mel_spectrogram_path} existiert bereits. Überspringen.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Erstelle das Bild des Mel-Spektrogramms\n",
    "                plt.figure(figsize=(2, 2))\n",
    "                librosa.display.specshow(mel_spectrogram_db, x_axis='time', y_axis='mel', sr=sr, cmap='magma')\n",
    "                plt.axis('off')\n",
    "\n",
    "                # Speichern des Bildes als PNG\n",
    "                plt.savefig(mel_spectrogram_path, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "                plt.close()\n",
    "                print(f\"Mel-Spektrogramm für {audio_file_stem} gespeichert in {mel_spectrogram_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Plotten von {audio_file_stem}: {e}\")\n",
    "            finally:\n",
    "                # Speicher freigeben\n",
    "                del mel_spectrogram_db\n",
    "                gc.collect()\n",
    "\n",
    "    print(f\"Alle Mel-Spektrogramme gespeichert in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_segments(output_dir):\n",
    "    \"\"\"\n",
    "    Diese Funktion vergleicht Mel-Spektrogramme aus zwei verschiedenen Label-Unterordnern, \n",
    "    indem sie die ersten fünf Dateien paarweise nach ihrem Index abgleicht und als Grafik anzeigt.\n",
    "\n",
    "    :param output_dir: Verzeichnis, das die Label-Unterordner mit Mel-Spektrogrammen enthält\n",
    "    \"\"\"\n",
    "\n",
    "    output_dir = Path(output_dir)\n",
    "    label_dirs = [d for d in output_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "    if len(label_dirs) < 2:\n",
    "        raise ValueError(\"Es müssen mindestens zwei Label-Unterordner vorhanden sein.\")\n",
    "\n",
    "    # Sortiere die Label-Unterordner alphabetisch\n",
    "    label_dirs.sort()\n",
    "\n",
    "    # Liste der Dateien in jedem Label-Unterordner\n",
    "    files_per_label = [sorted(label_dir.glob(\"*.png\")) for label_dir in label_dirs]\n",
    "\n",
    "    # Paare von Dateien basierend auf ihrem Index erstellen (maximal 5 Paare)\n",
    "    pairs = list(zip(*[files[::30] for files in files_per_label]))[:5]\n",
    "\n",
    "    # Vergleiche und zeige die Paare als Grafik an\n",
    "    for idx, (file1, file2) in enumerate(pairs):\n",
    "        img1 = plt.imread(file1)\n",
    "        img2 = plt.imread(file2)\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img1)\n",
    "        plt.title(f\"Label 1: {file1.stem[:80]}...\", fontsize=6)  # Verkürzt und kleinere Schriftgröße\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(img2)\n",
    "        plt.title(f\"Label 2: {file2.stem[:80]}...\", fontsize=6)  # Verkürzt und kleinere Schriftgröße\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Vergleich {idx + 1}\")\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "# compare_segments(train_mel_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_results(session):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Extrahieren der Daten aus dem Session-Objekt\n",
    "    history = session.history\n",
    "    metrics = history.history\n",
    "    \n",
    "    # Zugriff auf EarlyStopping Callback und Best-Weight Epoche\n",
    "    early_stopping = session.callbacks[0]  # EarlyStopping Callback\n",
    "    best_epoch = early_stopping.best_epoch  # Epoche des besten Modells (mit restore_best_weights)\n",
    "    \n",
    "    batch_size = session.model_batch_size\n",
    "    \n",
    "    epochs = np.array(history.epoch)\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    # Plot für den Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, metrics[\"loss\"], label=f\"Train Loss {session.model_values[2]:.3f}\")\n",
    "    plt.plot(epochs, metrics[\"val_loss\"], label=f\"Val Loss {session.model_values[0]:.3f}\")\n",
    "\n",
    "    # Vertikale Linie bei der Best-Weights Epoche\n",
    "    if best_epoch is not None:\n",
    "        plt.axvline(\n",
    "            x=best_epoch,  # 1-basierte Epoche\n",
    "            color=\"green\", \n",
    "            linestyle=\"--\",\n",
    "            label=f\"Best Weights Epoch {best_epoch}\"\n",
    "        )\n",
    "\n",
    "    print(session.best_model_values)  \n",
    "\n",
    "    # Überprüfen, ob 'session.best_model_values' nicht None ist\n",
    "    if session.best_model_values is not None:\n",
    "        # Initialisiere die Variablen mit 'N/A'\n",
    "        dropout_value = 'N/A'\n",
    "        regularization_value = 'N/A'\n",
    "        activation_function = 'N/A'\n",
    "\n",
    "        # Iteriere über das Dictionary, um den richtigen Wert für dropout und regularization zu extrahieren\n",
    "        for param, value in session.best_model_values.items():\n",
    "            if param.name == 'dropout':\n",
    "                dropout_value = value  # Der Wert von dropout\n",
    "            elif param.name == 'regularization':\n",
    "                regularization_value = value  # Der Wert von regularization\n",
    "            elif param.name == 'activation':\n",
    "                activation_function = value\n",
    "    else:\n",
    "        dropout_value = 'N/A'\n",
    "        regularization_value = 'N/A'\n",
    "        activation_function = 'N/A'\n",
    "\n",
    "    # Überprüfe die Ausgaben\n",
    "    print(f\"dropout: {dropout_value}, regularization: {regularization_value}, activation: {activation_function}\")\n",
    "\n",
    "    # Jetzt korrektes Anzeigen im Plot\n",
    "    plt.subplots_adjust(bottom=0.65)\n",
    "    plt.figtext(\n",
    "        0.5, 0.01,\n",
    "        f\"Batch Size: {session.model_batch_size}\\n\"\n",
    "        # f\"Training with: {'Mel_Spectogram' if mel_spectogram else 'Spectogram'}\\n\"\n",
    "        f\"HParams: {'Default' if session.best_model_values is None else f'dropout: {dropout_value}, regularization: {regularization_value}, activation: {activation_function}'}\",\n",
    "        fontsize=8, ha=\"center\", va=\"bottom\", color=\"black\"\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylim([0, max(plt.ylim())])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss [CrossEntropy]\")\n",
    "\n",
    "    # Plot für die Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, 100 * np.array(metrics[\"accuracy\"]), label=f\"Train Accuracy {session.model_values[3]:.3f}\")\n",
    "    plt.plot(epochs, 100 * np.array(metrics[\"val_accuracy\"]), label=f\"Val Accuracy {session.model_values[1]:.3f}\")\n",
    "\n",
    "    # Vertikale Linie bei der Best-Weights Epoche\n",
    "    if best_epoch is not None:\n",
    "        plt.axvline(\n",
    "            x=best_epoch,  # 1-basierte Epoche\n",
    "            color=\"green\", \n",
    "            linestyle=\"--\",\n",
    "            label=f\"Best Weights Epoch {best_epoch}\"\n",
    "        )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 100])\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy [%]\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"./saved/{adjust_zip_file_path(base_name='history')}.png\")\n",
    "    # plt.savefig(f\"./saved_models/{adjust_zip_file_path(base_name='history')}.png\")\n",
    "    # plt.show()\n",
    "\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_session_as_zip(session, train_ds, zip_dir=\"model_results\"):\n",
    "\n",
    "    # Erstelle das Verzeichnis, falls es nicht existiert\n",
    "    os.makedirs(zip_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Extrahiere Daten für den Dateinamen\n",
    "    num_files = sum(1 for _ in train_ds) * session.model_batch_size  # Gesamtanzahl der Dateien\n",
    "    file_duration_sec = 1  # Hier solltest du die tatsächliche Dauer einer Datei in Sekunden einfügen.\n",
    "\n",
    "    val_loss, val_acc, train_loss, train_acc = session.model_values\n",
    "    val_loss, val_acc = round(val_loss, 3), round(val_acc, 3)\n",
    "    train_loss, train_acc = round(train_loss, 3), round(train_acc, 3)\n",
    "\n",
    "    zip_filename = f\"model_({num_files}-{file_duration_sec})_loss_{train_loss}_acc_{train_acc}_val_loss_{val_loss}_val_acc_{val_acc}.zip\"\n",
    "    zip_path = os.path.join(zip_dir, zip_filename)\n",
    "\n",
    "    # 2. Speichere das Modell\n",
    "    model_path = os.path.join(zip_dir, \"model.h5\")\n",
    "    session.model.save(model_path)\n",
    "\n",
    "    # 3. Generiere und speichere den Plot mit deiner Funktion\n",
    "    plot = model_train_results(session)\n",
    "    plot_path = os.path.join(zip_dir, \"training_plot.png\")\n",
    "    plot.savefig(plot_path)\n",
    "    plot.show()\n",
    "    plot.close()\n",
    "\n",
    "    # 4. Speichere Variablen in session\n",
    "    session_data = {\n",
    "        \"model_values\": session.model_values,\n",
    "        \"model_batch_size\": session.model_batch_size,\n",
    "        \"best_model_values\": session.best_model_values,\n",
    "    }\n",
    "    session_path = os.path.join(zip_dir, \"session_data.json\")\n",
    "    with open(session_path, \"w\") as f:\n",
    "        json.dump(session_data, f, indent=4)\n",
    "\n",
    "    # 5. Packe alles in eine ZIP-Datei\n",
    "    with zipfile.ZipFile(zip_path, \"w\") as zipf:\n",
    "        zipf.write(model_path, arcname=\"model.h5\")\n",
    "        zipf.write(plot_path, arcname=\"training_plot.png\")\n",
    "        zipf.write(session_path, arcname=\"session_data.json\")\n",
    "\n",
    "    # Aufräumen\n",
    "    os.remove(model_path)\n",
    "    os.remove(plot_path)\n",
    "    os.remove(session_path)\n",
    "\n",
    "    print(f\"Session-Daten erfolgreich in {zip_path} gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_dir, val_dir, test_dir, dropout_rate=0.5, regularization_rate=0.001, activation='relu'):\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    train_ds = train_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def build_model(input_shape=(128, 128, 3), num_classes=2, dropout_rate=0.2, regularization_rate=0.001, activation='relu'):\n",
    "\n",
    "    norm_layer = layers.Normalization()\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Resizing(128, 128),\n",
    "        norm_layer,\n",
    "        layers.Conv2D(16, 3, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(32, 3, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        # layers.Dropout(dropout_rate),  # Dropout bleibt hier\n",
    "        layers.Conv2D(64, 3, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(128, 3, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        # layers.Dropout(dropout_rate),\n",
    "        layers.Dense(128, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(regularization_rate)),  # Kleinere Dense-Schicht\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(num_classes, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_and_result():\n",
    "    # # Define the paths for the mel-spectrogram directories\n",
    "    # train_mel_dir = Path(f\"{TRAIN_DIR.stem}_mel_spectrograms\")\n",
    "    # val_mel_dir = Path(f\"{VAL_DIR.stem}_mel_spectrograms\")\n",
    "    # test_mel_dir = Path(f\"{TEST_DIR.stem}_mel_spectrograms\")\n",
    "    \n",
    "    # Load datasets\n",
    "    train_ds, val_ds, test_ds = preprocess_data(train_mel_dir, val_mel_dir, test_mel_dir)\n",
    "\n",
    "    # Build the model\n",
    "    model = build_model(input_shape=(img_height, img_width, 3), num_classes=2)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=epochs // 4, restore_best_weights=True)\n",
    "    \n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_acc = model.evaluate(test_ds, verbose=2)\n",
    "    trained_epochs = len(history.epoch)\n",
    "    print(f\"Das Training wurde nach {trained_epochs} Epochen gestoppt.\")\n",
    "    best_epoch = early_stopping.best_epoch\n",
    "    print(f\"Das beste Modell wurde in Epoche {best_epoch} gefunden.\")\n",
    "        \n",
    "        \n",
    "    val_loss = history.history['val_loss'][best_epoch]\n",
    "    val_accuracy = history.history['val_accuracy'][best_epoch]\n",
    "    train_loss = history.history['loss'][best_epoch]\n",
    "    train_accuracy = history.history['accuracy'][best_epoch]\n",
    "\n",
    "    batch_size = None\n",
    "    for element in train_ds.take(1):\n",
    "        batch_size = element[0].shape[0] \n",
    "        break\n",
    "        \n",
    "    session = SimpleNamespace(\n",
    "        model=model,\n",
    "        history=history,\n",
    "        # epochs=EPOCHS,\n",
    "        callbacks=[early_stopping],\n",
    "        model_values = [val_loss, val_accuracy, train_loss, train_accuracy],\n",
    "        model_batch_size = batch_size,\n",
    "        best_model_values = None\n",
    "    )\n",
    "    # plt = model_train_results(session)\n",
    "    save_session_as_zip(session, train_ds)\n",
    "    print(f\"Evaluated model with best weights: val_loss={val_loss}, val_accuracy={val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    extract_zip(TRAIN_DIR, UNZIP_DIR)\n",
    "    extract_zip(TEST_DIR, UNZIP_DIR)\n",
    "    extract_zip(VAL_DIR, UNZIP_DIR)\n",
    "    rename_audio_files(UNZIP_DIR)\n",
    "\n",
    "    print(f\"Split Audio Data in {1000}ms\")\n",
    "    TRAIN_DIR=split_audio_dataset(TRAIN_DIR)\n",
    "    VAL_DIR=split_audio_dataset(VAL_DIR)\n",
    "    TEST_DIR=split_audio_dataset(TEST_DIR)\n",
    "\n",
    "    train_mel_dir = Path(f\"{TRAIN_DIR.stem}_mel_spectrograms\")  # Der Zielordner für die Mel-Spektrogramme\n",
    "    generate_mel_spectrograms_with_structure(TRAIN_DIR.resolve(), train_mel_dir)\n",
    "\n",
    "    val_mel_dir = Path(f\"{VAL_DIR.stem}_mel_spectrograms\")  # Der Zielordner für die Mel-Spektrogramme\n",
    "    generate_mel_spectrograms_with_structure(VAL_DIR, val_mel_dir)\n",
    "\n",
    "    test_mel_dir = Path(f\"{TEST_DIR.stem}_mel_spectrograms\")  # Der Zielordner für die Mel-Spektrogramme\n",
    "    generate_mel_spectrograms_with_structure(TEST_DIR, test_mel_dir)\n",
    "\n",
    "    compare_segments(train_mel_dir)\n",
    "\n",
    "    train_and_result()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
