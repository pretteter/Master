{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "import os\n",
    "import librosa\n",
    "import pathlib\n",
    "# import tensorflow_io as tfio  \n",
    "\n",
    "label_names = None\n",
    "example_labels = None\n",
    "example_audio = None\n",
    "train_ds = None\n",
    "val_ds = None\n",
    "test_ds = None\n",
    "waveform = None \n",
    "\n",
    "spectrogram = None\n",
    "label = None\n",
    "\n",
    "train_spectrogram_ds = None\n",
    "val_spectrogram_ds = None\n",
    "test_spectrogram_ds = None\n",
    "\n",
    "mel_spectogram = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert waveforms to spectrograms\n",
    "\n",
    "The waveforms in the dataset are represented in the time domain. Next, you'll transform the waveforms from the time-domain signals into the time-frequency-domain signals by computing the [short-time Fourier transform (STFT)](https://en.wikipedia.org/wiki/Short-time_Fourier_transform) to convert the waveforms to as [spectrograms](https://en.wikipedia.org/wiki/Spectrogram), which show frequency changes over time and can be represented as 2D images. You will feed the spectrogram images into your neural network to train the model.\n",
    "\n",
    "A Fourier transform (`tf.signal.fft`) converts a signal to its component frequencies, but loses all time information. In comparison, STFT (`tf.signal.stft`) splits the signal into windows of time and runs a Fourier transform on each window, preserving some time information, and returning a 2D tensor that you can run standard convolutions on.\n",
    "\n",
    "Create a utility function for converting waveforms to spectrograms:\n",
    "\n",
    "- The waveforms need to be of the same length, so that when you convert them to spectrograms, the results have similar dimensions. This can be done by simply zero-padding the audio clips that are shorter than one second (using `tf.zeros`).\n",
    "- When calling `tf.signal.stft`, choose the `frame_length` and `frame_step` parameters such that the generated spectrogram \"image\" is almost square. For more information on the STFT parameters choice, refer to [this Coursera video](https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe) on audio signal processing and STFT.\n",
    "- The STFT produces an array of complex numbers representing magnitude and phase. However, in this tutorial you'll only use the magnitude, which you can derive by applying `tf.abs` on the output of `tf.signal.stft`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mel_spectrogram(waveform):\n",
    "    def _get_mel_spectrogram(waveform_np):\n",
    "        # Konvertiere die Audiodaten in ein Mel-Spektrogramm\n",
    "        spec = librosa.feature.melspectrogram(y=waveform_np, sr=44100)\n",
    "        # Logarithmiere das Spektrogramm, um es für ML besser nutzbar zu machen\n",
    "        log_spec = librosa.power_to_db(spec, ref=np.max)\n",
    "        return log_spec\n",
    "\n",
    "    # Verwende tf.numpy_function, um die NumPy-basierte Funktion aufzurufen\n",
    "    spectrogram = tf.numpy_function(\n",
    "        func=_get_mel_spectrogram,  # Die Funktion, die auf NumPy läuft\n",
    "        inp=[waveform],             # Eingabe (TensorFlow Tensor)\n",
    "        Tout=tf.float32             # Ausgabe-Typ (TensorFlow-Tensor)\n",
    "    )\n",
    "\n",
    "    # Stelle sicher, dass das Mel-Spektrogramm die richtige Dimension hat\n",
    "    # Setze die exakte Form des Mel-Spektrogramms\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=-1)  # Füge die Kanal-Dimension hinzu\n",
    "\n",
    "    # Die Form des Mel-Spektrogramms könnte (None, None, 1) sein, wenn es in einer dynamischen Pipeline ist.\n",
    "    # Wenn du eine statische Form benötigst, setze sie explizit.\n",
    "    spectrogram.set_shape([None, 128, 87, 1])  # Setze die Form explizit\n",
    "\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "    spec=None\n",
    "    if(mel_spectogram):\n",
    "        spec = get_mel_spectrogram(waveform)\n",
    "    else:\n",
    "\n",
    "        # Convert the waveform to a spectrogram via a STFT.\n",
    "        spectrogram = tf.signal.stft(\n",
    "            waveform, frame_length=255, frame_step=128)\n",
    "        # Obtain the magnitude of the STFT.\n",
    "        spectrogram = tf.abs(spectrogram)\n",
    "        # Add a `channels` dimension, so that the spectrogram can be used\n",
    "        # as image-like input data with convolution layers (which expect\n",
    "        # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "        spectrogram = spectrogram[..., tf.newaxis]\n",
    "        spec = spectrogram\n",
    "\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, start exploring the data. Print the shapes of one example's tensorized waveform and the corresponding spectrogram, and play the original audio:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play():\n",
    "  global waveform\n",
    "  global label\n",
    "  global spectrogram\n",
    "  for i in range(3):\n",
    "    label = label_names[example_labels[i]]\n",
    "    waveform = example_audio[i]\n",
    "    spectrogram = get_spectrogram(waveform)\n",
    "    # spectrogram = get_mel_spectrogram(waveform)\n",
    "\n",
    "    print('Label:', label)\n",
    "    print('Waveform shape:', waveform.shape)\n",
    "    print('Spectrogram shape:', spectrogram.shape)\n",
    "    print('Audio playback')\n",
    "    display.display(display.Audio(waveform, rate=44100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define a function for displaying a spectrogram:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram, ax, sr=44100, duration=None):\n",
    "    if len(spectrogram.shape) > 2:\n",
    "        spectrogram = np.squeeze(spectrogram, axis=-1)\n",
    "\n",
    "    # Logarithmische Skalierung der Spektrogramm-Daten\n",
    "    log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
    "    height, width = log_spec.shape\n",
    "\n",
    "    # Berechnung der x-Achse basierend auf der Audiodauer\n",
    "    if duration is None:\n",
    "        duration = width / sr  # Dauer in Sekunden falls keine spezifische Dauer gegeben ist\n",
    "\n",
    "    X = np.linspace(0, duration, width)  # Zeit in Sekunden\n",
    "\n",
    "    # Frequenzen basierend auf der Anzahl der Frequenz-Bins und der Sample-Rate\n",
    "    freqs = np.linspace(0, sr / 2, height)  # Frequenzachse bis zur Nyquist-Frequenz\n",
    "    Y = freqs  # Frequenzen in Hertz\n",
    "    \n",
    "    ax.pcolormesh(X, Y, log_spec, shading='auto')\n",
    "    ax.set_xlabel(\"Time (seconds)\")\n",
    "    ax.set_ylabel(\"Frequency (Hz)\")\n",
    "    \n",
    "    # Setzen der y-Achsen-Limits auf die Nyquist-Frequenz\n",
    "    ax.set_ylim(0, sr / 2)\n",
    "\n",
    "    # # Optional: Hinzufügen einer Farbskala\n",
    "    # cbar = plt.colorbar(ax.collections[0], ax=ax)\n",
    "    # cbar.set_label('Log Power')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the example's waveform over time and the corresponding spectrogram (frequencies over time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_plot():\n",
    "    fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "    timescale = np.arange(waveform.shape[0])\n",
    "    axes[0].plot(timescale, waveform.numpy())\n",
    "    axes[0].set_title('Waveform')\n",
    "    axes[0].set_xlim([0, 44100])\n",
    "\n",
    "    plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "    axes[1].set_title('Spectrogram')\n",
    "    plt.suptitle(label.title())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_waveforms_and_spectrograms(\n",
    "    waveform_original, waveform_upsampled, \n",
    "    spectrogram_original, spectrogram_upsampled, \n",
    "    label, sr_original=44100, sr_upsampled=44100):\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 16))\n",
    "    duration_original = len(waveform_original) / sr_original\n",
    "    duration_upsampled = len(waveform_upsampled) / sr_upsampled\n",
    "\n",
    "    time_original = np.linspace(0, duration_original, num=len(waveform_original))\n",
    "    time_upsampled = np.linspace(0, duration_upsampled, num=len(waveform_upsampled))\n",
    "\n",
    "    # Original Waveform\n",
    "    axes[0, 0].plot(time_original, waveform_original)\n",
    "    axes[0, 0].set_title('Original Waveform')\n",
    "    axes[0, 0].set_xlim([0, duration_original])\n",
    "    axes[0, 0].set_xlabel(\"Time (seconds)\")\n",
    "    axes[0, 0].set_ylabel(\"Amplitude\")\n",
    "\n",
    "    # Upsampled Waveform\n",
    "    axes[0, 1].plot(time_upsampled, waveform_upsampled)\n",
    "    axes[0, 1].set_title('Upsampled Waveform')\n",
    "    axes[0, 1].set_xlim([0, duration_upsampled])\n",
    "    axes[0, 1].set_xlabel(\"Time (seconds)\")\n",
    "    axes[0, 1].set_ylabel(\"Amplitude\")\n",
    "\n",
    "    # Waveform Difference\n",
    "    min_length = min(len(waveform_original), len(waveform_upsampled))\n",
    "    waveform_diff = waveform_original[:min_length] - waveform_upsampled[:min_length]\n",
    "    time_diff = np.linspace(0, min_length / sr_original, num=min_length)\n",
    "    axes[1, 0].plot(time_diff, waveform_diff)\n",
    "    axes[1, 0].set_title('Waveform Difference')\n",
    "    axes[1, 0].set_xlim([0, time_diff[-1]])\n",
    "    axes[1, 0].set_xlabel(\"Time (seconds)\")\n",
    "    axes[1, 0].set_ylabel(\"Amplitude Difference\")\n",
    "\n",
    "    # Original Spectrogram (um 90 Grad gedreht und gespiegelte Zeitachse)\n",
    "    plot_spectrogram(np.flipud(np.rot90(spectrogram_original)), axes[1, 1], sr=sr_original, duration=duration_original)\n",
    "    axes[1, 1].set_title('Original Spectrogram')\n",
    "\n",
    "    # Upsampled Spectrogram (um 90 Grad gedreht und gespiegelte Zeitachse)\n",
    "    plot_spectrogram(np.flipud(np.rot90(spectrogram_upsampled)), axes[2, 1], sr=sr_upsampled, duration=duration_upsampled)\n",
    "    axes[2, 1].set_title('Upsampled Spectrogram')\n",
    "\n",
    "    # Spectrogram Difference (um 90 Grad gedreht und gespiegelte Zeitachse)\n",
    "    min_shape = np.minimum(spectrogram_original.shape, spectrogram_upsampled.shape)\n",
    "    spectrogram_diff = spectrogram_original[:min_shape[0], :min_shape[1]] - spectrogram_upsampled[:min_shape[0], :min_shape[1]]\n",
    "    plot_spectrogram(np.flipud(np.rot90(spectrogram_diff)), axes[2, 0], sr=sr_original, duration=duration_original)\n",
    "    axes[2, 0].set_title('Spectrogram Difference')\n",
    "\n",
    "    plt.suptitle(label)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_audio_files_and_compare(original_dir, upsampled_dir, n=3, s=1):\n",
    "    # Hole die Original- und Upsampled-Dateien\n",
    "    original_files = sorted(os.listdir(original_dir))[:n]\n",
    "    upsampled_files = sorted(os.listdir(upsampled_dir))[:n]\n",
    "    \n",
    "    for orig_file, up_file in zip(original_files, upsampled_files):\n",
    "        orig_path = os.path.join(original_dir, orig_file)\n",
    "        up_path = os.path.join(upsampled_dir, up_file)\n",
    "\n",
    "        # Laden der Audiodateien mit Librosa\n",
    "        waveform_original, sr_original = librosa.load(orig_path, sr=None)\n",
    "        waveform_upsampled, sr_upsampled = librosa.load(up_path, sr=None)\n",
    "\n",
    "        # Berechne die Anzahl der Samples, die dem gewünschten Ausschnitt (in Sekunden) entsprechen\n",
    "        samples_to_extract = s * sr_original  # in Samples\n",
    "        # Extrahiere den Anfang des Tracks\n",
    "        waveform_original_extract = waveform_original[:samples_to_extract]\n",
    "        waveform_upsampled_extract = waveform_upsampled[:samples_to_extract]\n",
    "\n",
    "        # Erzeuge die Spektrogramme aus den Ausschnitten\n",
    "        spectrogram_original = librosa.feature.melspectrogram(y=waveform_original_extract, sr=sr_original)\n",
    "        spectrogram_upsampled = librosa.feature.melspectrogram(y=waveform_upsampled_extract, sr=sr_upsampled)\n",
    "\n",
    "        # Vergleichstitel\n",
    "        label = f'Comparison: {orig_file} vs {up_file}'\n",
    "\n",
    "        # Aufruf der Vergleichsfunktion mit den Ausschnitten\n",
    "        compare_waveforms_and_spectrograms(\n",
    "            waveform_original_extract, waveform_upsampled_extract,\n",
    "            spectrogram_original, spectrogram_upsampled,\n",
    "            label, sr_original=sr_original, sr_upsampled=sr_upsampled\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create spectrogram datasets from the audio datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umwandeln der Lambda-Funktion in eine reguläre Funktion\n",
    "def map_audio_label(audio, label):\n",
    "    return get_spectrogram(audio), label\n",
    "    # return get_mel_spectrogram(audio), label\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def make_spec_ds(ds):\n",
    "    return ds.map(\n",
    "        map_func=map_audio_label,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot():\n",
    "    # Plot-Raster definieren\n",
    "    rows = 4\n",
    "    cols = 2\n",
    "    n = rows * cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
    "\n",
    "    for i in range(n):\n",
    "        r = i // cols\n",
    "        c = i % cols\n",
    "        ax = axes[r][c]\n",
    "        \n",
    "        # Plotten des Spektrogramms\n",
    "        plot_spectrogram(example_spectrograms[i].numpy(), ax)\n",
    "        \n",
    "        # Verwenden von example_spect_labels[i] statt einer nicht definierten Variable\n",
    "        ax.set_title(f\"{i}_Label: {example_spect_labels[i].numpy()}\")\n",
    "        \n",
    "        # Setzen der y-Achsenwerte und Grenzen\n",
    "        ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "        ax.set_ylim([-1.1, 1.1])\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ds():\n",
    "    global train_spectrogram_ds\n",
    "    global val_spectrogram_ds \n",
    "    global test_spectrogram_ds\n",
    "    global example_spectrograms\n",
    "    global example_spect_labels\n",
    "    \n",
    "    # Erstellen der Spektrogramme für Training, Validierung und Test\n",
    "    train_spectrogram_ds = make_spec_ds(train_ds)\n",
    "    val_spectrogram_ds = make_spec_ds(val_ds)\n",
    "    test_spectrogram_ds = make_spec_ds(test_ds)\n",
    "   \n",
    "    # for element in train_spectrogram_ds.take(1):\n",
    "    #     print(type(element))\n",
    "    #     print(len(element))  # Anzahl der Elemente im Batch\n",
    "    #     for e in element:\n",
    "    #         print(e.shape)\n",
    "\n",
    "    # for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
    "    #     break\n",
    "    \n",
    "    \n",
    "    # Funktion zum Plotten von Spektrogrammen mit y-Achse in Frequenzen (Hz)\n",
    "    def plot_spectrogram_grid(spectrograms, labels, rows=2, cols=3, sample_rate=44100, hop_length=512, n_mels=128):\n",
    "        \"\"\"\n",
    "        Plotte mehrere Mel-Spektrogramme in einem Raster und beschrifte die Achsen mit Zeit (s) und Frequenz (Hz).\n",
    "        \n",
    "        Args:\n",
    "            spectrograms: Liste oder Array von Mel-Spektrogrammen.\n",
    "            labels: Liste oder Array der zugehörigen Labels.\n",
    "            rows: Anzahl der Zeilen im Raster.\n",
    "            cols: Anzahl der Spalten im Raster.\n",
    "            sample_rate: Abtastrate der Audioaufnahme (Hz).\n",
    "            hop_length: Hop-Length (Anzahl der Samples zwischen den STFT-Fenstern).\n",
    "            n_mels: Anzahl der Mel-Bänder im Spektrogramm.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 8))\n",
    "        axes = axes.flatten()  # Um die Achsen in einer Schleife leichter zu durchlaufen\n",
    "        \n",
    "        # Frequenzachse in Hertz berechnen\n",
    "        mel_frequencies = librosa.mel_frequencies(n_mels=n_mels, fmin=0, fmax=sample_rate / 2)\n",
    "        \n",
    "        for i, ax in enumerate(axes):\n",
    "            if i < len(spectrograms):\n",
    "                spectrogram = np.squeeze(spectrograms[i])  # Entferne die Kanal-Dimension\n",
    "                \n",
    "                # Zeitachse berechnen\n",
    "                time_axis = np.arange(spectrogram.shape[1]) * (hop_length / sample_rate)\n",
    "                \n",
    "                # Spektrogramm anzeigen\n",
    "                im = ax.imshow(spectrogram, aspect='auto', origin='lower', cmap='viridis',\n",
    "                            extent=[0, time_axis[-1], mel_frequencies[0], mel_frequencies[-1]])\n",
    "                ax.set_title(f\"Label: {labels[i]}\")\n",
    "                ax.set_xlabel(\"Zeit (s)\")\n",
    "                ax.set_ylabel(\"Frequenz (Hz)\")\n",
    "                # fig.colorbar(im, ax=ax, format='%+2.0f dB')\n",
    "            else:\n",
    "                ax.axis('off')  # Leere Plots ausblenden\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Nehme ein Beispiel aus dem Dataset und plotte es in einem Raster\n",
    "    for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
    "        spectrograms = example_spectrograms.numpy()\n",
    "        labels = example_spect_labels.numpy()\n",
    "        plot_spectrogram_grid(spectrograms, labels, rows=2, cols=3, sample_rate=44100, hop_length=512, n_mels=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(input_label_names, input_train_ds, input_val_ds, input_test_ds, input_mel_spectogram ):\n",
    "    global label_names, example_labels, example_audio\n",
    "    global train_ds, val_ds, test_ds \n",
    "    global mel_spectogram\n",
    "\n",
    "    label_names = input_label_names\n",
    "    train_ds = input_train_ds\n",
    "    val_ds = input_val_ds\n",
    "    test_ds = input_test_ds\n",
    "    mel_spectogram = input_mel_spectogram\n",
    "    \n",
    "    for example_audio, example_labels in train_ds.take(1):\n",
    "        break\n",
    "\n",
    "    print(\"Loaded label_names (before play):\", label_names)\n",
    "    \n",
    "    # play()\n",
    "    # Verzeichnispfade\n",
    "    original_dir = r\"I:\\Uni-Git\\Master\\Tutorial\\data\\small_train_ds_splits\\orig-16-44-mono\"\n",
    "    upsampled_dir = r\"I:\\Uni-Git\\Master\\Tutorial\\data\\small_train_ds_splits\\upscale-from-mp3-128\"\n",
    "\n",
    "    # Vergleich der ersten 3 Dateien\n",
    "    load_audio_files_and_compare(original_dir, upsampled_dir, n=3)\n",
    "    # load_audio_from_dataset_and_compare(train_ds)\n",
    "    # small_plot()\n",
    "    build_ds()\n",
    "    # plot()\n",
    "    print(\"Loaded label_names (after play):\", label_names)\n",
    "    \n",
    "    return train_spectrogram_ds, val_spectrogram_ds, test_spectrogram_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Notebook individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DIR = pathlib.Path('data/train_files')\n",
    "# TEST_DIR = pathlib.Path('data/test_files')\n",
    "# DATA_DIR = pathlib.Path('data')\n",
    "\n",
    "\n",
    "# file_list = tf.data.Dataset.list_files(str(TRAIN_DIR / '**/*.wav'), shuffle=False)\n",
    "\n",
    "# seconds=20\n",
    "# train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "#     directory=TRAIN_DIR,\n",
    "#     batch_size=64,\n",
    "#     validation_split=0.2,\n",
    "#     seed=0,\n",
    "#     output_sequence_length=16000*seconds,\n",
    "#     subset='both'\n",
    "#     )\n",
    "\n",
    "# label_names = np.array(train_ds.class_names)\n",
    "# print()\n",
    "# print(\"label names:\", label_names)\n",
    "# play()\n",
    "# plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
