{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "import os\n",
    "import librosa\n",
    "# import tensorflow_io as tfio  \n",
    "\n",
    "\n",
    "label_names = None\n",
    "example_labels = None\n",
    "example_audio = None\n",
    "train_ds = None\n",
    "val_ds = None\n",
    "test_ds = None\n",
    "waveform = None \n",
    "\n",
    "spectrogram = None\n",
    "label = None\n",
    "\n",
    "train_spectrogram_ds = None\n",
    "val_spectrogram_ds = None\n",
    "test_spectrogram_ds = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert waveforms to spectrograms\n",
    "\n",
    "The waveforms in the dataset are represented in the time domain. Next, you'll transform the waveforms from the time-domain signals into the time-frequency-domain signals by computing the [short-time Fourier transform (STFT)](https://en.wikipedia.org/wiki/Short-time_Fourier_transform) to convert the waveforms to as [spectrograms](https://en.wikipedia.org/wiki/Spectrogram), which show frequency changes over time and can be represented as 2D images. You will feed the spectrogram images into your neural network to train the model.\n",
    "\n",
    "A Fourier transform (`tf.signal.fft`) converts a signal to its component frequencies, but loses all time information. In comparison, STFT (`tf.signal.stft`) splits the signal into windows of time and runs a Fourier transform on each window, preserving some time information, and returning a 2D tensor that you can run standard convolutions on.\n",
    "\n",
    "Create a utility function for converting waveforms to spectrograms:\n",
    "\n",
    "- The waveforms need to be of the same length, so that when you convert them to spectrograms, the results have similar dimensions. This can be done by simply zero-padding the audio clips that are shorter than one second (using `tf.zeros`).\n",
    "- When calling `tf.signal.stft`, choose the `frame_length` and `frame_step` parameters such that the generated spectrogram \"image\" is almost square. For more information on the STFT parameters choice, refer to [this Coursera video](https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe) on audio signal processing and STFT.\n",
    "- The STFT produces an array of complex numbers representing magnitude and phase. However, in this tutorial you'll only use the magnitude, which you can derive by applying `tf.abs` on the output of `tf.signal.stft`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "    # Convert the waveform to a spectrogram via a STFT.\n",
    "    spectrogram = tf.signal.stft(\n",
    "        waveform, frame_length=255, frame_step=128)\n",
    "    # Obtain the magnitude of the STFT.\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    # Add a `channels` dimension, so that the spectrogram can be used\n",
    "    # as image-like input data with convolution layers (which expect\n",
    "    # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "    spectrogram = spectrogram[..., tf.newaxis]\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mel_spectrogram(waveform):\n",
    "    spec = librosa.feature.melspectrogram(y=waveform, sr=44100)\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, start exploring the data. Print the shapes of one example's tensorized waveform and the corresponding spectrogram, and play the original audio:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play():\n",
    "  global waveform\n",
    "  global label\n",
    "  global spectrogram\n",
    "  for i in range(3):\n",
    "    label = label_names[example_labels[i]]\n",
    "    waveform = example_audio[i]\n",
    "    spectrogram = get_spectrogram(waveform)\n",
    "    # spectrogram = get_mel_spectrogram(waveform)\n",
    "\n",
    "    print('Label:', label)\n",
    "    print('Waveform shape:', waveform.shape)\n",
    "    print('Spectrogram shape:', spectrogram.shape)\n",
    "    print('Audio playback')\n",
    "    display.display(display.Audio(waveform, rate=44100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define a function for displaying a spectrogram:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_spectrogram(spectrogram, ax):\n",
    "#   if len(spectrogram.shape) > 2:\n",
    "#     assert len(spectrogram.shape) == 3\n",
    "#     spectrogram = np.squeeze(spectrogram, axis=-1)\n",
    "#   # Convert the frequencies to log scale and transpose, so that the time is\n",
    "#   # represented on the x-axis (columns).\n",
    "#   # Add an epsilon to avoid taking a log of zero.\n",
    "#   log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
    "#   height = log_spec.shape[0]\n",
    "#   width = log_spec.shape[1]\n",
    "#   X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "#   Y = range(height)\n",
    "#   ax.pcolormesh(X, Y, log_spec)\n",
    "\n",
    "def plot_spectrogram(spectrogram, ax, sr=44100, duration=None):\n",
    "    if len(spectrogram.shape) > 2:\n",
    "        spectrogram = np.squeeze(spectrogram, axis=-1)\n",
    "\n",
    "    # Logarithmische Skalierung der Spektrogramm-Daten\n",
    "    log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
    "    height, width = log_spec.shape\n",
    "\n",
    "    # Berechnung der x-Achse basierend auf der Audiodauer\n",
    "    if duration is None:\n",
    "        duration = width / sr  # Dauer in Sekunden falls keine spezifische Dauer gegeben ist\n",
    "\n",
    "    X = np.linspace(0, duration, width)  # Zeit in Sekunden\n",
    "\n",
    "    # Frequenzen basierend auf der Anzahl der Frequenz-Bins und der Sample-Rate\n",
    "    freqs = np.linspace(0, sr / 2, height)  # Frequenzachse bis zur Nyquist-Frequenz\n",
    "    Y = freqs  # Frequenzen in Hertz\n",
    "    \n",
    "    ax.pcolormesh(X, Y, log_spec, shading='auto')\n",
    "    ax.set_xlabel(\"Time (seconds)\")\n",
    "    ax.set_ylabel(\"Frequency (Hz)\")\n",
    "    \n",
    "    # Setzen der y-Achsen-Limits auf die Nyquist-Frequenz\n",
    "    ax.set_ylim(0, sr / 2)\n",
    "\n",
    "    # # Optional: Hinzuf√ºgen einer Farbskala\n",
    "    # cbar = plt.colorbar(ax.collections[0], ax=ax)\n",
    "    # cbar.set_label('Log Power')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the example's waveform over time and the corresponding spectrogram (frequencies over time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_plot():\n",
    "    fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "    timescale = np.arange(waveform.shape[0])\n",
    "    axes[0].plot(timescale, waveform.numpy())\n",
    "    axes[0].set_title('Waveform')\n",
    "    axes[0].set_xlim([0, 44100])\n",
    "\n",
    "    plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "    axes[1].set_title('Spectrogram')\n",
    "    plt.suptitle(label.title())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_waveforms_and_spectrograms(\n",
    "    waveform_original, waveform_upsampled, \n",
    "    spectrogram_original, spectrogram_upsampled, \n",
    "    label, sr_original=44100, sr_upsampled=44100):\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 16))\n",
    "    duration_original = len(waveform_original) / sr_original\n",
    "    duration_upsampled = len(waveform_upsampled) / sr_upsampled\n",
    "\n",
    "    time_original = np.linspace(0, duration_original, num=len(waveform_original))\n",
    "    time_upsampled = np.linspace(0, duration_upsampled, num=len(waveform_upsampled))\n",
    "\n",
    "    # Original Waveform\n",
    "    axes[0, 0].plot(time_original, waveform_original)\n",
    "    axes[0, 0].set_title('Original Waveform')\n",
    "    axes[0, 0].set_xlim([0, duration_original])\n",
    "    axes[0, 0].set_xlabel(\"Time (seconds)\")\n",
    "    axes[0, 0].set_ylabel(\"Amplitude\")\n",
    "\n",
    "    # Upsampled Waveform\n",
    "    axes[0, 1].plot(time_upsampled, waveform_upsampled)\n",
    "    axes[0, 1].set_title('Upsampled Waveform')\n",
    "    axes[0, 1].set_xlim([0, duration_upsampled])\n",
    "    axes[0, 1].set_xlabel(\"Time (seconds)\")\n",
    "    axes[0, 1].set_ylabel(\"Amplitude\")\n",
    "\n",
    "    # Waveform Difference\n",
    "    min_length = min(len(waveform_original), len(waveform_upsampled))\n",
    "    waveform_diff = waveform_original[:min_length] - waveform_upsampled[:min_length]\n",
    "    time_diff = np.linspace(0, min_length / sr_original, num=min_length)\n",
    "    axes[1, 0].plot(time_diff, waveform_diff)\n",
    "    axes[1, 0].set_title('Waveform Difference')\n",
    "    axes[1, 0].set_xlim([0, time_diff[-1]])\n",
    "    axes[1, 0].set_xlabel(\"Time (seconds)\")\n",
    "    axes[1, 0].set_ylabel(\"Amplitude Difference\")\n",
    "\n",
    "    # Original Spectrogram (um 90 Grad gedreht und gespiegelte Zeitachse)\n",
    "    plot_spectrogram(np.flipud(np.rot90(spectrogram_original)), axes[1, 1], sr=sr_original, duration=duration_original)\n",
    "    axes[1, 1].set_title('Original Spectrogram')\n",
    "\n",
    "    # Upsampled Spectrogram (um 90 Grad gedreht und gespiegelte Zeitachse)\n",
    "    plot_spectrogram(np.flipud(np.rot90(spectrogram_upsampled)), axes[2, 1], sr=sr_upsampled, duration=duration_upsampled)\n",
    "    axes[2, 1].set_title('Upsampled Spectrogram')\n",
    "\n",
    "    # Spectrogram Difference (um 90 Grad gedreht und gespiegelte Zeitachse)\n",
    "    min_shape = np.minimum(spectrogram_original.shape, spectrogram_upsampled.shape)\n",
    "    spectrogram_diff = spectrogram_original[:min_shape[0], :min_shape[1]] - spectrogram_upsampled[:min_shape[0], :min_shape[1]]\n",
    "    plot_spectrogram(np.flipud(np.rot90(spectrogram_diff)), axes[2, 0], sr=sr_original, duration=duration_original)\n",
    "    axes[2, 0].set_title('Spectrogram Difference')\n",
    "\n",
    "    plt.suptitle(label)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_audio_files_and_compare(original_dir, upsampled_dir, n=3, s=1):\n",
    "    # Hole die Original- und Upsampled-Dateien\n",
    "    original_files = sorted(os.listdir(original_dir))[:n]\n",
    "    upsampled_files = sorted(os.listdir(upsampled_dir))[:n]\n",
    "    \n",
    "    for orig_file, up_file in zip(original_files, upsampled_files):\n",
    "        orig_path = os.path.join(original_dir, orig_file)\n",
    "        up_path = os.path.join(upsampled_dir, up_file)\n",
    "\n",
    "        # Laden der Audiodateien mit Librosa\n",
    "        waveform_original, sr_original = librosa.load(orig_path, sr=None)\n",
    "        waveform_upsampled, sr_upsampled = librosa.load(up_path, sr=None)\n",
    "\n",
    "        # Berechne die Anzahl der Samples, die dem gew√ºnschten Ausschnitt (in Sekunden) entsprechen\n",
    "        samples_to_extract = s * sr_original  # in Samples\n",
    "        # Extrahiere den Anfang des Tracks\n",
    "        waveform_original_extract = waveform_original[:samples_to_extract]\n",
    "        waveform_upsampled_extract = waveform_upsampled[:samples_to_extract]\n",
    "\n",
    "        # Erzeuge die Spektrogramme aus den Ausschnitten\n",
    "        spectrogram_original = librosa.feature.melspectrogram(y=waveform_original_extract, sr=sr_original)\n",
    "        spectrogram_upsampled = librosa.feature.melspectrogram(y=waveform_upsampled_extract, sr=sr_upsampled)\n",
    "\n",
    "        # Vergleichstitel\n",
    "        label = f'Comparison: {orig_file} vs {up_file}'\n",
    "\n",
    "        # Aufruf der Vergleichsfunktion mit den Ausschnitten\n",
    "        compare_waveforms_and_spectrograms(\n",
    "            waveform_original_extract, waveform_upsampled_extract,\n",
    "            spectrogram_original, spectrogram_upsampled,\n",
    "            label, sr_original=sr_original, sr_upsampled=sr_upsampled\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create spectrogram datasets from the audio datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umwandeln der Lambda-Funktion in eine regul√§re Funktion\n",
    "def map_audio_label(audio, label):\n",
    "    return get_spectrogram(audio), label\n",
    "    # return get_mel_spectrogram(audio), label\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def make_spec_ds(ds):\n",
    "    return ds.map(\n",
    "        map_func=map_audio_label,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot():\n",
    "    # Plot-Raster definieren\n",
    "    rows = 4\n",
    "    cols = 2\n",
    "    n = rows * cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
    "\n",
    "    for i in range(n):\n",
    "        r = i // cols\n",
    "        c = i % cols\n",
    "        ax = axes[r][c]\n",
    "        \n",
    "        # Plotten des Spektrogramms\n",
    "        plot_spectrogram(example_spectrograms[i].numpy(), ax)\n",
    "        \n",
    "        # Verwenden von example_spect_labels[i] statt einer nicht definierten Variable\n",
    "        ax.set_title(f\"{i}_Label: {example_spect_labels[i].numpy()}\")\n",
    "        \n",
    "        # Setzen der y-Achsenwerte und Grenzen\n",
    "        ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "        ax.set_ylim([-1.1, 1.1])\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ds():\n",
    "    global train_spectrogram_ds\n",
    "    global val_spectrogram_ds \n",
    "    global test_spectrogram_ds\n",
    "    global example_spectrograms\n",
    "    global example_spect_labels\n",
    "    \n",
    "    train_spectrogram_ds = make_spec_ds(train_ds)\n",
    "    val_spectrogram_ds = make_spec_ds(val_ds)\n",
    "    test_spectrogram_ds = make_spec_ds(test_ds)\n",
    "    \n",
    "    # train_spectrogram_ds = make_combined_ds(train_ds)\n",
    "    # val_spectrogram_ds = make_combined_ds(val_ds)\n",
    "    # test_spectrogram_ds = make_combined_ds(test_ds)\n",
    "    \n",
    "    # @tf.autograph.experimental.do_not_convert\n",
    "    # def squeeze(audio, labels):\n",
    "    #      audio = tf.squeeze(audio, axis=-1)\n",
    "    #      return audio, labels\n",
    "\n",
    "    # train_spectrogram_ds = train_spectrogram_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "    # val_spectrogram_ds = val_spectrogram_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "    # test_spectrogram_ds = test_spectrogram_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(input_label_names, input_train_ds, input_val_ds, input_test_ds):\n",
    "    global label_names, example_labels, example_audio\n",
    "    global train_ds, val_ds, test_ds\n",
    "\n",
    "    label_names = input_label_names\n",
    "    train_ds = input_train_ds\n",
    "    val_ds = input_val_ds\n",
    "    test_ds = input_test_ds\n",
    "    \n",
    "    for example_audio, example_labels in train_ds.take(1):\n",
    "        break\n",
    "\n",
    "    print(\"Loaded label_names (before play):\", label_names)\n",
    "    \n",
    "    play()\n",
    "    # Verzeichnispfade\n",
    "    original_dir = r\"I:\\Uni-Git\\Master\\Tutorial\\data\\large_train_ds\\orig-16-44-mono\"\n",
    "    upsampled_dir = r\"I:\\Uni-Git\\Master\\Tutorial\\data\\large_train_ds\\upscale-from-mp3-128\"\n",
    "\n",
    "    # Vergleich der ersten 3 Dateien\n",
    "    load_audio_files_and_compare(original_dir, upsampled_dir, n=3)\n",
    "    # load_and_compare_from_dataset()\n",
    "    # small_plot()\n",
    "    build_ds()\n",
    "    # plot()\n",
    "    print(\"Loaded label_names (after play):\", label_names)\n",
    "    \n",
    "    return train_spectrogram_ds, val_spectrogram_ds, test_spectrogram_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Notebook individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DIR = pathlib.Path('data/train_files')\n",
    "# TEST_DIR = pathlib.Path('data/test_files')\n",
    "# DATA_DIR = pathlib.Path('data')\n",
    "\n",
    "\n",
    "# file_list = tf.data.Dataset.list_files(str(TRAIN_DIR / '**/*.wav'), shuffle=False)\n",
    "\n",
    "# seconds=20\n",
    "# train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "#     directory=TRAIN_DIR,\n",
    "#     batch_size=64,\n",
    "#     validation_split=0.2,\n",
    "#     seed=0,\n",
    "#     output_sequence_length=16000*seconds,\n",
    "#     subset='both'\n",
    "#     )\n",
    "\n",
    "# label_names = np.array(train_ds.class_names)\n",
    "# print()\n",
    "# print(\"label names:\", label_names)\n",
    "# play()\n",
    "# plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
