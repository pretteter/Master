{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fluF3_oOgkWF"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:16.549129Z",
     "iopub.status.busy": "2024-08-16T07:47:16.548739Z",
     "iopub.status.idle": "2024-08-16T07:47:16.552401Z",
     "shell.execute_reply": "2024-08-16T07:47:16.551835Z"
    },
    "id": "AJs7HHFmg1M9"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "# Simple audio recognition: Recognizing keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNbqmZy0gbyE"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/audio/simple_audio\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/audio/simple_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPfDNFlb66XF"
   },
   "source": [
    "This tutorial demonstrates how to preprocess audio files in the WAV format and build and train a basic [automatic speech recognition](https://en.wikipedia.org/wiki/Speech_recognition) (ASR) model for recognizing ten different words. You will use a portion of the [Speech Commands dataset](https://www.tensorflow.org/datasets/catalog/speech_commands) ([Warden, 2018](https://arxiv.org/abs/1804.03209)), which contains short (one-second or less) audio clips of commands, such as \"down\", \"go\", \"left\", \"no\", \"right\", \"stop\", \"up\" and \"yes\".\n",
    "\n",
    "Real-world speech and audio recognition [systems](https://ai.googleblog.com/search/label/Speech%20Recognition) are complex. But, like [image classification with the MNIST dataset](../quickstart/beginner.ipynb), this tutorial should give you a basic understanding of the techniques involved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go9C3uLL8Izc"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary modules and dependencies. You'll be using `tf.keras.utils.audio_dataset_from_directory` (introduced in TensorFlow 2.10), which helps generate audio classification datasets from directories of `.wav` files. You'll also need [seaborn](https://seaborn.pydata.org) for visualization in this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:16.555919Z",
     "iopub.status.busy": "2024-08-16T07:47:16.555546Z",
     "iopub.status.idle": "2024-08-16T07:47:17.865607Z",
     "shell.execute_reply": "2024-08-16T07:47:17.864292Z"
    },
    "id": "hhNW45sjDEDe"
   },
   "outputs": [],
   "source": [
    "# Step 1: Install TensorFlow and Datasets\n",
    "%pip install -U -q tensorflow tensorflow_datasets\n",
    "\n",
    "# Step 2: Install Wrapt\n",
    "%pip install wrapt==1.14.1\n",
    "\n",
    "# Step 3: Install Visualization Libraries\n",
    "%pip install matplotlib seaborn\n",
    "\n",
    "# Step 4: Install PySoundFile\n",
    "%pip install pysoundfile\n",
    "\n",
    "# Step 5: Reinstall TensorFlow I/O\n",
    "# !pip uninstall -y tensorflow-io \n",
    "%pip install tensorflow-io\n",
    "# %pip install --upgrade tensorflow\n",
    "\n",
    "%pip install nbformat\n",
    "\n",
    "# Step 6: Install IPykernel\n",
    "%pip install ipykernel\n",
    "\n",
    "%pip install ipynb\n",
    "\n",
    "%pip install pickleshare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import glob\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "# import tensorflow_io as tfio\n",
    "import pathlib\n",
    "from IPython import get_ipython\n",
    "\n",
    "TRAIN_DIR = pathlib.Path('data/small_train_ds')\n",
    "# TRAIN_DIR = pathlib.Path('data/train_files')\n",
    "TEST_DIR = pathlib.Path('data/small_test_ds')\n",
    "# TEST_DIR = pathlib.Path('data/test_files')\n",
    "DATA_DIR = pathlib.Path('data')\n",
    "\n",
    "\n",
    "import ipynb.fs.defs.audio_extraction as audio_extraction\n",
    "import ipynb.fs.defs.build_database as build_database\n",
    "import ipynb.fs.defs.waveforms_to_spectrograms as wave_to_spec\n",
    "import ipynb.fs.defs.build_train_model as build_train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notebook_extract():\n",
    "    audio_extraction.extract_zip(TRAIN_DIR, DATA_DIR)\n",
    "    print('-' * 50)  \n",
    "    audio_extraction.extract_zip(TEST_DIR, DATA_DIR)\n",
    "    print('-' * 50)\n",
    "    print('-' * 50)  \n",
    "    audio_extraction.rename_audio_files(DATA_DIR)\n",
    "    print('-' * 50)\n",
    "    print('-' * 50)  \n",
    "    audio_extraction.process_directory(TRAIN_DIR) \n",
    "\n",
    "notebook_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = build_database.run(TRAIN_DIR, TEST_DIR, DATA_DIR)\n",
    "file_list, example_labels, example_audio, train_ds, val_ds, test_ds, label_names, example_audio, example_labels, example_filenames = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voxGEwvuh2L7"
   },
   "source": [
    "Let's plot a few audio waveforms:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:29.765708Z",
     "iopub.status.busy": "2024-08-16T07:47:29.765166Z",
     "iopub.status.idle": "2024-08-16T07:47:31.051008Z",
     "shell.execute_reply": "2024-08-16T07:47:31.050343Z"
    },
    "id": "8yuX6Nqzf6wT"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "rows = 4\n",
    "cols = 2\n",
    "n = rows * cols\n",
    "for i in range(n):\n",
    "  print(\"i \",i)\n",
    "  # print(label_names)\n",
    "  # print(example_labels[1])\n",
    "  # print(example_labels)\n",
    "  plt.subplot(rows, cols, i+1)\n",
    "  audio_signal = example_audio[i]\n",
    "  plt.plot(audio_signal)\n",
    "  # plt.title(label_names[example_labels[i]])\n",
    "  label = label_names[example_labels[i].numpy()] \n",
    "  plt.title(f\"{i}_Label: {label} __ {example_filenames[i]}\")\n",
    "  plt.yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "  plt.ylim([-1.1, 1.1])\n",
    "  \n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert waveforms to spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = wave_to_spec.run(label_names, example_labels, example_audio, train_ds, val_ds, test_ds, example_filenames, file_list)\n",
    "label_names, example_labels, example_audio, train_ds, val_ds, test_ds, example_filenames, waveform, file_list, train_spectrogram_ds,val_spectrogram_ds,test_spectrogram_ds, example_spectrograms,example_spect_labels = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5KdY8IF8rkt"
   },
   "source": [
    "## Build and train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = build_train_model.run(train_spectrogram_ds,val_spectrogram_ds,test_spectrogram_ds, example_spectrograms, label_names)\n",
    "train_spectrogram_ds,val_spectrogram_ds,test_spectrogram_ds, example_spectrograms, label_names,  model, history = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjpCDeQ4mUfS"
   },
   "source": [
    "Let's plot the training and validation loss curves to check how your model has improved during training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:46.277290Z",
     "iopub.status.busy": "2024-08-16T07:47:46.276985Z",
     "iopub.status.idle": "2024-08-16T07:47:46.551824Z",
     "shell.execute_reply": "2024-08-16T07:47:46.551207Z"
    },
    "id": "nzhipg3Gu2AY"
   },
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "print(metrics)\n",
    "# plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.plot(history.epoch, metrics['loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss [CrossEntropy]')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "# plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
    "plt.plot(history.epoch, 100*np.array(metrics['accuracy']))\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.ylim([0, 100])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy [%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZTt3kO3mfm4"
   },
   "source": [
    "## Evaluate the model performance\n",
    "\n",
    "Run the model on the test set and check the model's performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:46.555333Z",
     "iopub.status.busy": "2024-08-16T07:47:46.555094Z",
     "iopub.status.idle": "2024-08-16T07:47:46.741008Z",
     "shell.execute_reply": "2024-08-16T07:47:46.740155Z"
    },
    "id": "FapuRT_SsWGQ"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_spectrogram_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en9Znt1NOabH"
   },
   "source": [
    "### Display a confusion matrix\n",
    "\n",
    "Use a [confusion matrix](https://developers.google.com/machine-learning/glossary#confusion-matrix) to check how well the model did classifying each of the commands in the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:47.150558Z",
     "iopub.status.busy": "2024-08-16T07:47:47.150278Z",
     "iopub.status.idle": "2024-08-16T07:47:47.738680Z",
     "shell.execute_reply": "2024-08-16T07:47:47.737949Z"
    },
    "id": "LvoSAOiXU3lL"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_spectrogram_ds)\n",
    "y_pred = tf.argmax(y_pred, axis=1)\n",
    "y_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)\n",
    "\n",
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx,\n",
    "            xticklabels=label_names,\n",
    "            yticklabels=label_names,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQGi_mzPcLvl"
   },
   "source": [
    "## Run inference on an audio file\n",
    "\n",
    "Finally, verify the model's prediction output using an input audio file of someone saying \"no\". How well does your model perform?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_audio(file_path):\n",
    "    try:\n",
    "        import ipynb.fs.defs.audio_extraction as audio_extraction\n",
    "        audio, sample_rate = audio_extraction.load_wav_file_basic(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process file '{file_path}': {e}\")\n",
    "        return\n",
    "    # Entfernen der letzten Achse, falls nur ein Kanal vorliegt\n",
    "    waveform = tf.squeeze(audio, axis=-1)\n",
    "\n",
    "    print(f\"Form des Audiosignals: {waveform.shape}\")\n",
    "    print(f\"Sample Rate: {sample_rate}\")\n",
    "\n",
    "    spectrogram = wave_to_spec.get_spectrogram(waveform)\n",
    "\n",
    "    # Dimension anpassen für das Modell\n",
    "    input_tensor = spectrogram[tf.newaxis, ...]\n",
    "\n",
    "    # Vorhersage des Modells\n",
    "    prediction = model(input_tensor)\n",
    "\n",
    "    # Labels für die Vorhersage\n",
    "    x_labels = label_names  # Annahme: 'label_names' ist definiert\n",
    "\n",
    "    # Balkendiagramm der Vorhersagen anzeigen\n",
    "    plt.bar(x_labels, tf.nn.softmax(prediction[0]))\n",
    "    plt.title(f'Vorhersage für {os.path.basename(file_path)}')\n",
    "    plt.show()\n",
    "\n",
    "    # Audio im Notebook abspielen\n",
    "    display.display(display.Audio(waveform, rate=16000))\n",
    "\n",
    "def process_directory_for_visualization(directory_path):\n",
    "    wav_files = glob.glob(os.path.join(directory_path, \"*.wav\"))\n",
    "\n",
    "    if not wav_files:\n",
    "        print(\"Keine WAV-Dateien im Verzeichnis gefunden.\")\n",
    "        return\n",
    "\n",
    "    for file_path in wav_files:\n",
    "        print(f\"Verarbeite Datei: {file_path}\")\n",
    "        visualize_audio(file_path)\n",
    "\n",
    "# extract_zip(TEST_DIR, DATA_DIR)\n",
    "# rename_audio_files(DATA_DIR)\n",
    "\n",
    "process_directory_for_visualization(TEST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1icqlM3ISW0"
   },
   "source": [
    "## Export the model with preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7HX-MjgIbji"
   },
   "source": [
    "The model's not very easy to use if you have to apply those preprocessing steps before passing data to the model for inference. So build an end-to-end version:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:48.457991Z",
     "iopub.status.busy": "2024-08-16T07:47:48.457717Z",
     "iopub.status.idle": "2024-08-16T07:47:48.464142Z",
     "shell.execute_reply": "2024-08-16T07:47:48.463566Z"
    },
    "id": "2lIeXdWjIbDE"
   },
   "outputs": [],
   "source": [
    "class ExportModel(tf.Module):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "\n",
    "    # Accept either a string-filename or a batch of waveforms.\n",
    "    # YOu could add additional signatures for a single wave, or a ragged-batch. \n",
    "    self.__call__.get_concrete_function(\n",
    "        x=tf.TensorSpec(shape=(), dtype=tf.string))\n",
    "    self.__call__.get_concrete_function(\n",
    "       x=tf.TensorSpec(shape=[None, 16000], dtype=tf.float32))\n",
    "\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, x):\n",
    "    # If they pass a string, load the file and decode it. \n",
    "    if x.dtype == tf.string:\n",
    "      x = tf.io.read_file(x)\n",
    "      x, _ = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n",
    "      x = tf.squeeze(x, axis=-1)\n",
    "      x = x[tf.newaxis, :]\n",
    "    \n",
    "    x = get_spectrogram(x)  \n",
    "    result = self.model(x, training=False)\n",
    "    \n",
    "    class_ids = tf.argmax(result, axis=-1)\n",
    "    class_names = tf.gather(label_names, class_ids)\n",
    "    return {'predictions':result,\n",
    "            'class_ids': class_ids,\n",
    "            'class_names': class_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtZBmUiB9HGY"
   },
   "source": [
    "Test run the \"export\" model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:48.467167Z",
     "iopub.status.busy": "2024-08-16T07:47:48.466892Z",
     "iopub.status.idle": "2024-08-16T07:47:48.770920Z",
     "shell.execute_reply": "2024-08-16T07:47:48.770248Z"
    },
    "id": "Z1_8TYaCIRue"
   },
   "outputs": [],
   "source": [
    "export = ExportModel(model)\n",
    "# export(tf.constant(str(data_dir/'no/01bb6a2a_nohash_0.wav')))\n",
    "export(tf.constant(str(\"../Tutorial/data/test_files/AMBIENCE_JAPAN_THUNDER_HEAVY_RAIN_SLIGHT_WIND_NOISE_STEREO.wav\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1J6Iuz829Cxo"
   },
   "source": [
    "Save and reload the model, the reloaded model gives identical output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:47:48.774390Z",
     "iopub.status.busy": "2024-08-16T07:47:48.774117Z",
     "iopub.status.idle": "2024-08-16T07:47:49.775771Z",
     "shell.execute_reply": "2024-08-16T07:47:49.775108Z"
    },
    "id": "wTAg4vsn3oEb"
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(export, \"saved\")\n",
    "imported = tf.saved_model.load(\"saved\")\n",
    "imported(waveform[tf.newaxis, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3jF933m9z1J"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "This tutorial demonstrated how to carry out simple audio classification/automatic speech recognition using a convolutional neural network with TensorFlow and Python. To learn more, consider the following resources:\n",
    "\n",
    "- The [Sound classification with YAMNet](https://www.tensorflow.org/hub/tutorials/yamnet) tutorial shows how to use transfer learning for audio classification.\n",
    "- The notebooks from [Kaggle's TensorFlow speech recognition challenge](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/overview).\n",
    "- The\n",
    "  [TensorFlow.js - Audio recognition using transfer learning codelab](https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0) teaches how to build your own interactive web app for audio classification.\n",
    "- [A tutorial on deep learning for music information retrieval](https://arxiv.org/abs/1709.04396) (Choi et al., 2017) on arXiv.\n",
    "- TensorFlow also has additional support for [audio data preparation and augmentation](https://www.tensorflow.org/io/tutorials/audio) to help with your own audio-based projects.\n",
    "- Consider using the [librosa](https://librosa.org/) library for music and audio analysis.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "CPU",
  "colab": {
   "collapsed_sections": [],
   "name": "simple_audio.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
